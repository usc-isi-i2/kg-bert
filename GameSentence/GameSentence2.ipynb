{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "incredible-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, spacy, gzip, rltk, gzip, random, os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "helpful-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir='input'\n",
    "wordnet_file=\"%s/kgtk_wordnet.tsv\" % input_dir\n",
    "cskg_embeddings_file=\"%s/bert_embeddings.txt\" % input_dir\n",
    "cskg_tranE_file=\"%s/trans_log_dot_0.1.tsv.gz\" % input_dir\n",
    "cskg_complex_file='%s/comp_log_dot_0.1.tsv.gz' % input_dir\n",
    "cskg_file=\"%s/cskg_renamed.tsv.gz\" % input_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-copper",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "load data model, embedding file, and black list (takes 10 mins, grab a coffee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138cd351-4e72-40ba-835d-1bb4855312f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/filipilievski/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greek-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/filipilievski/.cache/torch/sentence_transformers/sbert.net_models_nli-bert-large/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# sentence transformer model\n",
    "model = SentenceTransformer('nli-bert-large')\n",
    "# nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# sentiment model\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capable-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2token(text):\n",
    "    doc=nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        break\n",
    "        \n",
    "    return list(sent)\n",
    "\n",
    "def load_source(filename):\n",
    "    # load data\n",
    "    with open(filename,\"r\",encoding=\"utf-8\") as f:\n",
    "        head=f.readline().strip().split(\"\\t\")\n",
    "        data=[]\n",
    "        for line in f:\n",
    "            temp=line.strip().split(\"\\t\")\n",
    "            data.append([item.split(\"|\")[0] for item in temp])\n",
    "            \n",
    "    return head, data\n",
    "\n",
    "def cosine_similar(embed1, embed2):\n",
    "    similar=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "    return similar\n",
    "\n",
    "# load cskg embedding file\n",
    "def load_embedding():\n",
    "    with open(cskg_embeddings_file,\"r\") as f:\n",
    "        head= f.readline().strip().split(\"\\t\")\n",
    "\n",
    "        # obtain embedding_sentence in file\n",
    "        cskg_word_embeddings=dict()\n",
    "\n",
    "        for item in tqdm(f):\n",
    "            # obtain list of line\n",
    "            line=item.strip().split(\"\\t\")\n",
    "\n",
    "            # only property is text embedding can obtain embeddings\n",
    "            word=line[0]\n",
    "            prop=line[1]\n",
    "            embedding=line[2]\n",
    "\n",
    "            if prop==\"text_embedding\":\n",
    "                cskg_word_embeddings[word]=embedding\n",
    "    return cskg_word_embeddings\n",
    "\n",
    "# load complex and tranE file\n",
    "def load_embedding_gz(filename):\n",
    "    cskg_word_embeddings=dict()\n",
    "    f=gzip.open(filename,'rb')\n",
    "    \n",
    "    for item in tqdm(f):\n",
    "        # obtain list of line\n",
    "        line=item.strip().decode(\"utf-8\").split(\"\\t\")\n",
    "        word=line[0]\n",
    "        embed=np.array([float(_) for _ in line[2:]])\n",
    "        cskg_word_embeddings[word]=embed\n",
    "        \n",
    "    return cskg_word_embeddings\n",
    "\n",
    "def synsets_cosine_sim(label1, label2):\n",
    "    # find the highest cosine similarity between two label by synsets definition\n",
    "    syns1=wn.synsets(label1.replace(\" \",\"_\"))\n",
    "    syns2=wn.synsets(label2.replace(\" \",\"_\"))\n",
    "    \n",
    "    sents1=[]\n",
    "    sents2=[]\n",
    "    \n",
    "    for syn in syns1:\n",
    "        sent=syn.definition()\n",
    "        sents1.append(sent)\n",
    "        \n",
    "    for syn in syns2:\n",
    "        sent=syn.definition()\n",
    "        sents2.append(sent)\n",
    "        \n",
    "    sents1_embed=model.encode(sents1)\n",
    "    sents2_embed=model.encode(sents2)\n",
    "    max_sim=0\n",
    "    \n",
    "    for embed1 in sents1_embed:\n",
    "        for embed2 in sents2_embed:\n",
    "            sim=cosine_similar(embed1,embed2)\n",
    "            max_sim=max(max_sim,sim)\n",
    "            \n",
    "    return max_sim\n",
    "\n",
    "def find_sent(sent_target, items):\n",
    "    for item in items:\n",
    "        sent_candit=item[0]\n",
    "        \n",
    "        if sent_candit==sent_target:\n",
    "            return item\n",
    "        \n",
    "def prediction_check(predict_score, threshold, ground):\n",
    "    if predict_score>threshold:\n",
    "        predict=\"1\"\n",
    "    else:\n",
    "        predict=\"0\"\n",
    "        \n",
    "    return predict==ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "starting-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2160968it [01:15, 28493.80it/s]\n",
      "2160968it [01:21, 26573.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding file\n",
    "cskg_tranE_embedding=load_embedding_gz(cskg_tranE_file)\n",
    "cskg_complex_embedding=load_embedding_gz(cskg_complex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e08b5fee-94e5-4042-ae74-295d3a1b9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4322096it [00:42, 101251.85it/s]\n"
     ]
    }
   ],
   "source": [
    "cskg_word_embeddings=load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "narrow-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build wordnet ontology\n",
    "# load wordnet\n",
    "# use wordnet build ontology to find blacklist related to person and animal\n",
    "wordnet_head,wordnet_lines=load_source(wordnet_file)\n",
    "wordnet_g=nx.DiGraph()\n",
    "\n",
    "id2label=defaultdict(set)\n",
    "#black_list={\"room.n.02\",'room.n.01',\"food.n.01\",'animal.n.01','person.n.01',\"people.n.01\",\"peoples.n.01\"}\n",
    "black_list={'animal.n.01','person.n.01',\"people.n.01\",\"peoples.n.01\"}\n",
    "for line in wordnet_lines:\n",
    "    if line[1]== \"/r/IsA\":\n",
    "        node1=line[0].split(\":\")[1]\n",
    "        node2=line[2].split(\":\")[1]\n",
    "        node1_labels=line[3].replace('\"',\"\").split(\"|\")\n",
    "        node2_labels=line[4].replace('\"',\"\").split(\"|\")\n",
    "        id2label[node1]=id2label[node1].union(node1_labels)\n",
    "        id2label[node2]=id2label[node2].union(node2_labels)\n",
    "        \n",
    "        if \".v.\" in node1 or \".v.\" in node1:\n",
    "            continue\n",
    "        wordnet_g.add_edge(node1,node2)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "new_black_list=set()\n",
    "# check all sub class of blacklist\n",
    "\n",
    "for item in black_list:\n",
    "    temp=[item]\n",
    "    new_black_list.add(item)\n",
    "    while temp:\n",
    "        new_temp=[]\n",
    "        for issue in temp:\n",
    "            new_temp+=[edge[0] for edge in wordnet_g.in_edges(issue)]\n",
    "            \n",
    "        new_temp_set=set(new_temp)\n",
    "        new_black_list=new_black_list.union(new_temp_set)\n",
    "        temp=new_temp_set\n",
    "        \n",
    "# check the blacklist label\n",
    "blacklist_label=set()\n",
    "\n",
    "for id_ in new_black_list:\n",
    "    blacklist_label=blacklist_label.union(id2label[id_])\n",
    "    \n",
    "blacklist_label.add(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54ee9f9-873d-4789-b6ad-a66927b1c89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blacklist_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b66ca37-999d-4ac3-b1a6-11a80d11e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_black_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9dcb400-dd12-4bdd-b336-6081e9066835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of blacklist_label\n",
    "# check whether girl in blacklist\n",
    "\n",
    "\"girl\" in blacklist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50694a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cskg_renamed file\n",
    "with gzip.open(cskg_file) as f:\n",
    "\n",
    "    head=f.readline()\n",
    "    cskg_renamed_dist=dict()\n",
    "    for line in f:\n",
    "        line=line.strip().decode(\"utf-8\").split(\"\\t\")\n",
    "        node1=line[1]\n",
    "        node2=line[3]\n",
    "\n",
    "        node1_label=line[4].split(\"|\")[0].replace('\"','')\n",
    "        node2_label=line[5].split(\"|\")[0].replace('\"','')\n",
    "        if node1_label != \"\":\n",
    "            cskg_renamed_dist[node1]=node1_label\n",
    "\n",
    "        if node2_label != \"\":\n",
    "            cskg_renamed_dist[node2]=node2_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-outside",
   "metadata": {},
   "source": [
    "## Case 2: Alternatives\n",
    "\n",
    "I went to the (Y) and I wanted to (X). There was no (Z), can I use (C) instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "284896c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file location\n",
    "\n",
    "data_part1=\"output/case2_1.gz\"\n",
    "data_part2=\"output/case2_2.gz\"\n",
    "\n",
    "case2_output='output/case2.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fancy-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 09:49:07 sqlstore]: IMPORT graph directly into table graph_1 from /Users/filipilievski/mcs/kg-bert/GameSentence/input/cskg_renamed.tsv.gz ...\n",
      "[2021-05-26 09:50:16 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c1.\"node1\" \"_aLias.plan\", graph_1_c4.\"node1\" \"_aLias.alt\", graph_1_c1.\"node2\" \"_aLias.goal\", graph_1_c3.\"node2\" \"_aLias.loc\"\n",
      "     FROM graph_1 AS graph_1_c1, graph_1 AS graph_1_c2, graph_1 AS graph_1_c3, graph_1 AS graph_1_c4\n",
      "     WHERE graph_1_c1.\"label\"=?\n",
      "     AND graph_1_c2.\"label\"=?\n",
      "     AND graph_1_c3.\"label\"=?\n",
      "     AND graph_1_c4.\"label\"=?\n",
      "     AND graph_1_c1.\"node1\"=graph_1_c2.\"node1\"\n",
      "     AND graph_1_c1.\"node2\"=graph_1_c4.\"node2\"\n",
      "     AND graph_1_c2.\"node2\"=graph_1_c3.\"node2\"\n",
      "     AND graph_1_c3.\"node1\"=graph_1_c4.\"node1\"\n",
      "     AND (graph_1_c4.\"node1\" != graph_1_c1.\"node1\")\n",
      "  PARAS: ['/r/CapableOf', '/r/AtLocation', '/r/AtLocation', '/r/CapableOf']\n",
      "---------------------------------------------\n",
      "[2021-05-26 09:50:16 sqlstore]: CREATE INDEX on table graph_1 column node2 ...\n",
      "[2021-05-26 09:50:22 sqlstore]: ANALYZE INDEX on table graph_1 column node2 ...\n",
      "[2021-05-26 09:50:22 sqlstore]: CREATE INDEX on table graph_1 column label ...\n",
      "[2021-05-26 09:50:25 sqlstore]: ANALYZE INDEX on table graph_1 column label ...\n",
      "[2021-05-26 09:50:26 sqlstore]: CREATE INDEX on table graph_1 column node1 ...\n",
      "[2021-05-26 09:50:28 sqlstore]: ANALYZE INDEX on table graph_1 column node1 ...\n",
      "[2021-05-26 09:50:44 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c2.\"node1\" \"_aLias.plan\", graph_1_c3.\"node1\" \"_aLias.alt\", graph_1_c4.\"node2\" \"_aLias.goal\", graph_1_c3.\"node2\" \"_aLias.loc\"\n",
      "     FROM graph_1 AS graph_1_c1, graph_1 AS graph_1_c2, graph_1 AS graph_1_c3, graph_1 AS graph_1_c4\n",
      "     WHERE graph_1_c1.\"label\"=?\n",
      "     AND graph_1_c2.\"label\"=?\n",
      "     AND graph_1_c3.\"label\"=?\n",
      "     AND graph_1_c4.\"label\"=?\n",
      "     AND graph_1_c1.\"node1\"=graph_1_c2.\"node1\"\n",
      "     AND graph_1_c1.\"node2\"=graph_1_c4.\"node2\"\n",
      "     AND graph_1_c2.\"node2\"=graph_1_c3.\"node2\"\n",
      "     AND graph_1_c3.\"node1\"=graph_1_c4.\"node1\"\n",
      "     AND (graph_1_c3.\"node1\" != graph_1_c2.\"node1\")\n",
      "  PARAS: ['/r/UsedFor', '/r/AtLocation', '/r/AtLocation', '/r/UsedFor']\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kgtk query --debug -i input/cskg_renamed.tsv.gz --match '(x)<-[:`/r/CapableOf`]-(z)-[:`/r/AtLocation`]->(y)<-[:`/r/AtLocation`]-(c)-[:`/r/CapableOf`]->(x)' --where 'c!=z' --return 'z as plan, c as alt, x as goal, y as loc' -o output/case2_1.gz\n",
    "kgtk query --debug -i input/cskg_renamed.tsv.gz --match '(x)<-[:`/r/UsedFor`]-(z)-[:`/r/AtLocation`]->(y)<-[:`/r/AtLocation`]-(c)-[:`/r/UsedFor`]->(x)' --where 'c!=z' --return 'z as plan, c as alt, x as goal, y as loc' -o output/case2_2.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "third-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load case2.txt file (the data file generated by kgtk)\n",
    "# output head and content\n",
    "f=gzip.open(data_part1)\n",
    "\n",
    "head=f.readline()\n",
    "content=[]\n",
    "for line in f:\n",
    "    content.append(line.strip().decode(\"utf-8\").split(\"\\t\"))\n",
    "    \n",
    "f=gzip.open(data_part2)\n",
    "\n",
    "head=f.readline()\n",
    "for line in f:\n",
    "    content.append(line.strip().decode(\"utf-8\").split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4f23b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate\n",
    "# content line format: [z, c, x, y]\n",
    "# for case2, [z,c,x,y] is the same as [c,z,x,y]\n",
    "content_set=set()\n",
    "\n",
    "for line in content:\n",
    "    z,c,x,y=line\n",
    "    \n",
    "    if (z,c,x,y) not in content_set and (c,z,x,y) not in content_set:\n",
    "        content_set.add((z,c,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae59fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 29926\n",
      "Number of lines after removing duplicate: 14949\n"
     ]
    }
   ],
   "source": [
    "# before removing\n",
    "print(f\"Number of lines: {len(content)}\")\n",
    "# after removing\n",
    "print(f\"Number of lines after removing duplicate: {len(content_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "gorgeous-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of location type: 687\n",
      "Number of chosen location: 96\n"
     ]
    }
   ],
   "source": [
    "# from content obtain the frequency of each location\n",
    "loc_distribution=dict()\n",
    "for line in content_set:\n",
    "    location=line[3]\n",
    "    \n",
    "    loc_distribution[location]=loc_distribution.get(location,0)+1\n",
    "print(\"Number of location type:\", len(loc_distribution))\n",
    "\n",
    "# only choose top 100 locations\n",
    "num_loc=100\n",
    "loc_sort=sorted(loc_distribution.items(),key=lambda k:k[1], reverse=True)\n",
    "loc_chosen=set([_[0] for _ in loc_sort[:num_loc]])\n",
    "# manual remove \n",
    "manual_remove_list=[\"/c/en/symphony\",\"/c/en/marching_band\",\"/c/en/band\",\"/c/en/orchestra\"]\n",
    "for item in manual_remove_list:\n",
    "    loc_chosen.remove(item)\n",
    "\n",
    "print(\"Number of chosen location:\", len(loc_chosen))\n",
    "# filter contenct by loc_chosen\n",
    "new_content=[]\n",
    "\n",
    "for line in content_set:\n",
    "    location=line[3]\n",
    "    if location in loc_chosen:\n",
    "        new_content.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "quantitative-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for all locations: 14949\n",
      "Number of lines for top 100 locations: 9733\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lines for all locations: {len(content_set)}\")\n",
    "print(f\"Number of lines for top 100 locations: {len(new_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "mobile-fusion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9733/9733 [01:24<00:00, 115.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# filter content (target about 20 lines)\n",
    "# I went to the (Y) and I wanted to (X). There was no (Z), can I use (C) instead?\n",
    "# z as plan, c as alt, x as goal, y as loc\n",
    "\n",
    "\"\"\"\n",
    "Filter:\n",
    "Find similarity between Z and C\n",
    "filter sentence if leve similarity>0.6, jaccard similarity>0.6, and word synsets\n",
    "\"\"\"\n",
    "filter_content=[]\n",
    "leve_threshold=0.6\n",
    "jaccard_threshold=0.6\n",
    "for i in tqdm(range(len(new_content))):\n",
    "    line = new_content[i]\n",
    "    z_label=cskg_renamed_dist[line[0]]\n",
    "    c_label=cskg_renamed_dist[line[1]]\n",
    "    x_label=cskg_renamed_dist[line[2]]\n",
    "    \n",
    "    leve_sim=rltk.levenshtein_similarity(z_label,c_label)\n",
    "    jaccard_sim=rltk.hybrid_jaccard_similarity(set(z_label.split(\" \")),set(c_label.split(\" \")),\n",
    "                                                   function=rltk.levenshtein_similarity)\n",
    "    \n",
    "    if leve_sim>=leve_threshold or jaccard_sim>=jaccard_threshold:\n",
    "        continue\n",
    "    \n",
    "    # filter line by X, POS should be (verb)\n",
    "    tokens=text2token(x_label)\n",
    "    \n",
    "    if len(tokens)==0:\n",
    "        continue\n",
    "        \n",
    "    token1=tokens[0]\n",
    "    z_token=text2token(line[0].split(\"/\")[-1])[0]\n",
    "    c_token=text2token(line[1].split(\"/\")[-1])[0]\n",
    "    y_label=line[-1].split(\"/\")[-1]\n",
    "    \n",
    "    if z_token.lemma_==c_token.lemma_:\n",
    "        continue\n",
    "    \n",
    "    # filter token like \"cat\", \"dog\", \"woman\".....\n",
    "    if z_token.lemma_ in blacklist_label or c_token.lemma_ in blacklist_label:\n",
    "        continue\n",
    "    \n",
    "    # filter \"theater\" and \"movie theater\"\n",
    "    if z_token.lemma_ in c_token.lemma_ or c_token.lemma_ in z_token.lemma_:\n",
    "        continue\n",
    "    \n",
    "#     z_embed=np.array(eval(\"[\"+cskg_word_embeddings[line[0]]+\"]\"))\n",
    "#     c_embed=np.array(eval(\"[\"+cskg_word_embeddings[line[1]]+\"]\"))\n",
    "#     cosine_sim=synsets_cosine_sim(z_label,c_label)\n",
    "#     if cosine_sim>=0.9:\n",
    "#         continue\n",
    "        \n",
    "    if token1.pos_==\"VERB\":\n",
    "        filter_content.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a1e1aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines before filtering: 9733\n",
      "Number of lines after filtering: 4643\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lines before filtering: {len(new_content)}\")\n",
    "print(f\"Number of lines after filtering: {len(filter_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cellular-joyce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4643/4643 [00:16<00:00, 280.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate sentence based on the generation filter line\n",
    "# Sentence format: I went to the (Y) and I wanted to (X). There was no (Z), can I use (C) instead?\n",
    "location_sents=defaultdict(set)\n",
    "\n",
    "\"\"\"\n",
    "output: location_sents\n",
    "output format:{location_id:[sentence,\n",
    "                            Node Embedding Similarity,\n",
    "                            transE Embedding similarity,\n",
    "                            complex similarity,\n",
    "                            Z label name,\n",
    "                            C label name]}\n",
    "\"\"\"\n",
    "possible_items=set()\n",
    "with open(case2_output, 'w') as w:\n",
    "    for line in tqdm(filter_content):\n",
    "        z_label=cskg_renamed_dist[line[0]]\n",
    "        c_label=cskg_renamed_dist[line[1]]\n",
    "        x_label=cskg_renamed_dist[line[2]]\n",
    "        x_label = \" \".join([_.lemma_ for _ in text2token(x_label)])\n",
    "        y_label=cskg_renamed_dist[line[3]]\n",
    "        possible_items.add(c_label)\n",
    "        sent=f\"I went to the {y_label} and I wanted to {x_label}. There was no {z_label}, can I use {c_label} instead?\"\n",
    "        w.write(json.dumps({'story': sent, 'answer': 'Y'}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f19a38aa-d3e4-44ef-ae7f-21f0ffafc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subway platform\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(possible_items, 1)[0])\n",
    "print(len(possible_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "450de7c3-1dc7-41fe-9d0d-2605cf0abc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4643/4643 [00:15<00:00, 298.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Find negative cases\n",
    "import random\n",
    "with open(case2_output, 'a') as w:\n",
    "    for line in tqdm(filter_content):\n",
    "        z_label=cskg_renamed_dist[line[0]]\n",
    "        x_label=cskg_renamed_dist[line[2]]\n",
    "        x_label = \" \".join([_.lemma_ for _ in text2token(x_label)])\n",
    "        y_label=cskg_renamed_dist[line[3]]\n",
    "        c_label=random.sample(possible_items, 1)[0]\n",
    "        if c_label!=y_label and c_label!=cskg_renamed_dist[line[1]]:\n",
    "            sent=f\"I went to the {y_label} and I wanted to {x_label}. There was no {z_label}, can I use {c_label} instead?\"\n",
    "            w.write(json.dumps({'story': sent, 'answer': 'N'}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-education",
   "metadata": {},
   "source": [
    "## case 1: Unmet expectations\n",
    "I went to the Y. There was a X but it had no Z. Am I disappointed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3b12791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file location\n",
    "\n",
    "data_case1=\"output/case1.gz\"\n",
    "\n",
    "case1_output='output/case1.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "latter-messaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 10:16:55 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c1.\"node1\" \"_aLias.needed\", graph_1_c1.\"node2\" \"_aLias.place\", graph_1_c2.\"node2\" \"_aLias.Y\"\n",
      "     FROM graph_1 AS graph_1_c1, graph_1 AS graph_1_c2\n",
      "     WHERE graph_1_c1.\"label\"=?\n",
      "     AND graph_1_c2.\"label\"=?\n",
      "     AND graph_1_c1.\"node2\"=graph_1_c2.\"node1\"\n",
      "  PARAS: ['/r/AtLocation', '/r/AtLocation']\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kgtk query --debug -i input/cskg_renamed.tsv.gz --match '(z)-[:`/r/AtLocation`]->(x)-[:`/r/AtLocation`]->(y)' --return 'z as needed, x as place, y as Y' -o output/case1.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "challenging-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kgtk file\n",
    "with gzip.open(data_case1) as f:\n",
    "    head_case1=f.readline()\n",
    "    content_case1=[]\n",
    "    for line in f:\n",
    "        content_case1.append(line.strip().decode(\"utf-8\").split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "occupied-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate\n",
    "# content line format: [z, x, y]\n",
    "content_case1_set=set()\n",
    "\n",
    "for line in content_case1:\n",
    "    z,x,y=line\n",
    "    \n",
    "    if (z,x,y) not in content_case1_set and (z,x,y) not in content_case1_set:\n",
    "        content_case1_set.add((z,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52d5deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 121148\n",
      "Number of lines after removing duplicate: 121148\n"
     ]
    }
   ],
   "source": [
    "# before removing\n",
    "print(f\"Number of lines: {len(content_case1)}\")\n",
    "# after removing\n",
    "print(f\"Number of lines after removing duplicate: {len(content_case1_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "biological-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121148/121148 [00:00<00:00, 2280130.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of location type: 3064\n",
      "Number of chosen location: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from content obtain the frequency of each location\n",
    "loc_distribution=dict()\n",
    "for line in content_case1_set:\n",
    "    location=line[2]\n",
    "    \n",
    "    loc_distribution[location]=loc_distribution.get(location,0)+1\n",
    "\n",
    "print(\"Number of location type:\", len(loc_distribution))\n",
    "\n",
    "# only choose top 100 locations\n",
    "num_loc=100\n",
    "print(\"Number of chosen location:\", num_loc)\n",
    "loc_sort=sorted(loc_distribution.items(),key=lambda k:k[1], reverse=True)\n",
    "loc_chosen=set([_[0] for _ in loc_sort[:num_loc]])\n",
    "\n",
    "# filter contenct by loc_chosen\n",
    "new_content_case1=[]\n",
    "\n",
    "for line in tqdm(content_case1):\n",
    "    location=line[2]\n",
    "    if location in loc_chosen:\n",
    "        new_content_case1.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "decreased-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for all locations: 121148\n",
      "Number of lines for top 100 locations: 49966\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lines for all locations: {len(content_case1_set)}\")\n",
    "print(f\"Number of lines for top 100 locations: {len(new_content_case1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "demonstrated-grant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49966/49966 [04:55<00:00, 169.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# I went to the {y_label}. There was a {x_label} but it had no {z_label}. Am I disappointed?\n",
    "# filter by sentiment analysis and cosine similarity\n",
    "\n",
    "\"\"\"\n",
    "Output:\n",
    "filter_content_case1: filtered x,y,z id, [[z1,x1,y1],[z2,x2,y2],[z3,x3,y3]] \n",
    "loc_line:{location_id:[sentence,\n",
    "                        node bert embedding similarity,\n",
    "                        tranE Embedding similarity,\n",
    "                        complex embedding similarity,\n",
    "                        z label name,\n",
    "                        x label name]}\n",
    "\"\"\"\n",
    "filter_content_case1=[]\n",
    "loc_line=defaultdict(set)\n",
    "\n",
    "with open(data_case1, 'w') as w:\n",
    "    for line in tqdm(new_content_case1):\n",
    "        pos_sent=\"\"\n",
    "        neg_sent=\"\"\n",
    "        z_label=cskg_renamed_dist[line[0]]\n",
    "        x_label=cskg_renamed_dist[line[1]]\n",
    "        y_label=cskg_renamed_dist[line[2]]\n",
    "        if z_label==x_label:\n",
    "            continue\n",
    "        x_tokens=text2token(x_label)\n",
    "        tokens=text2token(z_label)\n",
    "        sent=f\"I went to the {y_label}. There was a {x_label} but it had no {z_label}. Am I disappointed?\"\n",
    "\n",
    "        polar_z=sia.polarity_scores(z_label)\n",
    "        polar_x=sia.polarity_scores(x_label)\n",
    "        if polar_z['neg']>0 or polar_x['neg']>0:\n",
    "            neg_sent=sent\n",
    "        else:\n",
    "            pos_sent=sent\n",
    "                \n",
    "        if x_label in blacklist_label:\n",
    "            continue\n",
    "\n",
    "        if [_.lemma_ for _ in x_tokens]==[_.lemma_ for _ in tokens]:\n",
    "            continue\n",
    "\n",
    "        if pos_sent:\n",
    "            w.write(json.dumps({'story': pos_sent, 'answer': 'Y'}) + '\\n')\n",
    "        elif neg_sent:\n",
    "            w.write(json.dumps({'story': pos_sent, 'answer': 'N'}) + '\\n')\n",
    "\n",
    "\n",
    "        filter_content_case1.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34ab6850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines before filter: 49966\n",
      "Number of lines after filter: 46817\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lines before filter: {len(new_content_case1)}\")\n",
    "print(f\"Number of lines after filter: {len(filter_content_case1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac871f5a-cce1-440a-84c3-fcb74f9a92a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/c/en/00t_shirts', '/c/en/drawer', '/c/en/den']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "c? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/c/en/00t_shirts', '/c/en/drawer', '/c/en/kitchen']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "c? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/c/en/32_teeth', '/c/en/mouth', '/c/en/river']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "c? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/c/en/abandoned_tractor', '/c/en/meadow', '/c/en/countryside']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "c? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/c/en/address_label', '/c/en/drawer', '/c/en/den']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-c3c8da179549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter_content_case1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/scenegen/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/scenegen/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for n in new_content_case1:\n",
    "    if n not in filter_content_case1:\n",
    "        print(n)\n",
    "        input('c?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-outside",
   "metadata": {},
   "source": [
    "## Case3: Object modifications\n",
    "There was `X` in the `Y`. What can it do to the `Z1n`? `Z1v` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0ce82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file location\n",
    "\n",
    "data_case31=\"output/case3_1.gz\"\n",
    "data_case32=\"output/case3_2.gz\"\n",
    "\n",
    "\n",
    "case3_output='output/case3.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "brown-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-26 10:47:42 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c1.\"node2\" \"_aLias.needed\", graph_1_c1.\"node1\" \"_aLias.place\", graph_1_c2.\"node2\" \"_aLias.Y\"\n",
      "     FROM graph_1 AS graph_1_c1, graph_1 AS graph_1_c2\n",
      "     WHERE graph_1_c1.\"label\"=?\n",
      "     AND graph_1_c2.\"label\"=?\n",
      "     AND graph_1_c1.\"node1\"=graph_1_c2.\"node1\"\n",
      "  PARAS: ['/r/UsedFor', '/r/AtLocation']\n",
      "---------------------------------------------\n",
      "[2021-05-26 10:47:44 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c1.\"node2\" \"_aLias.needed\", graph_1_c2.\"node1\" \"_aLias.place\", graph_1_c2.\"node2\" \"_aLias.Y\"\n",
      "     FROM graph_1 AS graph_1_c1, graph_1 AS graph_1_c2\n",
      "     WHERE graph_1_c1.\"label\"=?\n",
      "     AND graph_1_c2.\"label\"=?\n",
      "     AND graph_1_c1.\"node1\"=graph_1_c2.\"node1\"\n",
      "  PARAS: ['/r/CapableOf', '/r/AtLocation']\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kgtk query --debug -i input/cskg_renamed.tsv.gz --match '(z)<-[:`/r/UsedFor`]-(x)-[:`/r/AtLocation`]->(y)' --return 'z as needed, x as place, y as Y' -o output/case3_2.gz\n",
    "kgtk query --debug -i input/cskg_renamed.tsv.gz --match '(z)<-[:`/r/CapableOf`]-(x)-[:`/r/AtLocation`]->(y)' --return 'z as needed, x as place, y as Y' -o output/case3_1.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sensitive-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kgtk file\n",
    "with gzip.open(data_case31) as f:\n",
    "    head_case1=f.readline()\n",
    "    content_case3=[]\n",
    "    for line in f:\n",
    "        content_case3.append(line.strip().decode(\"utf-8\").split(\"\\t\"))\n",
    "    \n",
    "with gzip.open(data_case32) as f:\n",
    "\n",
    "    head_case1=f.readline()\n",
    "    content_case3=[]\n",
    "    for line in f:\n",
    "        content_case3.append(line.strip().decode(\"utf-8\").split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0d4bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate\n",
    "# content line format: [z, x, y]\n",
    "content_case3_set=set()\n",
    "\n",
    "for line in content_case3:\n",
    "    z,x,y=line\n",
    "    \n",
    "    if (z,x,y) not in content_case3_set and (z,x,y) not in content_case3_set:\n",
    "        content_case3_set.add((z,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fc94ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 214878\n",
      "Number of lines after removing duplicate: 214878\n"
     ]
    }
   ],
   "source": [
    "# before removing\n",
    "print(f\"Number of lines: {len(content_case3)}\")\n",
    "# after removing\n",
    "print(f\"Number of lines after removing duplicate: {len(content_case3_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "accepting-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of location type: 5166\n",
      "Number of chosen location: 100\n"
     ]
    }
   ],
   "source": [
    "# from content obtain the frequency of each location\n",
    "loc_distribution=dict()\n",
    "for line in content_case3:\n",
    "    location=line[2]\n",
    "    \n",
    "    loc_distribution[location]=loc_distribution.get(location,0)+1\n",
    "\n",
    "print(\"Number of location type:\", len(loc_distribution))\n",
    "\n",
    "# only choose top 100 locations\n",
    "num_loc=100\n",
    "print(\"Number of chosen location:\", num_loc)    \n",
    "\n",
    "loc_sort=sorted(loc_distribution.items(),key=lambda k:k[1], reverse=True)\n",
    "loc_chosen=set([_[0] for _ in loc_sort[:num_loc]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "objective-jason",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214878/214878 [00:00<00:00, 1770613.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# filter contenct by loc_chosen\n",
    "new_content_case3=[]\n",
    "\n",
    "for line in tqdm(content_case3_set):\n",
    "    location=line[2]\n",
    "    if location in loc_chosen:\n",
    "        new_content_case3.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "freelance-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for all locations: 214878\n",
      "Number of lines for top 100 locations: 69509\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lines for all locations: {len(content_case3_set)}\")\n",
    "print(f\"Number of lines for top 100 locations: {len(new_content_case3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "strong-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69509/69509 [05:03<00:00, 229.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# filter content\n",
    "# z as plan, c as alt, x as goal, y as loc\n",
    "\"\"\"\n",
    "Output:\n",
    "loc_line:{location_id:[sentence,\n",
    "                        node bert embedding similarity,\n",
    "                        tranE Embedding similarity,\n",
    "                        complex embedding similarity,\n",
    "                        y label name,\n",
    "                        z label name]}\n",
    "\"\"\"\n",
    "loc_line_case3=defaultdict(set)\n",
    "with open(case3_output, 'w') as w:\n",
    "    for i in tqdm(range(len(new_content_case3))):\n",
    "        line=new_content_case3[i]\n",
    "\n",
    "        y_id=line[2]\n",
    "        x_id=line[1]\n",
    "        z_id=line[0]\n",
    "        x_label=cskg_renamed_dist[x_id]\n",
    "        y_label=cskg_renamed_dist[y_id]\n",
    "        z_label=cskg_renamed_dist[z_id]\n",
    "        # filter line by X, POS should be (verb)\n",
    "        z_tokens=text2token(z_label)\n",
    "\n",
    "        if len(z_tokens)<2:\n",
    "            continue\n",
    "        else:\n",
    "            zv=z_tokens[0]\n",
    "            zn_tokens=z_tokens[1:]\n",
    "\n",
    "            y_embed=np.array(eval(\"[\"+cskg_word_embeddings[y_id]+\"]\"))\n",
    "            zn_text=\" \".join([_.lemma_ for _ in zn_tokens])\n",
    "            zn_id=\"/c/en/\"+zn_text.replace(\" \",\"_\")\n",
    "\n",
    "    #         # find embed.\n",
    "    #         z_embed=np.array(eval(\"[\"+cskg_word_embeddings[z_id]+\"]\"))\n",
    "\n",
    "    #         z_embed_transE=cskg_tranE_embedding[line[0]]\n",
    "    #         y_embed_transE=cskg_tranE_embedding[line[1]]\n",
    "\n",
    "    #         z_embed_complex=cskg_complex_embedding[line[0]]\n",
    "    #         y_embed_complex=cskg_complex_embedding[line[1]]\n",
    "\n",
    "    #         cskg_similar=cosine_similar(z_embed,y_embed)\n",
    "    #         tranE_similar=cosine_similar(z_embed_transE,y_embed_transE)\n",
    "    #         complex_similar=cosine_similar(z_embed_complex,y_embed_complex)\n",
    "\n",
    "            # remove item in blacklist\n",
    "            if x_label in blacklist_label or zn_text in blacklist_label:\n",
    "                continue\n",
    "\n",
    "            # remove same item\n",
    "            if x_label==zn_text:\n",
    "                continue\n",
    "\n",
    "            # only verb noun structure are allowed\n",
    "            if zv.pos_ == \"VERB\" and all([_.pos_==\"NOUN\" for _ in zn_tokens]):\n",
    "\n",
    "                temp=loc_line_case3[y_id]\n",
    "                sent=f\"There was {x_label} in the {y_label}. What can it do to the {zn_text}?\"\n",
    "                w.write(json.dumps({'story': sent, 'answer': f'{zv.lemma_} it.'}) + '\\n')\n",
    "    #             temp.add((sent,cskg_similar,tranE_similar,complex_similar,y_label,z_label))\n",
    "    #             loc_line_case3[y_id]=temp\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aab96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
