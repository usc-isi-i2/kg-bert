{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bert_link_prediction import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pickle\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "data_path = \"./data/wn\"\n",
    "data_saved_path = \"./output_wn_result\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=50\n",
    "eval_batch_size=1500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precessor\n",
    "processors = {\"kg\": KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = [\"0\",\"1\"]\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load All data\n",
    "train_triples = processor.get_train_triples(data_path)\n",
    "dev_triples = processor.get_dev_triples(data_path)\n",
    "test_triples = processor.get_test_triples(data_path)\n",
    "all_triples = train_triples + dev_triples + test_triples\n",
    "\n",
    "all_triples_str_set = set()\n",
    "for triple in all_triples:\n",
    "    triple_str = '\\t'.join(triple)\n",
    "    all_triples_str_set.add(triple_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kg-Bert Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_new_examples(lines, corup_loc = \"head\",set_type=\"test\",data_dir=data_path, remove_entity=False,entity_list=entity_list):\n",
    "    #use NLTK WorfNet to Build Corrupted Lines for Prediction\n",
    "    ent2text = {}\n",
    "    with open(os.path.join(data_dir, \"entity2text.txt\"), 'r') as f:\n",
    "        ent_lines = f.readlines()\n",
    "        for line in ent_lines:\n",
    "            temp = line.strip().split('\\t')\n",
    "            if len(temp) == 2:\n",
    "                end = temp[1]#.find(',')\n",
    "                ent2text[temp[0]] = temp[1]#[:end]\n",
    "\n",
    "    if data_dir.find(\"FB15\") != -1:\n",
    "        with open(os.path.join(data_dir, \"entity2textlong.txt\"), 'r') as f:\n",
    "            ent_lines = f.readlines()\n",
    "            for line in ent_lines:\n",
    "                temp = line.strip().split('\\t')\n",
    "                #first_sent_end_position = temp[1].find(\".\")\n",
    "                ent2text[temp[0]] = temp[1]#[:first_sent_end_position + 1] \n",
    "\n",
    "    entities = list(ent2text.keys())\n",
    "\n",
    "    rel2text = {}\n",
    "    with open(os.path.join(data_dir, \"relation2text.txt\"), 'r') as f:\n",
    "        rel_lines = f.readlines()\n",
    "        for line in rel_lines:\n",
    "            temp = line.strip().split('\\t')\n",
    "            rel2text[temp[0]] = temp[1]      \n",
    "\n",
    "    lines_str_set = set(['\\t'.join(line) for line in lines])\n",
    "    examples = []\n",
    "    \n",
    "    corrupt_list=[]\n",
    "    \n",
    "    for (i, line) in enumerate(lines):\n",
    "        label = \"1\"\n",
    "        #print(lines)\n",
    "        \n",
    "        ent_a = line[0]\n",
    "        ent_b = line[1]\n",
    "        ent_c = line[2]\n",
    "        head_ent_text = ent2text[line[0]]\n",
    "        tail_ent_text = ent2text[line[2]]\n",
    "        relation_text = rel2text[line[1]]\n",
    "        text_a = head_ent_text\n",
    "        text_b = relation_text\n",
    "        text_c = tail_ent_text\n",
    "        \n",
    "        corrupt_list.append(line)\n",
    "        \n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, text_c = text_c, label=label))\n",
    "        \n",
    "        ent_a_modify = ent_a.split(\":\")[1].split(\".\")[0]\n",
    "        ent_c_modify = ent_c.split(\":\")[1].split(\".\")[0]\n",
    "        \n",
    "        if corup_loc == \"head\":\n",
    "                text_a_candits = wn.synsets(ent_a_modify)\n",
    "                for item in text_a_candits:\n",
    "                    if item.name() == ent_a.split(\":\")[1]:\n",
    "                        continue\n",
    "                        \n",
    "                    if remove_entity and item.name() not in entity_list:\n",
    "                        continue\n",
    "                    text_a = item.definition()\n",
    "                    \n",
    "                    corrupt_list.append([\"wn:\"+item.name(),ent_b,ent_c])\n",
    "                    \n",
    "                    examples.append(\n",
    "                        InputExample(guid=guid, text_a=text_a, text_b=text_b, text_c = text_c, label=\"0\"))\n",
    "        \n",
    "        if corup_loc == \"tail\":\n",
    "                text_c_candits = wn.synsets(ent_c_modify)\n",
    "                for item in text_c_candits:\n",
    "                    if item.name() == ent_c.split(\":\")[1]:\n",
    "                        continue\n",
    "                        \n",
    "                    if remove_entity and item.name() not in entity_list:\n",
    "                        continue\n",
    "                        \n",
    "                    text_c = item.definition()\n",
    "                    corrupt_list.append([ent_a,ent_b,\"wn:\"+item.name()])\n",
    "                    \n",
    "                    examples.append(\n",
    "                        InputExample(guid=guid, text_a=text_a, text_b=text_b, text_c = text_c, label=\"0\"))\n",
    "\n",
    "    return examples,corrupt_list\n",
    "\n",
    "def rank_accuracy(ranks):\n",
    "    # check the accuracy for different hits\n",
    "    max_dep = 1\n",
    "    threshold = 0\n",
    "    accuracy_dict = dict()\n",
    "    ite = 0\n",
    "    \n",
    "    while threshold <= max_dep and ite <500:\n",
    "        for rank in ranks:\n",
    "            max_dep = max(max_dep, rank)\n",
    "            if rank <= threshold:\n",
    "                accuracy_dict[threshold] = accuracy_dict.get(threshold,0)+1\n",
    "                \n",
    "        accuracy_dict[threshold] = accuracy_dict[threshold]/len(ranks)\n",
    "        threshold += 1\n",
    "        ite += 1\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "def kg_bert_prediction(triples):\n",
    "    ranks_left = []\n",
    "    ranks_right = []\n",
    "    count = 0\n",
    "    for test_triple in tqdm(triples):\n",
    "        head = test_triple[0]\n",
    "        relation = test_triple[1]\n",
    "        tail = test_triple[2]\n",
    "\n",
    "        head_corrupt_list = [test_triple]\n",
    "        temp_examples, head_corrupt_list = _create_new_examples(head_corrupt_list, corup_loc = \"head\")\n",
    "        temp_features = convert_examples_to_features(temp_examples, label_list, max_seq_length, tokenizer, print_info = False)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in temp_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in temp_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in temp_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in temp_features], dtype=torch.long)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # Run prediction for temp data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = preds[0]\n",
    "\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = torch.tensor(rel_values)\n",
    "\n",
    "        _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "\n",
    "        argsort1 = argsort1.cpu().numpy()\n",
    "        rank1 = np.where(argsort1 == 0)[0][0]\n",
    "\n",
    "        ranks_left.append(rank1)\n",
    "\n",
    "        # build corrupted tail\n",
    "        head_corrupt_list = [test_triple]\n",
    "        temp_examples,head_corrupt_list = _create_new_examples(head_corrupt_list, corup_loc = \"tail\")\n",
    "        temp_features = convert_examples_to_features(temp_examples, label_list, max_seq_length, tokenizer, print_info = False)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in temp_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in temp_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in temp_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in temp_features], dtype=torch.long)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # Run prediction for temp data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = preds[0]\n",
    "\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = torch.tensor(rel_values)\n",
    "\n",
    "        _, argsort2 = torch.sort(rel_values, descending=True)\n",
    "\n",
    "        argsort2 = argsort2.cpu().numpy()\n",
    "        rank2 = np.where(argsort2 == 0)[0][0]\n",
    "\n",
    "        ranks_right.append(rank2)\n",
    "    return ranks_left, ranks_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK Train Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [11:26<00:00, 11.65it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_left_train1, rank_right_train1 = kg_bert_prediction(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_train1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_train1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK DEV Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 12.03it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_left_dev1, rank_right_dev1 = kg_bert_prediction(dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_dev1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_dev1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK TEST Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:32<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_left_test1, rank_right_test1 = kg_bert_prediction(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.884"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_test1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_test1)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kg_Bert II Link Prediction\n",
    "\n",
    "Filter the training data set and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saved_path = \"./output_wn_result2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK Train Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [15:33<00:00,  8.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "ranks_left_train2, rank_right_train2 = kg_bert_prediction(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9045"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_train2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821125"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_train2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK DEV Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:32<00:00, 10.82it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_left_dev2, rank_right_dev2 = kg_bert_prediction(dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_dev2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_dev2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet NLTK TEST Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:30<00:00, 11.05it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_left_test2, rank_right_test2 = kg_bert_prediction(test_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_left = rank_accuracy(ranks_left_test2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_right = rank_accuracy(rank_right_test2)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Output&Input example for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:27<00:00, 11.41it/s]\n"
     ]
    }
   ],
   "source": [
    "triples = test_triples\n",
    "ranks_left = []\n",
    "ranks_right = []\n",
    "count = 0\n",
    "res= []\n",
    "with open(\"wn_TestResult_head.txt\",\"w\") as f1, open(\"wn_TestResult_tail.txt\",\"w\") as f2:\n",
    "    for test_triple in tqdm(triples):\n",
    "        head = test_triple[0]\n",
    "        relation = test_triple[1]\n",
    "        tail = test_triple[2]\n",
    "        # write the actuacl triple to predict\n",
    "        f1.write(\"The Actual Triple to Test: \"+\"\\t\".join(test_triple)+\"\\n\")\n",
    "        f1.write(\"\\n\")\n",
    "        head_corrupt_list = [test_triple]\n",
    "        \n",
    "        temp_examples,head_corrupt_list = _create_new_examples(head_corrupt_list, corup_loc = \"head\")\n",
    "        temp_features = convert_examples_to_features(temp_examples, label_list, max_seq_length, tokenizer, print_info = False)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in temp_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in temp_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in temp_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in temp_features], dtype=torch.long)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # Run prediction for temp data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = preds[0]\n",
    "        res.append(preds)\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = torch.tensor(rel_values)\n",
    "        \n",
    "        f1.write(\"-----Prediction Detail-----\\n\")\n",
    "        for idx in range(len(rel_values)):\n",
    "            example_ = temp_examples[idx]\n",
    "            feature_ = temp_features[idx]\n",
    "            input_ids = all_input_ids[idx].cpu().numpy()\n",
    "            input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            predict_text = \"\\t\".join(head_corrupt_list[idx])\n",
    "            f1.write(\"Triple_to_Predict: {}\\n\".format(predict_text))\n",
    "            \n",
    "            #text of triples:\n",
    "            f1.write(\"Text of Triples:{a}\\t{b}\\t{c}\\n\".format(a=example_.text_a,\n",
    "                                                          b=example_.text_b,\n",
    "                                                          c=example_.text_c))\n",
    "            \n",
    "            # input tokens& ids\n",
    "            f1.write(\"Input_Tokens: {}\\n\".format(\",\".join(input_tokens)))\n",
    "            f1.write(\"Input_Ids: {}\\n\".format(\",\".join([str(_) for _ in input_ids])))\n",
    "            f1.write(\"Label_id: {}\\n\".format(label_ids[idx]))\n",
    "            f1.write(\"\\n\")\n",
    "            #output_score\n",
    "            f1.write(\"Score of This Triple: {}\\n\".format(rel_values[idx]))\n",
    "            f1.write(\"\\n\")\n",
    "        f1.write(\"-----Prediction Final Result-----\\n\")\n",
    "        _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "\n",
    "        argsort1 = argsort1.cpu().numpy()\n",
    "        rank1 = np.where(argsort1 == 0)[0][0]\n",
    "        \n",
    "        f1.write(\"The rank of correct answer in the candidates list: {}\\n\".format(rank1))\n",
    "        f1.write(\"The Predicted Triples:{}\".format(head_corrupt_list[argsort1[0]]))\n",
    "        f1.write(\"\\n\")\n",
    "        f1.write(\"############################################################\\n\")\n",
    "        ranks_left.append(rank1)\n",
    "        ############################################################################################\n",
    "        # write the actuacl triple to predict\n",
    "        f2.write(\"The Actual Triple to Test: \"+\"\\t\".join(test_triple)+\"\\n\")\n",
    "        f2.write(\"\\n\")\n",
    "        tail_corrupt_list = [test_triple]\n",
    "        \n",
    "        temp_examples,tail_corrupt_list = _create_new_examples(tail_corrupt_list, corup_loc = \"tail\")\n",
    "        temp_features = convert_examples_to_features(temp_examples, label_list, max_seq_length, tokenizer, print_info = False)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in temp_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in temp_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in temp_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in temp_features], dtype=torch.long)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # Run prediction for temp data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = preds[0]\n",
    "        res.append(preds)\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = torch.tensor(rel_values)\n",
    "        \n",
    "        f2.write(\"-----Prediction Detail-----\\n\")\n",
    "        for idx in range(len(rel_values)):\n",
    "            example_ = temp_examples[idx]\n",
    "            feature_ = temp_features[idx]\n",
    "            input_ids = all_input_ids[idx].cpu().numpy()\n",
    "            input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            predict_text = \"\\t\".join(tail_corrupt_list[idx])\n",
    "            f2.write(\"Triple_to_Predict: {}\\n\".format(predict_text))\n",
    "            \n",
    "            #text of triples:\n",
    "            f2.write(\"Text of Triples:{a}\\t{b}\\t{c}\\n\".format(a=example_.text_a,\n",
    "                                                          b=example_.text_b,\n",
    "                                                          c=example_.text_c))\n",
    "            \n",
    "            # input tokens& ids\n",
    "            f2.write(\"Input_Tokens: {}\\n\".format(\",\".join(input_tokens)))\n",
    "            f2.write(\"Input_Ids: {}\\n\".format(\",\".join([str(_) for _ in input_ids])))\n",
    "            f2.write(\"Label_id: {}\\n\".format(label_ids[idx]))\n",
    "            f2.write(\"\\n\")\n",
    "            #output_score\n",
    "            f2.write(\"Score of This Triple: {}\\n\".format(rel_values[idx]))\n",
    "            f2.write(\"\\n\")\n",
    "        f2.write(\"-----Prediction Final Result-----\\n\")\n",
    "        _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "\n",
    "        argsort1 = argsort1.cpu().numpy()\n",
    "        rank1 = np.where(argsort1 == 0)[0][0]\n",
    "        \n",
    "        f2.write(\"The rank of correct answer in the candidates list: {}\\n\".format(rank1))\n",
    "        f2.write(\"The Predicted Triples:{}\".format(tail_corrupt_list[argsort1[0]]))\n",
    "        f2.write(\"\\n\")\n",
    "        f2.write(\"############################################################\\n\")\n",
    "        ranks_left.append(rank1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_create_new_examples(tail_corrupt_list, corup_loc = \"tail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
