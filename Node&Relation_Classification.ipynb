{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import random, csv\n",
    "\n",
    "import run_bert_relation_prediction as relation_prediction\n",
    "import run_bert_link_prediction as link_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file\n",
    "cskg_file = \"./data/cskg/cskg_connected.tsv\"\n",
    "\n",
    "# output file\n",
    "cskg_relation_prediction = \"./data/cskg/relation_prediction.tsv\"\n",
    "cskg_link_prediction = \"./data/cskg/link_prediction.tsv\"\n",
    "cskg_100 = \"./data/cskg/cskg_100.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename,encoding=None,errors=None):\n",
    "    # load data file\n",
    "    with open(filename,\"r\",encoding=encoding, errors=errors) as f:\n",
    "        head_str = f.readline()\n",
    "        head=head_str.split(\"\\t\")\n",
    "        \n",
    "        lines = []\n",
    "        for line_str in f:\n",
    "            line_str=line_str.strip()\n",
    "            \n",
    "            if line_str:\n",
    "                line=line_str.split(\"\\t\")\n",
    "\n",
    "                lines.append(line)\n",
    "            \n",
    "    return head,lines\n",
    "\n",
    "def line_filter(lines, label_=\"/r/HasProperty\"):\n",
    "    #\n",
    "    res = []\n",
    "    \n",
    "    for line in lines:\n",
    "        entity1 = line[1]\n",
    "        entity2 = line[3]\n",
    "        relation = line[2]\n",
    "        \n",
    "        entity1_label=line[4]\n",
    "        entity2_label=line[5]\n",
    "        relation_label=line[6]\n",
    "        \n",
    "        if line[2] == label_:\n",
    "            # has relation label hasproperty\n",
    "            \n",
    "            # subject 2+ candidates\n",
    "            # object 1+ candidates\n",
    "            \n",
    "            entity1_synsets = wn.synsets(entity1_label.replace(\" \",\"_\"))\n",
    "            entity2_synsets = wn.synsets(entity2_label.replace(\" \",\"_\"))\n",
    "            #res.append(line)\n",
    "            \n",
    "            if len(entity1_synsets) >= 2 and len(entity2_synsets)>=1:\n",
    "                res.append(line)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def write_file(filename,lines,encoding=None,errors=None):\n",
    "    # write data file\n",
    "    with open(filename,\"w\",encoding=encoding, errors=errors) as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(lines)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "head, lines=load_file(cskg_file,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/en/about_ten_percent_of_people-/r/HasProperty-/c/en/left_handed-0000',\n",
       " '/c/en/about_ten_percent_of_people',\n",
       " '/r/HasProperty',\n",
       " '/c/en/left_handed',\n",
       " 'about ten percent of people',\n",
       " 'left handed',\n",
       " 'has property',\n",
       " '',\n",
       " 'CN',\n",
       " '[[About ten percent of people]] are [[left-handed]]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of line\n",
    "lines[15014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/c/en/1000-/r/HasProperty-/c/en/one_thousand-0000',\n",
       "  '/c/en/1000',\n",
       "  '/r/HasProperty',\n",
       "  '/c/en/one_thousand',\n",
       "  '1000',\n",
       "  'one thousand',\n",
       "  'has property',\n",
       "  '',\n",
       "  'CN',\n",
       "  '[[1000]] is [[one thousand]]'],\n",
       " 1788)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter\n",
    "filter_lines = line_filter(lines)\n",
    "\n",
    "# example of filter line, and number\n",
    "filter_lines[0], len(filter_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<run_bert_relation_prediction.InputExample at 0x7fb23f15eda0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random choose 100 samples from filter_lines\n",
    "lines_100=random.sample(filter_lines,100)\n",
    "\n",
    "# generate triple\n",
    "ent2text={}\n",
    "relation2text={}\n",
    "\n",
    "for line in lines_100:\n",
    "    ent2text[line[1]]=wn.synsets(line[4].replace(\" \",\"_\"))[0].definition()\n",
    "    ent2text[line[3]]=wn.synsets(line[5].replace(\" \",\"_\"))[0].definition()\n",
    "    relation2text[line[2]] = line[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "write_file(cskg_100,[_[1:7] for _ in lines_100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "with open(cskg_100,\"r\") as f:\n",
    "    lines = []\n",
    "    for line_str in f:\n",
    "        line_str=line_str.strip()\n",
    "\n",
    "        if line_str:\n",
    "            line=line_str.split(\"\\t\")\n",
    "\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q190024', '/r/HasProperty', 'Q39338', 'mandarin orange', 'orange', 'color']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of new lines\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<run_bert_relation_prediction.InputExample at 0x7fb237cce5f8>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic parameter\n",
    "relation_prediction.os.environ['CUDA_VISIBLE_DEVICES']= '2'\n",
    "data_path = \"./data/wc\"\n",
    "data_saved_path = \"./output_wc_result2\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=100\n",
    "eval_batch_size=32\n",
    "\n",
    "device = relation_prediction.torch.device(\"cuda\" if relation_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors = {\"kg\": relation_prediction.KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = relation_prediction.BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = relation_prediction.BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)\n",
    "\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "examples = []\n",
    "set_type=\"test\"\n",
    "for (i, line) in enumerate(lines_100):\n",
    "    guid = \"%s-%s\" % (set_type, i)\n",
    "    text_a = ent2text[line[1]]\n",
    "    text_b = ent2text[line[3]]\n",
    "    label = \"wn:quality.n.1\"\n",
    "    \n",
    "    examples.append(\n",
    "        relation_prediction.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 4/4 [00:00<00:00, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.53604269e-02  1.46461636e-01 -2.98043787e-02 ... -3.50986511e-01\n",
      "  -1.88698745e+00  4.34495598e-01]\n",
      " [ 1.56642199e-02 -1.59057188e+00  1.33126907e+01 ... -2.62189841e+00\n",
      "  -6.84538782e-01 -2.39190435e+00]\n",
      " [ 9.39943254e-01  4.68988836e-01 -1.89230680e+00 ... -6.65193176e+00\n",
      "  -6.22472668e+00 -6.11609173e+00]\n",
      " ...\n",
      " [-1.15386076e-01 -2.62573361e-04 -1.00252843e+00 ... -9.57773924e-01\n",
      "  -2.14523363e+00  2.09113359e-01]\n",
      " [-1.16387403e+00  8.94880295e-03 -2.47058153e+00 ... -5.04134369e+00\n",
      "  -5.43410635e+00 -5.10229588e+00]\n",
      " [ 1.05213976e+00 -1.40085506e+00 -8.30978751e-01 ... -4.22682667e+00\n",
      "  -3.15243816e+00 -6.16434956e+00]] (100, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "eval_features = relation_prediction.convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = relation_prediction.SequentialSampler(eval_data)\n",
    "eval_dataloader = relation_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in relation_prediction.tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with relation_prediction.torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = relation_prediction.np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()\n",
    "\n",
    "result = []\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = relation_prediction.torch.tensor(pred)\n",
    "    _, argsort1 = relation_prediction.torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "    \n",
    "    result.append(argsort1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the prediction\n",
    "with open(cskg_relation_prediction, \"w\") as f:\n",
    "    for line, idx_ in zip(lines_100, result):\n",
    "        sentence_list=line[1:6]\n",
    "        sentence_list[1]=label_list[idx_]\n",
    "        sentence_list.append(\"0\")\n",
    "        sentence=\"\\t\".join(sentence_list)\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "data_path = \"./data/wn\"\n",
    "data_saved_path = \"./output_wn_result\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=50\n",
    "eval_batch_size=1500\n",
    "\n",
    "device = link_prediction.torch.device(\"cuda\" if link_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors = {\"kg\": link_prediction.KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = [\"0\",\"1\"]\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = link_prediction.BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = link_prediction.BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_examples(line):\n",
    "    examples=[]\n",
    "    text_a = ent2text[line[1]]\n",
    "    text_b = relation2text[line[2]]\n",
    "    text_c = ent2text[line[3]]\n",
    "    \n",
    "    # corrupt head\n",
    "    text_a_candits = wn.synsets(line[4].replace(\" \",\"_\"))\n",
    "    #print(text_a_candits)\n",
    "    for idx in range(len(text_a_candits)):\n",
    "        item = text_a_candits[idx]\n",
    "        \n",
    "        text_a = item.definition()\n",
    "        \n",
    "        if idx == 0:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"1\"))\n",
    "        else:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"0\"))\n",
    "            \n",
    "    return examples, text_a_candits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:01<00:00, 63.49it/s]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "temp = []\n",
    "for line in link_prediction.tqdm(lines_100, desc=\"Testing\"):\n",
    "    examples, candits=_build_examples(line)\n",
    "    \n",
    "    # load data\n",
    "    eval_features = link_prediction.convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "    all_input_ids = link_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_input_mask = link_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_segment_ids = link_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_label_ids = link_prediction.torch.tensor([f.label_id for f in eval_features], dtype=link_prediction.torch.long)\n",
    "\n",
    "    eval_data = link_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # do predict\n",
    "    eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "    eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with link_prediction.torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = link_prediction.np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    all_label_ids = all_label_ids.numpy()\n",
    "    preds = preds[0]\n",
    "    rel_values = preds[:, all_label_ids[0]]\n",
    "    rel_values = link_prediction.torch.tensor(rel_values)\n",
    "    \n",
    "    _, argsort1 = link_prediction.torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "    \n",
    "    idx_ = argsort1[0]\n",
    "    temp.append(idx_)\n",
    "    predict_output = candits[idx_]\n",
    "    res.append(predict_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the prediction\n",
    "with open(cskg_link_prediction, \"w\") as f:\n",
    "    for line, out_ in zip(lines_100, res):\n",
    "        sentence_list=line[1:6]\n",
    "        sentence_list[0]=\"wn:\"+out_.name()\n",
    "        \n",
    "        sentence_list.append(out_.definition())\n",
    "        sentence_list.append(\"0\")\n",
    "        sentence=\"\\t\".join(sentence_list)\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preidcition, go to the output file. inspect whether the system was right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "# output the relation prediction\n",
    "with open(cskg_relation_prediction, \"r\") as f:\n",
    "    accuracy_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    for line in f:\n",
    "        label_ = int(line.split(\"\\t\")[-1])\n",
    "        accuracy_count += label_\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(accuracy_count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "# output the link prediction\n",
    "with open(cskg_link_prediction, \"r\") as f:\n",
    "    accuracy_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    for line in f:\n",
    "        label_ = int(line.split(\"\\t\")[-1])\n",
    "        accuracy_count += label_\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(accuracy_count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
