{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Relation prediction on WebChild property data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random, os, time, faiss, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file\n",
    "kgtk_webchild = \"./data/wc/kgtk_webchild_comparative.tsv\"\n",
    "\n",
    "# ouput_file\n",
    "wc_gold_file = \"./data/wc/wc_gold_500k.tsv\"\n",
    "wc_entity_file = \"./data/wc/entities.txt\"\n",
    "wc_entity2id_file = \"./data/wc/entity2id.txt\"\n",
    "wc_entity2text_file = \"./data/wc/entity2text.txt\"\n",
    "wc_entity2textlong_file = \"./data/wc/entity2textlong.txt\"\n",
    "wc_relation_file = \"./data/wc/relations.txt\"\n",
    "wc_relation2id_file = \"./data/wc/relation2id.txt\"\n",
    "wc_relation2text_file = \"./data/wc/relation2text.txt\"\n",
    "wc_train_500k = \"./data/wc/train.tsv\"\n",
    "wc_dev_500k = \"./data/wc/dev.tsv\"\n",
    "wc_test_500k = \"./data/wc/test.tsv\"\n",
    "wc_train2id_500k = \"./data/wc/train2id.txt\"\n",
    "wc_dev2id_500k = \"./data/wc/valid2id.txt\"\n",
    "wc_test2id_500k = \"./data/wc/test2id.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gold_file(lines):\n",
    "    # Generate TSV file for relation classification\n",
    "    wn_gold_all = []\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        node1_id = line[0]\n",
    "        relation_id = line[1]\n",
    "        node2_id = line[2]\n",
    "        node1_labels = line[3]\n",
    "        node2_labels = line[4]\n",
    "        relation_label = line[5]\n",
    "\n",
    "        # modeify the node labels, check with leve distance\n",
    "        sent = line[8].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        node1_label = multiple_labels(node1_labels,node1_id)\n",
    "        node2_label = multiple_labels(node2_labels,node2_id)\n",
    "\n",
    "        wn_gold_all.append([node1_label, relation_label, node2_label, node1_id, node2_id,relation_id,sent])\n",
    "        i += 1\n",
    "        if i%10000==1:\n",
    "            print(f\"\\r {i}/{len(lines)}\", end=\"\")\n",
    "        \n",
    "    return wn_gold_all\n",
    "        \n",
    "def write_split_file(filename1, fielename2, lines,entity2detail,relation2detail):\n",
    "    # write train & train2id\n",
    "    with open(filename1,\"w\",newline='') as f1, open(fielename2,\"w\",newline='') as f2:\n",
    "        w1 = csv.writer(f1, delimiter='\\t')\n",
    "        w2 = csv.writer(f2, delimiter='\\t')\n",
    "\n",
    "        #write head\n",
    "        w2.writerow([len(lines)])\n",
    "        for line in lines:\n",
    "            # wtite content\n",
    "            entity1, relation, entity2 = line\n",
    "            entity1_id, entity1_text, entity1_textlong = entity2detail[entity1]\n",
    "            entity2_id, entity2_text, entity2_textlong = entity2detail[entity2]\n",
    "            relation_id, relation_text, relation_textlong = relation2detail[relation]\n",
    "            w1.writerow([entity1,relation,entity2])\n",
    "            w2.writerow([entity1_id,entity2_id,relation_id,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "head, lines = load_file(kgtk_webchild,encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple example\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether it has triplr without predicate\n",
    "# if yes, remove this triples\n",
    "# check number of relations, unique nodes, total edges, distribution of relations, most frequent nodes\n",
    "\n",
    "valid_lines = []\n",
    "invalid_lines_count = 0\n",
    "unique_nodes = dict()\n",
    "relations_dict = dict()\n",
    "\n",
    "node2text = dict()\n",
    "relation2text = dict()\n",
    "\n",
    "for line in lines:\n",
    "    node1_id = line[0]\n",
    "    node2_id = line[2]\n",
    "    relation_id = line[1]\n",
    "    \n",
    "    node1_label = line[3]\n",
    "    node2_label = line[4]\n",
    "    relation_label = line[5]\n",
    "    \n",
    "    if not relation_id:\n",
    "        #filter no relation\n",
    "        invalid_lines_count += 1\n",
    "        continue\n",
    "    \n",
    "    # triple with relation\n",
    "    valid_lines.append(line)\n",
    "    \n",
    "    # count the number of unique nodes\n",
    "    unique_nodes[node1_id] = unique_nodes.get(node1_id,0)+1\n",
    "    unique_nodes[node2_id] = unique_nodes.get(node2_id,0)+1\n",
    "    \n",
    "    # count the number of unique relations\n",
    "    relations_dict[relation_id] = relations_dict.get(relation_id,0)+1\n",
    "    \n",
    "    node2text[node1_id] = node1_label\n",
    "    node2text[node2_id] = node2_label\n",
    "    relation2text[relation_id] = relation_label\n",
    "    \n",
    "# Summary\n",
    "print(f\"The number of triples without relations: {invalid_lines_count}\")\n",
    "print(f\"The number of triples with relation: {len(valid_lines)}\")\n",
    "print(f\"The number of unique nodes: {len(unique_nodes)}\")\n",
    "print(f\"The number of unique relations: {len(relations_dict)}\")\n",
    "max_fre_nodes = max(unique_nodes, key=unique_nodes.get)\n",
    "print(f\"The most frequent node: {max_fre_nodes}\")\n",
    "max_fre_relation = max(relations_dict, key=relations_dict.get)\n",
    "print(f\"The most frequent relation: {max_fre_relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relations_dict.values(),log=True,bins=20)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Number of Edges Having One Specified Relation')\n",
    "plt.ylabel('Number of Relations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2fre=[]\n",
    "\n",
    "for line in lines:\n",
    "    relation_id = line[1]\n",
    "    \n",
    "    relation2fre.append(relations_dict[relation_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relation2fre,bins=20)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Number of Edges Having One Specified Relation')\n",
    "plt.ylabel('Number of Edges for the Relations Having Same Number of Edges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random select 500k lines from data\n",
    "valid_lines_10k = random.choices(valid_lines, k=500000)\n",
    "\n",
    "# generate 500k lines gold file\n",
    "wc_gold_500k = generate_gold_file(valid_lines_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Data File**\n",
    "\n",
    "1. Generate Train, Dev and Test Dataset.\n",
    "2. Generate basic data files used for kg-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2detail = dict()\n",
    "relation2detail = dict()\n",
    "\n",
    "idx= 0\n",
    "for node_id in unique_nodes:\n",
    "    # build dict for each entity with their id, text, and long text\n",
    "    definition = wn.synset(node_id.split(\":\")[1]).definition()\n",
    "    entity_label = node2text[node_id]\n",
    "    entity2detail[node_id]=[idx, entity_label, definition]\n",
    "    idx += 1\n",
    "\n",
    "idx = 0\n",
    "for relation_id in relation2text:\n",
    "    # build dict for each relation with their id, text, and long text\n",
    "    definition = wn.synset(relation_id.split(\":\")[1].replace(\" \",\"_\")).definition()\n",
    "    relation_label = relation2text[relation_id]\n",
    "    relation2detail[relation_id]=[idx, relation_label, definition]\n",
    "    idx += 1\n",
    "    \n",
    "# Transfer gold data to the structure of kg-bert\n",
    "kgBert_data_sent = [[_[3],_[5],_[4],_[6]] for _ in wc_gold_500k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "# entities.txt, entity2id, entity2text, entity2textlong\n",
    "\n",
    "with open(wc_entity_file,\"w\",newline='') as f1, open(wc_entity2id_file,\"w\",newline='') as f2,\\\n",
    "open(wc_entity2text_file, \"w\",newline='') as f3, open(wc_entity2textlong_file,\"w\",newline='') as f4:\n",
    "    w1 = csv.writer(f1, delimiter='\\t')\n",
    "    w2 = csv.writer(f2, delimiter='\\t')\n",
    "    w3 = csv.writer(f3, delimiter='\\t')\n",
    "    w4 = csv.writer(f4, delimiter='\\t')\n",
    "    \n",
    "    #write head\n",
    "    w2.writerow([len(entity2detail)])\n",
    "    for entity in entity2detail:\n",
    "        # wtite content\n",
    "        entity_id, entity_text, entity_textlong = entity2detail[entity]\n",
    "        w1.writerow([entity,])\n",
    "        w2.writerow([entity,entity_id])\n",
    "        w3.writerow([entity,entity_text])\n",
    "        w4.writerow([entity,entity_textlong])\n",
    "        \n",
    "# relations.txt, rekation2id.txt, relation2text.txt\n",
    "wc_relation_file = \"./data/wc/relations.txt\"\n",
    "wc_relation2id_file = \"./data/wc/relation2id.txt\"\n",
    "wc_relation2text_file = \"./data/wc/relation2text.txt\"\n",
    "with open(wc_relation_file,\"w\",newline='') as f1, open(wc_relation2id_file,\"w\",newline='') as f2,\\\n",
    "open(wc_relation2text_file, \"w\",newline='') as f3:\n",
    "    w1 = csv.writer(f1, delimiter='\\t')\n",
    "    w2 = csv.writer(f2, delimiter='\\t')\n",
    "    w3 = csv.writer(f3, delimiter='\\t')\n",
    "    \n",
    "    #write head\n",
    "    w2.writerow([len(relation2detail)])\n",
    "    for relation in relation2detail:\n",
    "        # wtite content\n",
    "        relation_id, relation_text, relation_textlong = relation2detail[relation]\n",
    "        w1.writerow([relation,])\n",
    "        w2.writerow([relation,relation_id])\n",
    "        w3.writerow([relation,relation_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, dev, test file\n",
    "\n",
    "random.shuffle(kgBert_data_sent)\n",
    "\n",
    "# define ratio of train, dev, test\n",
    "train_ratio = 0.8\n",
    "dev_ratio = 0.1\n",
    "test_ratio = 1 - train_ratio - dev_ratio\n",
    "\n",
    "len_gold = len(kgBert_data_sent)\n",
    "train, dev, test = np.split(kgBert_data_sent,[int(train_ratio*len_gold),int((train_ratio+dev_ratio)*len_gold)])\n",
    "print(\"length of train, dev, test: \", len(train), len(dev), len(test))\n",
    "\n",
    "# write train & train2id\n",
    "write_split_file(wc_train_500k, wc_train2id_500k, [_[0:-1] for _ in train],entity2detail,relation2detail)\n",
    "\n",
    "# write dev & dev2id\n",
    "write_split_file(wc_dev_500k, wc_dev2id_500k, [_[0:-1] for _ in dev],entity2detail,relation2detail)\n",
    "\n",
    "# write test & test2id\n",
    "write_split_file(wc_test_500k, wc_test2id_500k, [_[0:-1] for _ in test],entity2detail,relation2detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of gold file\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synsets(labels):\n",
    "    # According to the generation of labels, obtain the synsets\n",
    "    for label in labels:\n",
    "        synsets = list(wn.synsets(label, pos=\"a\"))\n",
    "        \n",
    "        if synsets:\n",
    "            return synsets, label\n",
    "        \n",
    "    return [], label\n",
    "\n",
    "def generate_candidates(label):\n",
    "    candidates,_ = generate_synsets(transfer_words(WordNetLemmatizer().lemmatize(label, pos=\"a\")))\n",
    "    return candidates\n",
    "\n",
    "def MRS(wn_gold):\n",
    "    # Random Baseline calculation\n",
    "    # only predict subject\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        entity1  = line[0]\n",
    "        entity2 = line[2]\n",
    "        relation_label = line[3]\n",
    "        relation_label = relation_transfer(relation_label)\n",
    "        \n",
    "        # generate all possible combination of \"_\"&\"-\" and check whether it can find sysets.\n",
    "        # if the combination can generate sysets, use this combinations to generate candidates\n",
    "        candidates = generate_candidates(relation_label)\n",
    "        \n",
    "        if candidates:\n",
    "            # random choose candidates\n",
    "            relation_id = random.choice(candidates)\n",
    "        else:\n",
    "            relation_id = \"\"\n",
    "        \n",
    "        node1_id=\"\"\n",
    "        node2_id=\"\"\n",
    "        \n",
    "        wn_predict.append([entity1, relation_id, entity2, relation_label])\n",
    "        \n",
    "    return wn_predict\n",
    "\n",
    "def relation_transfer(label, indexs=[0,1,1,1], str_=[\" than\",\"be \",\"more \",\"less \"]):\n",
    "    if str_:\n",
    "        # modify\n",
    "        index = indexs[0]\n",
    "        if index == 0:\n",
    "            label = label.split(str_[0])[index]\n",
    "            return relation_transfer(label, indexs=indexs[1:], str_=str_[1:])\n",
    "        else:\n",
    "            temp = label.split(str_[0])\n",
    "            if len(temp)>index:\n",
    "                label = temp[index]\n",
    "                return relation_transfer(label, indexs=indexs[1:], str_=str_[1:])\n",
    "            else:\n",
    "                return relation_transfer(label, indexs=indexs[1:], str_=str_[1:])\n",
    "    else:\n",
    "        # no more modify\n",
    "        return label.strip()\n",
    "    \n",
    "def modify_data(lines,relation2detail):\n",
    "    # add label in the dataset used to make prediction\n",
    "    for line in lines:\n",
    "        entity1, relation, entity2, sent = line\n",
    "        relation_label = relation2detail[relation][1]\n",
    "        yield entity1, relation, entity2, relation_label, sent\n",
    "\n",
    "def validation(wn_predict, wn_gold):\n",
    "    # valid the accuracy of prediction: only compare the accuracy of prediction\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for predict, actual in zip(wn_predict, wn_gold):\n",
    "        predict_label1 = predict[1]\n",
    "        actual_label1 = wn.synset(actual[1].split(\":\")[1].replace(\" \",\"_\"))\n",
    "        if not predict_label1:\n",
    "            continue\n",
    "        if predict_label1 == actual_label1:\n",
    "            # predict and actual is same\n",
    "            correct += 1\n",
    "            \n",
    "    return correct/len(wn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify train, dev, test data\n",
    "train_modify = list(modify_data(train,relation2detail))\n",
    "dev_modify = list(modify_data(dev,relation2detail))\n",
    "test_modify = list(modify_data(test,relation2detail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "wc_ran_train_predict = MRS(train_modify)\n",
    "accuracy = validation(wc_ran_train_predict, train_modify)\n",
    "print(\"Accuracy of train dataset (random baseline): \", accuracy)\n",
    "\n",
    "#dev dataset\n",
    "wc_ran_dev_predict = MRS(dev_modify)\n",
    "accuracy = validation(wc_ran_dev_predict, dev_modify)\n",
    "print(\"Accuracy of dev dataset (random baseline): \", accuracy)\n",
    "\n",
    "#test dataset\n",
    "wc_ran_test_predict = MRS(test_modify)\n",
    "accuracy = validation(wc_ran_test_predict, test_modify)\n",
    "print(\"Accuracy of test dataset (random baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MFS Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFS(wn_gold):\n",
    "    # Frequent Baseline Calculation\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        entity1  = line[0]\n",
    "        entity2 = line[2]\n",
    "        relation_label = line[3]\n",
    "        relation_label = relation_transfer(relation_label)\n",
    "        \n",
    "        # generate all possible combination of \"_\"&\"-\" and check whether it can find sysets.\n",
    "        # if the combination can generate sysets, use this combinations to generate candidates\n",
    "        candidates = generate_candidates(relation_label)\n",
    "        \n",
    "        if candidates:\n",
    "            # random choose candidates\n",
    "            relation_id = candidates[0]\n",
    "        else:\n",
    "            relation_id = \"\"\n",
    "        \n",
    "        node1_id=\"\"\n",
    "        node2_id=\"\"\n",
    "        \n",
    "        wn_predict.append([entity1, relation_id, entity2, relation_label])\n",
    "    return wn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "wc_ran_train_predict = MFS(train_modify)\n",
    "accuracy = validation(wc_ran_train_predict, train_modify)\n",
    "print(\"Accuracy of train dataset (frequency baseline): \", accuracy)\n",
    "\n",
    "#dev dataset\n",
    "wc_ran_dev_predict = MFS(dev_modify)\n",
    "accuracy = validation(wc_ran_dev_predict, dev_modify)\n",
    "print(\"Accuracy of dev dataset (frequency baseline): \", accuracy)\n",
    "\n",
    "#test dataset\n",
    "wc_ran_test_predict = MFS(test_modify)\n",
    "accuracy = validation(wc_ran_test_predict, test_modify)\n",
    "print(\"Accuracy of test dataset (frequency baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STBert Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STB = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_normalize(vector):\n",
    "    # input should be a numpy array\n",
    "    # change to unit vector\n",
    "    return vector / (vector**2).sum()**0.5\n",
    "\n",
    "def max_candidate_faiss(label_,sent_embedding_,label_embeddings):\n",
    "    # return the max similarity candidates\n",
    "    #output: [[similarity, synset, the pos of synset]]\n",
    "\n",
    "    sent_embedding_ = vector_normalize(sent_embedding_)\n",
    "    if label_ not in label_embeddings:\n",
    "        # label is not exists in the labels embeddings means:\n",
    "            #there is no sysets for this label\n",
    "            # return \"\"\n",
    "        return \"\"\n",
    "    else:\n",
    "        # label exists\n",
    "        # return the max similarity candidates\n",
    "        top_n = 1\n",
    "        index_ = label_embeddings[label_][1]\n",
    "        _, I = index_.search(np.array([sent_embedding_]), top_n)\n",
    "        #print(int(I),label_embeddings[label_][0])\n",
    "        return label_embeddings[label_][0][int(I)]\n",
    "\n",
    "def candidates_embeddings_faiss(wn_gold, model):\n",
    "    # generate label node id defination embeddings from file\n",
    "    # output:{\"label_name\":[[node_id, embedding of node_id defination],[X,X],[X,X]]}\n",
    "    \n",
    "    # store label, synset\n",
    "    label_synsets = []\n",
    "    # store the defination sentence of synset\n",
    "    sents_combine = []\n",
    "    \n",
    "    embeddings = dict()\n",
    "    \n",
    "    length1 = len(wn_gold)\n",
    "    count1 = 0\n",
    "    \n",
    "    for line in wn_gold:\n",
    "        label1 = line[0]\n",
    "        label2 = line[2]\n",
    "        relation_label = line[3]\n",
    "        relation_label = relation_transfer(relation_label)\n",
    "        \n",
    "        label_synsets,sents_combine = label2sentence2sent(relation_label, model,label_synsets,sents_combine)\n",
    "        count1 += 1\n",
    "        \n",
    "        #if count1%1000==0:print(f\"\\r lines counting {count1}/{length1}\",end=\"\")\n",
    "\n",
    "    # generate embedding of sentence\n",
    "    start = time.time()\n",
    "    sents_embed = model.encode(sents_combine)\n",
    "    # dimension of faiss\n",
    "    d = len(sents_embed[0])\n",
    "    end = time.time()\n",
    "    #print(f\"model time: {end-start}\")\n",
    "    #print(\"candidates sentences embedding generated\")\n",
    "    \n",
    "    length2 = len(label_synsets)\n",
    "    count2 = 0\n",
    "    for label_synset, embed in zip(label_synsets,sents_embed):\n",
    "        label, synset = label_synset\n",
    "        \n",
    "        temp = embeddings.get(label,dict())\n",
    "        if synset not in temp:\n",
    "            temp[synset] = embed\n",
    "        \n",
    "        # generate embedding of label synset\n",
    "        embeddings[label]=temp\n",
    "        count2 +=1\n",
    "        \n",
    "        #if count2%1000==0:print(f\"\\r embed countung {count2}/{length2}\", end=\"\")\n",
    "            \n",
    "    # write embedding into faiss\n",
    "    for label in embeddings:\n",
    "        label_embeds = embeddings[label]\n",
    "        index_ = faiss.IndexFlatL2(d)\n",
    "        sub_embeds = []\n",
    "        sub_labelId = []\n",
    "        for label_id in label_embeds:\n",
    "            embed = label_embeds[label_id]\n",
    "            sub_labelId.append(label_id)\n",
    "            sub_embeds.append(vector_normalize(embed))\n",
    "            \n",
    "        # write into faiss store with label name\n",
    "        index_.add(np.array(sub_embeds))\n",
    "        \n",
    "        embeddings[label] = (sub_labelId,index_)\n",
    "    return embeddings\n",
    "\n",
    "def sentence_embedding_faiss(wn_gold, model, label_embeddings = None):\n",
    "    # use sentences embedding to find most similar candit\n",
    "    wn_predict = []\n",
    "    sents_combine = []\n",
    "    \n",
    "    length1 = len(wn_gold)\n",
    "    count1 = 0\n",
    "    for line in wn_gold:\n",
    "        sentence = line[4]\n",
    "        sents_combine.append(sentence)\n",
    "        \n",
    "        count1 += 1\n",
    "        #if count1%1000==0:print(f\"\\r lines counting {count1}/{length1}\",end=\"\")\n",
    "    # obtain sentence embedding\n",
    "    start = time.time()\n",
    "    sents_embedding = model.encode(sents_combine)\n",
    "    end = time.time()\n",
    "    #print(f\"model time: {end-start}\")\n",
    "    \n",
    "    length2 = len(wn_gold)\n",
    "    count2 = 0\n",
    "    for line,sent_embedding in zip(wn_gold,sents_embedding):\n",
    "        relation_label = line[3]\n",
    "        relation_label = relation_transfer(relation_label)\n",
    "\n",
    "        #obtain the max similar item for label1\n",
    "        relation_id = max_candidate_faiss(relation_label,sent_embedding,label_embeddings) \n",
    "                \n",
    "        wn_predict.append([line[0], relation_id, line[2],line[3],line[4]])\n",
    "        count2 +=1\n",
    "        #print(f\"\\r line countung {count2}/{length2}\", end=\"\")\n",
    "    return wn_predict\n",
    "\n",
    "def chunks_divide(data, num=10000):\n",
    "    for idx in range(0,len(data),num):\n",
    "        yield data[idx:idx+num]\n",
    "\n",
    "def process_data_inChunk(data, model, chunk_num = 10000):\n",
    "    # data file is processed in chunks\n",
    "    total = 0\n",
    "    total_predict = []\n",
    "    start_total = time.time()\n",
    "    \n",
    "    for sub_data in chunks_divide(data, num=chunk_num):\n",
    "        start = time.time()\n",
    "        label_embeddings = candidates_embeddings_faiss(sub_data, model)\n",
    "        predict = sentence_embedding_faiss(sub_data, model,label_embeddings = label_embeddings)\n",
    "        \n",
    "        total_predict += predict\n",
    "        process_num = len(sub_data)\n",
    "        total +=process_num\n",
    "        usedtime = time.time() - start\n",
    "        print(f\"The time used for this iteration: {usedtime}, finished lines {total}/{len(data)}\")\n",
    "    accuracy = validation(total_predict, data)\n",
    "    \n",
    "    total_time = time.time()-start_total\n",
    "    print(f\"Process finished! Total time: {total_time}\")\n",
    "    return total_predict, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wc_ran_train_predict, accuracy = process_data_inChunk(dev_modify[:20000], model_STB, chunk_num = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "wc_stb_train_predict, accuracy = process_data_inChunk(train_modify, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of train dataset (STB baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#dev dataset\n",
    "wc_stb_dev_predict, accuracy = process_data_inChunk(dev_modify, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of dev dataset (STB baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#test dataset\n",
    "wc_stb_test_predict, accuracy = process_data_inChunk(test_modify, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of test dataset (STB baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRoberta Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STR = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "wc_str_train_predict, accuracy = process_data_inChunk(train_modify, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of train dataset (STR baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#dev dataset\n",
    "wc_str_dev_predict, accuracy = process_data_inChunk(dev_modify, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of dev dataset (STR baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "\n",
    "#test dataset\n",
    "wc_str_test_predict, accuracy = process_data_inChunk(test_modify, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of test dataset (STR baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isi",
   "language": "python",
   "name": "isi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
