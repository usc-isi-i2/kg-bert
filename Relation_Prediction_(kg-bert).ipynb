{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bert_relation_prediction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '2'\n",
    "data_path = \"./data/wc\"\n",
    "data_saved_path = \"./output_wc_result\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=25\n",
    "eval_batch_size=32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precessor\n",
    "processors = {\"kg\": KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_accuracy(ranks):\n",
    "    # check the accuracy for different hits\n",
    "    max_dep = 1\n",
    "    threshold = 0\n",
    "    accuracy_dict = dict()\n",
    "    ite = 0\n",
    "    \n",
    "    while threshold <= max_dep and ite <500:\n",
    "        for rank in ranks:\n",
    "            max_dep = max(max_dep, rank)\n",
    "            if rank <= threshold:\n",
    "                accuracy_dict[threshold] = accuracy_dict.get(threshold,0)+1\n",
    "                \n",
    "        accuracy_dict[threshold] = accuracy_dict[threshold]/len(ranks)\n",
    "        threshold += 1\n",
    "        ite += 1\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 12500/12500 [7:45:34<00:00,  2.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.418129     0.37876976   2.9576035  ...  -6.878661    -6.9027905\n",
      "   -8.332947  ]\n",
      " [ -1.7731899   -0.17856944   4.0300937  ...  -7.6012      -8.458498\n",
      "   -9.099413  ]\n",
      " [  0.0993562   -3.0691712    4.315088   ...  -8.305269    -8.870956\n",
      "   -9.867961  ]\n",
      " ...\n",
      " [ -2.8487058    0.8943816    5.0485106  ...  -4.2121305   -4.3122816\n",
      "   -6.001689  ]\n",
      " [ -1.1938187    3.0598202    6.8402953  ...  -9.60652     -8.834217\n",
      "  -10.4396925 ]\n",
      " [ -1.72309      1.6641227   13.399329   ...  -7.3354406   -7.329503\n",
      "   -8.942232  ]] (400000, 6331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "eval_examples = processor.get_train_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257225"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1563/1563 [09:08<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.3663081  -0.35011637  4.215994   ... -5.3430133  -5.9850407\n",
      "  -7.349557  ]\n",
      " [ 2.7053308  11.519518    6.2654366  ... -6.949276   -7.282262\n",
      "  -8.332974  ]\n",
      " [-4.7281375   0.35013202  2.667777   ... -6.9289436  -6.215674\n",
      "  -8.118363  ]\n",
      " ...\n",
      " [-2.7581234   1.8021257   3.6017327  ... -8.592715   -8.177821\n",
      "  -9.730576  ]\n",
      " [-1.4960191   1.7245771   4.6012874  ... -6.1359034  -6.5996304\n",
      "  -7.611351  ]\n",
      " [-3.303895   -0.8380291   2.221367   ... -7.6081243  -7.2373414\n",
      "  -8.242722  ]] (50000, 6331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dev data\n",
    "eval_examples = processor.get_dev_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1563/1563 [06:42<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.323254    5.1532063   4.985848   ... -6.9448853  -7.2415037\n",
      "  -8.597521  ]\n",
      " [-0.45913562  4.6801825  12.855275   ... -6.348547   -6.891628\n",
      "  -8.719812  ]\n",
      " [ 0.41970402  2.4043767   9.534689   ... -8.552229   -8.642421\n",
      "  -9.782871  ]\n",
      " ...\n",
      " [-2.9259968   0.89694214  6.954977   ... -7.8598304  -8.264762\n",
      "  -9.413802  ]\n",
      " [-1.40529    -1.5783433   3.7033048  ... -7.8304453  -8.069033\n",
      "  -9.473032  ]\n",
      " [-2.7316413   5.6561646   4.4248576  ... -8.029417   -7.7321568\n",
      "  -9.467815  ]] (50000, 6331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "eval_examples = processor.get_test_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37148"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
