{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import random, csv,time\n",
    "\n",
    "import run_bert_relation_prediction as relation_prediction\n",
    "import run_bert_link_prediction as link_prediction\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file\n",
    "cskg_file = \"./data/cskg/cskg_connected.tsv\"\n",
    "ground_truth=\"./data/cskg/ground_truth_100.tsv\"\n",
    "web_child = \"./data/wc/train.tsv\"\n",
    "\n",
    "# output file\n",
    "cskg_relation_prediction = \"./data/cskg/relation_prediction.tsv\"\n",
    "cskg_link_prediction = \"./data/cskg/link_prediction.tsv\"\n",
    "cskg_prediction = \"./data/cskg/prediction.tsv\"\n",
    "cskg_100 = \"./data/cskg/cskg_100.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename,encoding=None,errors=None):\n",
    "    # load data file\n",
    "    with open(filename,\"r\",encoding=encoding, errors=errors) as f:\n",
    "        head_str = f.readline()\n",
    "        head=head_str.split(\"\\t\")\n",
    "        \n",
    "        lines = []\n",
    "        for line_str in f:\n",
    "            line_str=line_str.strip()\n",
    "            \n",
    "            if line_str:\n",
    "                line=line_str.split(\"\\t\")\n",
    "\n",
    "                lines.append(line)\n",
    "            \n",
    "    return head,lines\n",
    "\n",
    "def line_filter(lines, label_=\"/r/HasProperty\"):\n",
    "    #\n",
    "    res = []\n",
    "    \n",
    "    for line in lines:\n",
    "        entity1 = line[1]\n",
    "        entity2 = line[3]\n",
    "        relation = line[2]\n",
    "        \n",
    "        entity1_label=line[4]\n",
    "        entity2_label=line[5]\n",
    "        relation_label=line[6]\n",
    "        \n",
    "        if line[2] == label_:\n",
    "            # has relation label hasproperty\n",
    "            \n",
    "            # subject 2+ candidates\n",
    "            # object 1+ candidates\n",
    "            \n",
    "            entity1_synsets = wn.synsets(entity1_label.replace(\" \",\"_\"))\n",
    "            entity2_synsets = wn.synsets(entity2_label.replace(\" \",\"_\"))\n",
    "            #res.append(line)\n",
    "            \n",
    "            if len(entity1_synsets) >= 2 and len(entity2_synsets)>=1:\n",
    "                res.append(line)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def write_file(filename,lines,encoding=None,errors=None):\n",
    "    # write data file\n",
    "    with open(filename,\"w\",encoding=encoding, errors=errors) as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(lines)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "head, lines=load_file(cskg_file,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/en/about_ten_percent_of_people-/r/HasProperty-/c/en/left_handed-0000',\n",
       " '/c/en/about_ten_percent_of_people',\n",
       " '/r/HasProperty',\n",
       " '/c/en/left_handed',\n",
       " 'about ten percent of people',\n",
       " 'left handed',\n",
       " 'has property',\n",
       " '',\n",
       " 'CN',\n",
       " '[[About ten percent of people]] are [[left-handed]]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of line\n",
    "lines[15014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "filter_lines = line_filter(lines)\n",
    "\n",
    "# example of filter line, and number\n",
    "filter_lines[0], len(filter_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random choose 100 samples from filter_lines\n",
    "lines_100=random.sample(filter_lines,100)\n",
    "\n",
    "# generate triple\n",
    "ent2text={}\n",
    "relation2text={}\n",
    "\n",
    "for line in lines_100:\n",
    "    ent2text[line[1]]=wn.synsets(line[4].replace(\" \",\"_\"))[0].definition()\n",
    "    ent2text[line[3]]=wn.synsets(line[5].replace(\" \",\"_\"))[0].definition()\n",
    "    relation2text[line[2]] = line[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "write_file(cskg_100,[_[1:7] for _ in lines_100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "with open(cskg_100,\"r\") as f:\n",
    "    lines = []\n",
    "    for line_str in f:\n",
    "        line_str=line_str.strip()\n",
    "\n",
    "        if line_str:\n",
    "            line=line_str.split(\"\\t\")\n",
    "\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of new lines\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameter\n",
    "relation_prediction.os.environ['CUDA_VISIBLE_DEVICES']= '2'\n",
    "data_path = \"./data/wc\"\n",
    "data_saved_path = \"./output_wc_result2\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=100\n",
    "eval_batch_size=32\n",
    "\n",
    "device = relation_prediction.torch.device(\"cuda\" if relation_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors = {\"kg\": relation_prediction.KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = relation_prediction.BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = relation_prediction.BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)\n",
    "\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "examples = []\n",
    "set_type=\"test\"\n",
    "for (i, line) in enumerate(lines_100):\n",
    "    guid = \"%s-%s\" % (set_type, i)\n",
    "    text_a = ent2text[line[1]]\n",
    "    text_b = ent2text[line[3]]\n",
    "    label = \"wn:quality.n.1\"\n",
    "    \n",
    "    examples.append(\n",
    "        relation_prediction.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "eval_features = relation_prediction.convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = relation_prediction.SequentialSampler(eval_data)\n",
    "eval_dataloader = relation_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in relation_prediction.tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with relation_prediction.torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = relation_prediction.np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()\n",
    "\n",
    "result = []\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = relation_prediction.torch.tensor(pred)\n",
    "    _, argsort1 = relation_prediction.torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "    \n",
    "    result.append(argsort1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the prediction\n",
    "with open(cskg_relation_prediction, \"w\") as f:\n",
    "    for line, idx_ in zip(lines_100, result):\n",
    "        sentence_list=line[1:6]\n",
    "        sentence_list[1]=label_list[idx_]\n",
    "        sentence_list.append(\"0\")\n",
    "        sentence=\"\\t\".join(sentence_list)\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "data_path = \"./data/wn\"\n",
    "data_saved_path = \"./output_wn_result\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=50\n",
    "eval_batch_size=1500\n",
    "\n",
    "device = link_prediction.torch.device(\"cuda\" if link_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors = {\"kg\": link_prediction.KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = [\"0\",\"1\"]\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = link_prediction.BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = link_prediction.BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_examples(line):\n",
    "    examples=[]\n",
    "    text_a = ent2text[line[1]]\n",
    "    text_b = relation2text[line[2]]\n",
    "    text_c = ent2text[line[3]]\n",
    "    \n",
    "    # corrupt head\n",
    "    text_a_candits = wn.synsets(line[4].replace(\" \",\"_\"))\n",
    "    #print(text_a_candits)\n",
    "    for idx in range(len(text_a_candits)):\n",
    "        item = text_a_candits[idx]\n",
    "        \n",
    "        text_a = item.definition()\n",
    "        \n",
    "        if idx == 0:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"1\"))\n",
    "        else:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"0\"))\n",
    "            \n",
    "    return examples, text_a_candits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "temp = []\n",
    "for line in link_prediction.tqdm(line_100, desc=\"Testing\"):\n",
    "    examples, candits=_build_examples(line)\n",
    "    \n",
    "    # load data\n",
    "    eval_features = link_prediction.convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "    all_input_ids = link_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_input_mask = link_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_segment_ids = link_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=link_prediction.torch.long)\n",
    "    all_label_ids = link_prediction.torch.tensor([f.label_id for f in eval_features], dtype=link_prediction.torch.long)\n",
    "\n",
    "    eval_data = link_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # do predict\n",
    "    eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "    eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with link_prediction.torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = link_prediction.np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    all_label_ids = all_label_ids.numpy()\n",
    "    preds = preds[0]\n",
    "    rel_values = preds[:, all_label_ids[0]]\n",
    "    rel_values = link_prediction.torch.tensor(rel_values)\n",
    "    \n",
    "    _, argsort1 = link_prediction.torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "    \n",
    "    idx_ = argsort1[0]\n",
    "    temp.append(idx_)\n",
    "    predict_output = candits[idx_]\n",
    "    res.append(predict_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the prediction\n",
    "with open(cskg_link_prediction, \"w\") as f:\n",
    "    for line, out_ in zip(lines_100, res):\n",
    "        sentence_list=line[1:6]\n",
    "        sentence_list[0]=\"wn:\"+out_.name()\n",
    "        \n",
    "        sentence_list.append(out_.definition())\n",
    "        sentence_list.append(\"0\")\n",
    "        sentence=\"\\t\".join(sentence_list)\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preidcition, go to the output file. inspect whether the system was right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the relation prediction\n",
    "with open(cskg_relation_prediction, \"r\") as f:\n",
    "    accuracy_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    for line in f:\n",
    "        label_ = int(line.split(\"\\t\")[-1])\n",
    "        accuracy_count += label_\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(accuracy_count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the link prediction\n",
    "with open(cskg_link_prediction, \"r\") as f:\n",
    "    accuracy_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    for line in f:\n",
    "        label_ = int(line.split(\"\\t\")[-1])\n",
    "        accuracy_count += label_\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(accuracy_count/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Node & Relation Calssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head,lines=load_file(web_child)\n",
    "lines.append(head)\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameter\n",
    "relation_prediction.os.environ['CUDA_VISIBLE_DEVICES']= '2'\n",
    "data_path_relation = \"./data/wc\"\n",
    "data_saved_path_relation = \"./output_wc_result2\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length_relation=100\n",
    "eval_batch_size_relation=32\n",
    "\n",
    "device_relation = relation_prediction.torch.device(\"cuda\" if relation_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors_relation = {\"kg\": relation_prediction.KGProcessor,}\n",
    "processor_relation = processors_relation[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list_relation = processor_relation.get_relations(data_path_relation)\n",
    "num_labels_relation = len(label_list_relation)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list_relation = processor_relation.get_entities(data_path_relation)\n",
    "\n",
    "# load model\n",
    "tokenizer = relation_prediction.BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model_relation = relation_prediction.BertForSequenceClassification.from_pretrained(data_saved_path_relation, \n",
    "                                                                                   num_labels=num_labels_relation)\n",
    "location_detail_1=model_relation.to(device_relation)\n",
    "\n",
    "\n",
    "data_path_link = \"./data/wc\"\n",
    "data_saved_path_link = \"./output_wn_result\"\n",
    "max_seq_length_link=50\n",
    "eval_batch_size_link=1500\n",
    "\n",
    "device_link = link_prediction.torch.device(\"cuda\" if link_prediction.torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load precessor\n",
    "processors_link = {\"kg\": link_prediction.KGProcessor,}\n",
    "processor_link = processors_link[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list_link = [\"0\",\"1\"]\n",
    "num_labels_link = len(label_list_link)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list_link = processor_link.get_entities(data_path_link)\n",
    "\n",
    "# load model\n",
    "model_link = link_prediction.BertForSequenceClassification.from_pretrained(data_saved_path_link, num_labels=num_labels_link)\n",
    "location_detail=model_link.to(device_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check relation frequency\n",
    "fre = {}\n",
    "\n",
    "for item in lines:\n",
    "    relation = item[1]\n",
    "    fre[relation]=fre.get(relation,0)+1\n",
    "    \n",
    "fre_sort = sorted(fre.items(),key=lambda k:k[1],reverse=True)\n",
    "fre={}\n",
    "idx = 0\n",
    "for item in fre_sort:\n",
    "    idx +=1\n",
    "    fre[item[0]] = 1/idx\n",
    "\n",
    "fre_relation_list = []\n",
    "for item in label_list_relation:\n",
    "    fre_relation_list.append(fre[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "with open(cskg_100,\"r\") as f:\n",
    "    lines = []\n",
    "    for line_str in f:\n",
    "        line_str=line_str.strip()\n",
    "\n",
    "        if line_str:\n",
    "            line=line_str.split(\"\\t\")\n",
    "\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_examples_(ent_candits,text_a=None,text_b=None,text_c=None, pos=\"head\"):\n",
    "    examples = []\n",
    "    for idx in range(len(ent_candits)):\n",
    "        item = ent_candits[idx]\n",
    "        text_ = item.definition()\n",
    "        \n",
    "        if pos ==\"head\":\n",
    "            text_a = text_\n",
    "        else:\n",
    "            text_c = text_\n",
    "        \n",
    "        if idx == 0:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"1\"))\n",
    "        else:\n",
    "            examples.append(link_prediction.InputExample(guid=None,text_a=text_a, text_b=text_b, text_c=text_c, label=\"0\"))\n",
    "    return examples\n",
    "\n",
    "def combine_examples_generation(line):\n",
    "    # create three examples for node resolution and relation classification.\n",
    "    # examples_relation: for relation \n",
    "    examples=[]\n",
    "    \n",
    "    # use MFS to find id\n",
    "    relation_id = \"wn:quality.n.1\"\n",
    "    \n",
    "    # corrupt head & tail\n",
    "    ent_1_candits = wn.synsets(line[3].replace(\" \",\"_\"))\n",
    "    ent_2_candits = wn.synsets(line[4].replace(\" \",\"_\"))\n",
    "    \n",
    "    # use MFS to find id\n",
    "    relation_id = \"wn:quality.n.1\"\n",
    "    ent_1_text = ent_1_candits[0].definition()\n",
    "    ent_2_text = ent_2_candits[0].definition()\n",
    "    relation_text = \"quality\"\n",
    "    \n",
    "    ent_1_examples = corrupt_examples_(ent_1_candits,text_a=ent_1_text,text_b=relation_text,text_c=ent_2_text, pos=\"head\")\n",
    "    ent_2_examples = corrupt_examples_(ent_2_candits,text_a=ent_1_text,text_b=relation_text,text_c=ent_2_text, pos=\"tail\")\n",
    "    \n",
    "    relation_examples = [relation_prediction.InputExample(guid=None, text_a=ent_1_text, text_b=ent_2_text, label=relation_id)]\n",
    "    \n",
    "    return ent_1_candits,ent_2_candits,ent_1_examples,ent_2_examples,relation_examples\n",
    "\n",
    "def validation(predict_lines, filename=ground_truth):\n",
    "    accuracy_left = 0\n",
    "    accuracy_relation = 0\n",
    "    accuracy_right = 0\n",
    "    \n",
    "    count= 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for p_line,g_line_str in zip(predict_lines,f):\n",
    "            g_line=g_line_str.strip().split(\"\\t\")\n",
    "            \n",
    "            # check head\n",
    "            if p_line[0]:\n",
    "                p_head = wn.synset(p_line[0].split(\":\")[1])\n",
    "                g_heads=g_line[0].split(\"|\")\n",
    "                temp = accuracy_left\n",
    "                for item in g_heads:\n",
    "                    if not item:\n",
    "                        continue\n",
    "                    try:\n",
    "                        g_head=wn.synset(item.split(\":\")[1])\n",
    "                    except:\n",
    "                        print(\"split error: \",item)\n",
    "\n",
    "                    if p_head==g_head:\n",
    "                        accuracy_left+=1\n",
    "                        break\n",
    "\n",
    "            # check tail\n",
    "            if p_line[2]:\n",
    "                p_tail = wn.synset(p_line[2].split(\":\")[1])\n",
    "                g_tails=g_line[2].split(\"|\")\n",
    "\n",
    "                for item in g_tails:\n",
    "                    if not item:\n",
    "                        continue\n",
    "                    try:\n",
    "                        g_tail=wn.synset(item.split(\":\")[1])\n",
    "                    except:\n",
    "                        print(item,item.split(\":\"))\n",
    "\n",
    "                    if p_tail==g_tail:\n",
    "                        accuracy_right+=1\n",
    "                        break\n",
    "                    \n",
    "            # check relation\n",
    "            if p_line[1]:\n",
    "                p_relation = wn.synset(p_line[1].split(\":\")[1])\n",
    "                g_relations=g_line[1].split(\"|\")\n",
    "\n",
    "                for item in g_relations:\n",
    "                    if not item:\n",
    "                        continue\n",
    "                    try:\n",
    "                        g_relation=wn.synset(item.split(\":\")[1])\n",
    "                    except:\n",
    "                        print(item,item.split(\":\"))\n",
    "\n",
    "                    if p_relation==g_relation:\n",
    "                        accuracy_relation+=1\n",
    "                        break\n",
    "    return accuracy_left/len(predict_lines),accuracy_right/len(predict_lines),accuracy_relation/len(predict_lines)\n",
    "\n",
    "def inverse_fre(length):\n",
    "    res= []\n",
    "    \n",
    "    for i in range(1,length+1):\n",
    "        res.append(1/i)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_test(alpha=0):\n",
    "    predict_lines = []\n",
    "    input_lines = [[None,None,None, line[3],line[4]]for line in lines]\n",
    "    for line in link_prediction.tqdm(input_lines, desc=\"Testing\"):\n",
    "        head_candits,tail_candits,head_examples,tail_examples,relation_examples=combine_examples_generation(line)\n",
    "\n",
    "        # predict relation\n",
    "        eval_features = relation_prediction.convert_examples_to_features(relation_examples, label_list_relation, \n",
    "                                                                         max_seq_length_relation, tokenizer)\n",
    "\n",
    "        all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "        eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # do predict\n",
    "        eval_sampler = relation_prediction.SequentialSampler(eval_data)\n",
    "        eval_dataloader = relation_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_relation)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device_relation)\n",
    "            input_mask = input_mask.to(device_relation)\n",
    "            segment_ids = segment_ids.to(device_relation)\n",
    "            label_ids = label_ids.to(device_relation)\n",
    "\n",
    "            with relation_prediction.torch.no_grad():\n",
    "                logits = model_relation(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = relation_prediction.np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = preds[0]\n",
    "\n",
    "        all_label_ids = all_label_ids.numpy()\n",
    "\n",
    "        result = []\n",
    "        for i, pred in enumerate(preds):\n",
    "            rel_values = relation_prediction.torch.tensor(pred)\n",
    "            rel_values = rel_values.cpu().numpy()\n",
    "            fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "            \n",
    "            rel_values = rel_values*(1-alpha)+np.array(fre_relation_list)*alpha\n",
    "            \n",
    "            argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "            result.append(argsort1[0][0])\n",
    "\n",
    "        predict_relation = label_list_relation[result[0]]\n",
    "\n",
    "        # head prediction\n",
    "        eval_features = link_prediction.convert_examples_to_features(head_examples, label_list_link, \n",
    "                                                                         max_seq_length_link, tokenizer)\n",
    "\n",
    "        all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "        eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # do predict\n",
    "        eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "        eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_link)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device_link)\n",
    "            input_mask = input_mask.to(device_link)\n",
    "            segment_ids = segment_ids.to(device_link)\n",
    "            label_ids = label_ids.to(device_link)\n",
    "\n",
    "            with link_prediction.torch.no_grad():\n",
    "                logits = model_link(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = link_prediction.np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        all_label_ids = all_label_ids.numpy()\n",
    "        preds = preds[0]\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = link_prediction.torch.tensor(rel_values)\n",
    "        rel_values = rel_values.cpu().numpy()\n",
    "\n",
    "        fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "\n",
    "        rel_values = rel_values*(1-alpha)+alpha*fre_list\n",
    "\n",
    "        argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "\n",
    "        idx_ = argsort1[0][0]\n",
    "        predict_output = head_candits[idx_]\n",
    "        predict_head=\"wn:\"+predict_output.name()\n",
    "\n",
    "        # tail prediction\\\n",
    "        eval_features = link_prediction.convert_examples_to_features(tail_examples, label_list_link, \n",
    "                                                                         max_seq_length_link, tokenizer)\n",
    "\n",
    "        all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "        all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "        eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        # do predict\n",
    "        eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "        eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_link)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device_link)\n",
    "            input_mask = input_mask.to(device_link)\n",
    "            segment_ids = segment_ids.to(device_link)\n",
    "            label_ids = label_ids.to(device_link)\n",
    "\n",
    "            with link_prediction.torch.no_grad():\n",
    "                logits = model_link(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = link_prediction.np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        all_label_ids = all_label_ids.numpy()\n",
    "        preds = preds[0]\n",
    "        rel_values = preds[:, all_label_ids[0]]\n",
    "        rel_values = link_prediction.torch.tensor(rel_values)\n",
    "        rel_values = rel_values.cpu().numpy()\n",
    "\n",
    "        fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "\n",
    "        rel_values = rel_values*(1-alpha)+alpha*fre_list\n",
    "\n",
    "        argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "\n",
    "        idx_ = argsort1[0][0]\n",
    "        predict_output = tail_candits[idx_]\n",
    "        predict_tail=\"wn:\"+predict_output.name()\n",
    "\n",
    "        predict_lines.append([predict_head,predict_relation,predict_tail,line[3],line[4]])\n",
    "    time.sleep(1)\n",
    "    print(print(\"alpha value: {}\".format(alpha)))\n",
    "    return predict_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:04<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "predict_lines = alpha_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cskg_prediction, \"w\") as f:\n",
    "    for line in predict_lines:\n",
    "        sentence=\"\\t\".join(line)\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39, 0.37, 0.78)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(predict_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:04<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.0\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   2%|▏         | 2/100 [00:00<00:07, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4, 0.3, 0.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:04<00:00, 20.97it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:07, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.1\n",
      "None\n",
      "(0.37, 0.34, 0.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 19.87it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:07, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.2\n",
      "None\n",
      "(0.52, 0.36, 0.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:06<00:00, 15.76it/s]\n",
      "Testing:   1%|          | 1/100 [00:00<00:11,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.3\n",
      "None\n",
      "(0.53, 0.4, 0.74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:06<00:00, 16.47it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:06, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.4\n",
      "None\n",
      "(0.64, 0.43, 0.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 17.47it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:07, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.5\n",
      "None\n",
      "(0.65, 0.49, 0.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 18.63it/s]\n",
      "Testing:   1%|          | 1/100 [00:00<00:11,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.6\n",
      "None\n",
      "(0.7, 0.49, 0.74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 16.74it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:07, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.7\n",
      "None\n",
      "(0.7, 0.5, 0.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:04<00:00, 20.05it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:08, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.8\n",
      "None\n",
      "(0.7, 0.49, 0.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 17.60it/s]\n",
      "Testing:   2%|▏         | 2/100 [00:00<00:06, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 0.9\n",
      "None\n",
      "(0.7, 0.49, 0.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:05<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha value: 1.0\n",
      "None\n",
      "(0.7, 0.49, 0.65)\n"
     ]
    }
   ],
   "source": [
    "for alpha in range(11):\n",
    "    alpha = alpha/10\n",
    "    predict_lines = alpha_test(alpha=alpha)\n",
    "    print(validation(predict_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24500589  0.38136557 -0.31796426 15.985915   -1.5484285   1.1072263\n",
      "  1.4132776   0.4049753  -0.32776767  0.1809184  -4.7585135  -0.62195414\n",
      "  0.503321    0.27086622 -1.1540339  -0.781703   -0.24960321 -1.2921071\n",
      "  0.43842727 -1.130424   -1.29697    -0.5626882  -0.7122333   0.529878\n",
      " -1.2836498  -1.662291   -0.19502428]\n",
      "[3.42065907 3.47564721]\n",
      "[ 3.09846354  0.01112205  3.34114218 -1.28109705 -4.65054989  0.80405474]\n"
     ]
    }
   ],
   "source": [
    "line=[None,None,None,\"mandarin orange\",\"orange\"]\n",
    "alpha = 0\n",
    "\n",
    "head_candits,tail_candits,head_examples,tail_examples,relation_examples=combine_examples_generation(line)\n",
    "\n",
    "# predict relation\n",
    "eval_features = relation_prediction.convert_examples_to_features(relation_examples, label_list_relation, \n",
    "                                                                 max_seq_length_relation, tokenizer)\n",
    "\n",
    "all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = relation_prediction.SequentialSampler(eval_data)\n",
    "eval_dataloader = relation_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_relation)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device_relation)\n",
    "    input_mask = input_mask.to(device_relation)\n",
    "    segment_ids = segment_ids.to(device_relation)\n",
    "    label_ids = label_ids.to(device_relation)\n",
    "\n",
    "    with relation_prediction.torch.no_grad():\n",
    "        logits = model_relation(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = relation_prediction.np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()\n",
    "\n",
    "result = []\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = relation_prediction.torch.tensor(pred)\n",
    "    rel_values = rel_values.cpu().numpy()\n",
    "\n",
    "    fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "\n",
    "    rel_values = rel_values*(1-alpha)\n",
    "\n",
    "    argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "    result.append(argsort1[0][0])\n",
    "\n",
    "predict_relation = label_list_relation[result[0]]\n",
    "\n",
    "print(rel_values)\n",
    "\n",
    "# head prediction\n",
    "eval_features = link_prediction.convert_examples_to_features(head_examples, label_list_link, \n",
    "                                                                 max_seq_length_link, tokenizer)\n",
    "\n",
    "all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_link)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device_link)\n",
    "    input_mask = input_mask.to(device_link)\n",
    "    segment_ids = segment_ids.to(device_link)\n",
    "    label_ids = label_ids.to(device_link)\n",
    "\n",
    "    with link_prediction.torch.no_grad():\n",
    "        logits = model_link(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = link_prediction.np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()\n",
    "preds = preds[0]\n",
    "rel_values = preds[:, all_label_ids[0]]\n",
    "rel_values = link_prediction.torch.tensor(rel_values)\n",
    "rel_values = rel_values.cpu().numpy()\n",
    "\n",
    "fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "\n",
    "rel_values = rel_values*(1-alpha)+alpha*fre_list\n",
    "print(rel_values)\n",
    "argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "\n",
    "idx_ = argsort1[0][0]\n",
    "predict_output = head_candits[idx_]\n",
    "predict_head=\"wn:\"+predict_output.name()\n",
    "\n",
    "# tail prediction\\\n",
    "eval_features = link_prediction.convert_examples_to_features(tail_examples, label_list_link, \n",
    "                                                                 max_seq_length_link, tokenizer)\n",
    "\n",
    "all_input_ids = relation_prediction.torch.tensor([f.input_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_input_mask = relation_prediction.torch.tensor([f.input_mask for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_segment_ids = relation_prediction.torch.tensor([f.segment_ids for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "all_label_ids = relation_prediction.torch.tensor([f.label_id for f in eval_features], dtype=relation_prediction.torch.long)\n",
    "\n",
    "eval_data = relation_prediction.TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = link_prediction.SequentialSampler(eval_data)\n",
    "eval_dataloader = link_prediction.DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size_link)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device_link)\n",
    "    input_mask = input_mask.to(device_link)\n",
    "    segment_ids = segment_ids.to(device_link)\n",
    "    label_ids = label_ids.to(device_link)\n",
    "\n",
    "    with link_prediction.torch.no_grad():\n",
    "        logits = model_link(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = link_prediction.np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()\n",
    "preds = preds[0]\n",
    "rel_values = preds[:, all_label_ids[0]]\n",
    "rel_values = link_prediction.torch.tensor(rel_values)\n",
    "rel_values = rel_values.cpu().numpy()\n",
    "\n",
    "fre_list = np.array(inverse_fre(len(rel_values)))\n",
    "\n",
    "rel_values = rel_values*(1-alpha)+alpha*fre_list\n",
    "\n",
    "argsort1 = sorted(enumerate(rel_values), key=lambda k:k[1], reverse=True)\n",
    "\n",
    "idx_ = argsort1[0][0]\n",
    "predict_output = tail_candits[idx_]\n",
    "predict_tail=\"wn:\"+predict_output.name()\n",
    "print(rel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wn:mandarin.n.05', 'wn:color.n.1', 'wn:orange.n.01')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_head,predict_relation,predict_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
