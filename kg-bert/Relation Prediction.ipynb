{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Relation prediction on WebChild property data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random, os, time, faiss, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file\n",
    "webchild = \"./data/wc/property.txt\"\n",
    "\n",
    "# ouput_file\n",
    "wc_gold_file = \"./data/wc/wc_gold_500k.tsv\"\n",
    "wc_entity_file = \"./data/wc/entities.txt\"\n",
    "wc_entity2id_file = \"./data/wc/entity2id.txt\"\n",
    "wc_entity2text_file = \"./data/wc/entity2text.txt\"\n",
    "wc_entity2textlong_file = \"./data/wc/entity2textlong.txt\"\n",
    "wc_relation_file = \"./data/wc/relations.txt\"\n",
    "wc_relation2id_file = \"./data/wc/relation2id.txt\"\n",
    "wc_relation2text_file = \"./data/wc/relation2text.txt\"\n",
    "wc_train_500k = \"./data/wc/train.tsv\"\n",
    "wc_dev_500k = \"./data/wc/dev.tsv\"\n",
    "wc_test_500k = \"./data/wc/test.tsv\"\n",
    "wc_train2id_500k = \"./data/wc/train2id.txt\"\n",
    "wc_dev2id_500k = \"./data/wc/valid2id.txt\"\n",
    "wc_test2id_500k = \"./data/wc/test2id.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gold_file(lines):\n",
    "    # Generate TSV file for relation classification\n",
    "    wn_gold_all = []\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        node1_id = line[0]\n",
    "        relation_id = line[1]\n",
    "        node2_id = line[2]\n",
    "        node1_labels = line[3]\n",
    "        node2_labels = line[4]\n",
    "        relation_label = line[5]\n",
    "\n",
    "        # modeify the node labels, check with leve distance\n",
    "        sent = line[8].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        node1_label = multiple_labels(node1_labels,node1_id)\n",
    "        node2_label = multiple_labels(node2_labels,node2_id)\n",
    "\n",
    "        wn_gold_all.append([node1_label, relation_label, node2_label, node1_id, node2_id,relation_id,sent])\n",
    "        i += 1\n",
    "        if i%10000==1:\n",
    "            print(f\"\\r {i}/{len(lines)}\", end=\"\")\n",
    "        \n",
    "    return wn_gold_all\n",
    "\n",
    "def id_generation(id_):\n",
    "    # transfer property.txt file id to kgtk format\n",
    "    if id_ == \"-\":\n",
    "        return \"wn:\"\n",
    "    id_ = id_.replace(\"#\",\".\").replace(\" \",\"_\")\n",
    "    id_=\"wn:\"+id_\n",
    "    return id_\n",
    "\n",
    "def label_generation(id_):\n",
    "    # transfer property.txt file id to kgtk format label\n",
    "    return id_.split(\":\")[1].split(\".\")[0].replace(\"_\",\" \")\n",
    "    \n",
    "def write_split_file(filename1, fielename2, lines,entity2detail,relation2detail):\n",
    "    # write train & train2id\n",
    "    with open(filename1,\"w\",newline='') as f1, open(fielename2,\"w\",newline='') as f2:\n",
    "        w1 = csv.writer(f1, delimiter='\\t')\n",
    "        w2 = csv.writer(f2, delimiter='\\t')\n",
    "\n",
    "        #write head\n",
    "        w2.writerow([len(lines)])\n",
    "        for line in lines:\n",
    "            # wtite content\n",
    "            entity1, relation, entity2 = line\n",
    "            entity1_id, entity1_text, entity1_textlong = entity2detail[entity1]\n",
    "            entity2_id, entity2_text, entity2_textlong = entity2detail[entity2]\n",
    "            relation_id, relation_text, relation_textlong = relation2detail[relation]\n",
    "            w1.writerow([entity1,relation,entity2])\n",
    "            w2.writerow([entity1_id,entity2_id,relation_id,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "head, lines = load_file(webchild,encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aardvark#n#1',\n",
       " 'quality#n#1',\n",
       " 'general#a#1',\n",
       " 'aardvark',\n",
       " 'general',\n",
       " '44',\n",
       " '1',\n",
       " '1',\n",
       " '2gms,',\n",
       " '0.453608',\n",
       " 'a']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triple example\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate file to kgtk file type\n",
    "# node1\trelation\tnode2\tnode1;label\tnode2;label\trelation;label\trelation;dimension\tsource\tsentence\n",
    "\n",
    "temp = []\n",
    "\n",
    "for line in lines:\n",
    "    entity1_id = id_generation(line[0])\n",
    "    relation_id = id_generation(line[1])\n",
    "    entity2_id = id_generation(line[2])\n",
    "    entity1_label = line[3]\n",
    "    entity2_label = line[4]\n",
    "    relation_label = label_generation(relation_id)\n",
    "    temp.append([entity1_id,relation_id,entity2_id,entity1_label,entity2_label,relation_label,\"\",\"wc\",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wn:aardvark.n.1',\n",
       " 'wn:quality.n.1',\n",
       " 'wn:general.a.1',\n",
       " 'aardvark',\n",
       " 'general',\n",
       " 'quality',\n",
       " '',\n",
       " 'wc',\n",
       " '']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kgtk_file example\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of triples without relations: 3673697\n",
      "The number of triples with relation: 2836191\n",
      "The number of unique nodes: 48076\n",
      "The number of unique relations: 27\n",
      "The most frequent node: wn:new.a.1\n",
      "The most frequent relation: wn:quality.n.1\n"
     ]
    }
   ],
   "source": [
    "# check whether it has triplr without predicate\n",
    "# if yes, remove this triples\n",
    "# check number of relations, unique nodes, total edges, distribution of relations, most frequent nodes\n",
    "\n",
    "valid_lines = []\n",
    "invalid_lines_count = 0\n",
    "unique_nodes = dict()\n",
    "relations_dict = dict()\n",
    "\n",
    "node2text = dict()\n",
    "relation2text = dict()\n",
    "\n",
    "for line in lines:\n",
    "    node1_id = line[0]\n",
    "    node2_id = line[2]\n",
    "    relation_id = line[1]\n",
    "    \n",
    "    node1_label = line[3]\n",
    "    node2_label = line[4]\n",
    "    relation_label = line[5]\n",
    "    \n",
    "    if relation_id == \"wn:\":\n",
    "        #filter no relation\n",
    "        invalid_lines_count += 1\n",
    "        continue\n",
    "    \n",
    "    # triple with relation\n",
    "    valid_lines.append(line)\n",
    "    \n",
    "    # count the number of unique nodes\n",
    "    unique_nodes[node1_id] = unique_nodes.get(node1_id,0)+1\n",
    "    unique_nodes[node2_id] = unique_nodes.get(node2_id,0)+1\n",
    "    \n",
    "    # count the number of unique relations\n",
    "    relations_dict[relation_id] = relations_dict.get(relation_id,0)+1\n",
    "    \n",
    "    node2text[node1_id] = node1_label\n",
    "    node2text[node2_id] = node2_label\n",
    "    relation2text[relation_id] = relation_label\n",
    "    \n",
    "# Summary\n",
    "print(f\"The number of triples without relations: {invalid_lines_count}\")\n",
    "print(f\"The number of triples with relation: {len(valid_lines)}\")\n",
    "print(f\"The number of unique nodes: {len(unique_nodes)}\")\n",
    "print(f\"The number of unique relations: {len(relations_dict)}\")\n",
    "max_fre_nodes = max(unique_nodes, key=unique_nodes.get)\n",
    "print(f\"The most frequent node: {max_fre_nodes}\")\n",
    "max_fre_relation = max(relations_dict, key=relations_dict.get)\n",
    "print(f\"The most frequent relation: {max_fre_relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(relations_dict.values(),log=True,bins=20)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Number of Edges Having One Specified Relation')\n",
    "plt.ylabel('Number of Relations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90001/100000"
     ]
    }
   ],
   "source": [
    "# random select 500k lines from data\n",
    "valid_lines_10k = random.choices(valid_lines, k=100000)\n",
    "\n",
    "# generate 500k lines gold file\n",
    "wc_gold_500k = generate_gold_file(valid_lines_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Data File**\n",
    "\n",
    "1. Generate Train, Dev and Test Dataset.\n",
    "2. Generate basic data files used for kg-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2detail = dict()\n",
    "relation2detail = dict()\n",
    "\n",
    "idx= 0\n",
    "for node_id in unique_nodes:\n",
    "    # build dict for each entity with their id, text, and long text\n",
    "    definition = wn.synset(node_id.split(\":\")[1]).definition()\n",
    "\n",
    "    entity_label = node2text[node_id]\n",
    "    entity2detail[node_id]=[idx, entity_label, definition]\n",
    "    idx += 1\n",
    "\n",
    "idx = 0\n",
    "for relation_id in relation2text:\n",
    "    # build dict for each relation with their id, text, and long text\n",
    "    definition = wn.synset(relation_id.split(\":\")[1].replace(\" \",\"_\")).definition()\n",
    "    relation_label = relation2text[relation_id]\n",
    "    relation2detail[relation_id]=[idx, relation_label, definition]\n",
    "    idx += 1\n",
    "    \n",
    "# Transfer gold data to the structure of kg-bert\n",
    "kgBert_data_sent = [[_[3],_[5],_[4],_[6]] for _ in wc_gold_500k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "# entities.txt, entity2id, entity2text, entity2textlong\n",
    "\n",
    "with open(wc_entity_file,\"w\",newline='') as f1, open(wc_entity2id_file,\"w\",newline='') as f2,\\\n",
    "open(wc_entity2text_file, \"w\",newline='') as f3, open(wc_entity2textlong_file,\"w\",newline='') as f4:\n",
    "    w1 = csv.writer(f1, delimiter='\\t')\n",
    "    w2 = csv.writer(f2, delimiter='\\t')\n",
    "    w3 = csv.writer(f3, delimiter='\\t')\n",
    "    w4 = csv.writer(f4, delimiter='\\t')\n",
    "    \n",
    "    #write head\n",
    "    w2.writerow([len(entity2detail)])\n",
    "    for entity in entity2detail:\n",
    "        # wtite content\n",
    "        entity_id, entity_text, entity_textlong = entity2detail[entity]\n",
    "        w1.writerow([entity,])\n",
    "        w2.writerow([entity,entity_id])\n",
    "        w3.writerow([entity,entity_text])\n",
    "        w4.writerow([entity,entity_textlong])\n",
    "        \n",
    "# relations.txt, rekation2id.txt, relation2text.txt\n",
    "wc_relation_file = \"./data/wc/relations.txt\"\n",
    "wc_relation2id_file = \"./data/wc/relation2id.txt\"\n",
    "wc_relation2text_file = \"./data/wc/relation2text.txt\"\n",
    "with open(wc_relation_file,\"w\",newline='') as f1, open(wc_relation2id_file,\"w\",newline='') as f2,\\\n",
    "open(wc_relation2text_file, \"w\",newline='') as f3:\n",
    "    w1 = csv.writer(f1, delimiter='\\t')\n",
    "    w2 = csv.writer(f2, delimiter='\\t')\n",
    "    w3 = csv.writer(f3, delimiter='\\t')\n",
    "    \n",
    "    #write head\n",
    "    w2.writerow([len(relation2detail)])\n",
    "    for relation in relation2detail:\n",
    "        # wtite content\n",
    "        relation_id, relation_text, relation_textlong = relation2detail[relation]\n",
    "        w1.writerow([relation,])\n",
    "        w2.writerow([relation,relation_id])\n",
    "        w3.writerow([relation,relation_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train, dev, test:  80000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# Split train, dev, test file\n",
    "\n",
    "random.shuffle(kgBert_data_sent)\n",
    "\n",
    "# define ratio of train, dev, test\n",
    "train_ratio = 0.8\n",
    "dev_ratio = 0.1\n",
    "test_ratio = 1 - train_ratio - dev_ratio\n",
    "\n",
    "len_gold = len(kgBert_data_sent)\n",
    "train, dev, test = np.split(kgBert_data_sent,[int(train_ratio*len_gold),int((train_ratio+dev_ratio)*len_gold)])\n",
    "print(\"length of train, dev, test: \", len(train), len(dev), len(test))\n",
    "\n",
    "# write train & train2id\n",
    "write_split_file(wc_train_500k, wc_train2id_500k, [_[0:-1] for _ in train],entity2detail,relation2detail)\n",
    "\n",
    "# write dev & dev2id\n",
    "write_split_file(wc_dev_500k, wc_dev2id_500k, [_[0:-1] for _ in dev],entity2detail,relation2detail)\n",
    "\n",
    "# write test & test2id\n",
    "write_split_file(wc_test_500k, wc_test2id_500k, [_[0:-1] for _ in test],entity2detail,relation2detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wn:simulation.n.3', 'wn:state.n.2', 'wn:immune.a.1', ''],\n",
       "      dtype='<U37')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of gold file\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_list_generation(file_path):\n",
    "    with open(file_path,\"r\") as f:\n",
    "        relation_list = []\n",
    "        for line in f:\n",
    "            relation_list.append(line.strip().split(\"\\t\")[0])\n",
    "        return relation_list\n",
    "\n",
    "def MRS(wn_gold, relation_list):\n",
    "    # Random Baseline calculation\n",
    "    # only predict subject\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        entity1  = line[0]\n",
    "        entity2 = line[2]\n",
    "        relation_label = line[3]\n",
    "        \n",
    "        # generate all possible combination of \"_\"&\"-\" and check whether it can find sysets.\n",
    "        # if the combination can generate sysets, use this combinations to generate candidates\n",
    "        \n",
    "        relation_id = wn.synset(random.choice(relation_list).split(\":\")[1])\n",
    "        \n",
    "        node1_id=\"\"\n",
    "        node2_id=\"\"\n",
    "        \n",
    "        wn_predict.append([entity1, relation_id, entity2, relation_label])\n",
    "        \n",
    "    return wn_predict\n",
    "\n",
    "def modify_data(lines,relation2detail):\n",
    "    # add label in the dataset used to make prediction\n",
    "    for line in lines:\n",
    "        entity1, relation, entity2, sent = line\n",
    "        relation_label = relation2detail[relation][1]\n",
    "        yield entity1, relation, entity2, relation_label, sent\n",
    "\n",
    "def validation(wn_predict, wn_gold):\n",
    "    # valid the accuracy of prediction: only compare the accuracy of prediction\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for predict, actual in zip(wn_predict, wn_gold):\n",
    "        predict_label1 = predict[1]\n",
    "        actual_label1 = wn.synset(actual[1].split(\":\")[1].replace(\" \",\"_\"))\n",
    "        if not predict_label1:\n",
    "            continue\n",
    "        if predict_label1 == actual_label1:\n",
    "            # predict and actual is same\n",
    "            correct += 1\n",
    "            \n",
    "    return correct/len(wn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_list = relation_list_generation(wc_relation2text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify train, dev, test data\n",
    "train_modify = list(modify_data(train,relation2detail))\n",
    "dev_modify = list(modify_data(dev,relation2detail))\n",
    "test_modify = list(modify_data(test,relation2detail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train dataset (random baseline):  0.0372375\n",
      "Accuracy of dev dataset (random baseline):  0.0387\n",
      "Accuracy of test dataset (random baseline):  0.0386\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "wc_ran_train_predict = MRS(train_modify,relation_list)\n",
    "accuracy = validation(wc_ran_train_predict, train_modify)\n",
    "print(\"Accuracy of train dataset (random baseline): \", accuracy)\n",
    "\n",
    "#dev dataset\n",
    "wc_ran_dev_predict = MRS(dev_modify,relation_list)\n",
    "accuracy = validation(wc_ran_dev_predict, dev_modify)\n",
    "print(\"Accuracy of dev dataset (random baseline): \", accuracy)\n",
    "\n",
    "#test dataset\n",
    "wc_ran_test_predict = MRS(test_modify,relation_list)\n",
    "accuracy = validation(wc_ran_test_predict, test_modify)\n",
    "print(\"Accuracy of test dataset (random baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MFS Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFS(wn_gold, most_frequency_relation = \"wn:quality.n.1\"):\n",
    "    # Frequent Baseline Calculation\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        entity1  = line[0]\n",
    "        entity2 = line[2]\n",
    "        relation_label = line[3]\n",
    "        \n",
    "        # generate all possible combination of \"_\"&\"-\" and check whether it can find sysets.\n",
    "        # if the combination can generate sysets, use this combinations to generate candidates\n",
    "        relation_id = wn.synset(most_frequency_relation.split(\":\")[1])\n",
    "        \n",
    "        node1_id=\"\"\n",
    "        node2_id=\"\"\n",
    "        \n",
    "        wn_predict.append([entity1, relation_id, entity2, relation_label])\n",
    "    return wn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train dataset (frequency baseline):  0.3814875\n",
      "Accuracy of dev dataset (frequency baseline):  0.3757\n",
      "Accuracy of test dataset (frequency baseline):  0.3913\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "wc_ran_train_predict = MFS(train_modify)\n",
    "accuracy = validation(wc_ran_train_predict, train_modify)\n",
    "print(\"Accuracy of train dataset (frequency baseline): \", accuracy)\n",
    "\n",
    "#dev dataset\n",
    "wc_ran_dev_predict = MFS(dev_modify)\n",
    "accuracy = validation(wc_ran_dev_predict, dev_modify)\n",
    "print(\"Accuracy of dev dataset (frequency baseline): \", accuracy)\n",
    "\n",
    "#test dataset\n",
    "wc_ran_test_predict = MFS(test_modify)\n",
    "accuracy = validation(wc_ran_test_predict, test_modify)\n",
    "print(\"Accuracy of test dataset (frequency baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STBert Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STB = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wn:quality.n.1',\n",
       " 'wn:trait.n.1',\n",
       " 'wn:age.n.1',\n",
       " 'wn:color.n.1',\n",
       " 'wn:beauty.n.1',\n",
       " 'wn:shape.n.2',\n",
       " 'wn:size.n.1',\n",
       " 'wn:state.n.2',\n",
       " 'wn:weight.n.1',\n",
       " 'wn:emotion.n.1']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wn:simulation.n.3', 'wn:state.n.2', 'wn:immune.a.1', 'state', '')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_modify[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_normalize(vector):\n",
    "    # input should be a numpy array\n",
    "    # change to unit vector\n",
    "    return vector / (vector**2).sum()**0.5\n",
    "\n",
    "def label2sentence2sent(relation_id, model,label_synsets,sents_combine):\n",
    "\n",
    "    candidates = wn.synset(relation_id.split(\":\")[1])\n",
    "    candits_sent = candidates.definition()\n",
    "    \n",
    "    sents_combine.append(candits_sent)\n",
    "    label_synsets.append(candidates)\n",
    "            \n",
    "    return label_synsets,sents_combine\n",
    "\n",
    "def max_candidate_faiss(label_,sent_embedding_,label_embeddings):\n",
    "    # return the max similarity candidates\n",
    "    #output: [[similarity, synset, the pos of synset]]\n",
    "\n",
    "    sent_embedding_ = vector_normalize(sent_embedding_)\n",
    "        \n",
    "    top_n = 1\n",
    "    index_ = label_embeddings[1]\n",
    "    _, I = index_.search(np.array([sent_embedding_]), top_n)\n",
    "    #print(int(I),label_embeddings[label_][0])\n",
    "    return label_embeddings[0][int(I)]\n",
    "\n",
    "def candidates_embeddings_faiss(relation_list, model):\n",
    "    # generate label node id defination embeddings from file\n",
    "    # output:{\"label_name\":[[node_id, embedding of node_id defination],[X,X],[X,X]]}\n",
    "    \n",
    "    # store label, synset\n",
    "    label_synsets = []\n",
    "    # store the defination sentence of synset\n",
    "    sents_combine = []\n",
    "    \n",
    "    embeddings = list()\n",
    "    \n",
    "    sents_combine =[]\n",
    "    label_synsets =[]\n",
    "    \n",
    "    for relation in relation_list:\n",
    "        candidate = wn.synset(relation.split(\":\")[1])\n",
    "        candit_sent = candidate.definition()\n",
    "\n",
    "        sents_combine.append(candit_sent)\n",
    "        label_synsets.append(candidate)\n",
    "        \n",
    "        #if count1%1000==0:print(f\"\\r lines counting {count1}/{length1}\",end=\"\")\n",
    "\n",
    "    # generate embedding of sentence\n",
    "    start = time.time()\n",
    "    sents_embed = model.encode(sents_combine)\n",
    "    sents_embed = [vector_normalize(_) for _ in sents_embed]\n",
    "    # dimension of faiss\n",
    "    d = len(sents_embed[0])\n",
    "    end = time.time()\n",
    "    \n",
    "    # write embedding into faiss\n",
    "    index_ = faiss.IndexFlatL2(d)\n",
    "    index_.add(np.array(sents_embed))\n",
    "    \n",
    "    return [label_synsets,index_]\n",
    "\n",
    "def sentence_embedding_faiss(wn_gold, model, label_embeddings = None):\n",
    "    # use sentences embedding to find most similar candit\n",
    "    wn_predict = []\n",
    "    sents_combine = []\n",
    "    \n",
    "    length1 = len(wn_gold)\n",
    "    count1 = 0\n",
    "    for line in wn_gold:\n",
    "        label1 = line[0]\n",
    "        label2 = line[2]\n",
    "        relation_label=line[1]\n",
    "        \n",
    "        sentence = label1 + f\" {relation_label} \"+ label2\n",
    "        sents_combine.append(sentence)\n",
    "        \n",
    "        count1 += 1\n",
    "        #if count1%1000==0:print(f\"\\r lines counting {count1}/{length1}\",end=\"\")\n",
    "    # obtain sentence embedding\n",
    "    start = time.time()\n",
    "    sents_embedding = model.encode(sents_combine)\n",
    "    end = time.time()\n",
    "    #print(f\"model time: {end-start}\")\n",
    "    \n",
    "    length2 = len(wn_gold)\n",
    "    count2 = 0\n",
    "    for line,sent_embedding in zip(wn_gold,sents_embedding):\n",
    "        relation_label = line[3]\n",
    "\n",
    "        #obtain the max similar item for label1\n",
    "        relation_id = max_candidate_faiss(relation_label,sent_embedding,label_embeddings) \n",
    "                \n",
    "        wn_predict.append([line[0], relation_id, line[2],line[3],line[4]])\n",
    "        count2 +=1\n",
    "        #print(f\"\\r line countung {count2}/{length2}\", end=\"\")\n",
    "    return wn_predict\n",
    "\n",
    "def chunks_divide(data, num=10000):\n",
    "    for idx in range(0,len(data),num):\n",
    "        yield data[idx:idx+num]\n",
    "\n",
    "def process_data_inChunk(data,relation_list, model, chunk_num = 10000):\n",
    "    # data file is processed in chunks\n",
    "    total = 0\n",
    "    total_predict = []\n",
    "    start_total = time.time()\n",
    "    \n",
    "    label_embeddings = candidates_embeddings_faiss(relation_list, model)\n",
    "    for sub_data in chunks_divide(data, num=chunk_num):\n",
    "        start = time.time()\n",
    "        predict = sentence_embedding_faiss(sub_data, model,label_embeddings = label_embeddings)\n",
    "        \n",
    "        total_predict += predict\n",
    "        process_num = len(sub_data)\n",
    "        total +=process_num\n",
    "        usedtime = time.time() - start\n",
    "        print(f\"The time used for this iteration: {usedtime}, finished lines {total}/{len(data)}\")\n",
    "    accuracy = validation(total_predict, data)\n",
    "    \n",
    "    total_time = time.time()-start_total\n",
    "    print(f\"Process finished! Total time: {total_time}\")\n",
    "    return total_predict, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wc_ran_train_predict, accuracy = process_data_inChunk(dev_modify[:1000],relation_list, model_STB, chunk_num = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time used for this iteration: 13.924280643463135, finished lines 10000/80000\n",
      "The time used for this iteration: 13.578641414642334, finished lines 20000/80000\n",
      "The time used for this iteration: 17.39479970932007, finished lines 30000/80000\n",
      "The time used for this iteration: 18.082597732543945, finished lines 40000/80000\n",
      "The time used for this iteration: 18.18126106262207, finished lines 50000/80000\n",
      "The time used for this iteration: 16.356355667114258, finished lines 60000/80000\n",
      "The time used for this iteration: 15.864845037460327, finished lines 70000/80000\n",
      "The time used for this iteration: 18.58260464668274, finished lines 80000/80000\n",
      "Process finished! Total time: 132.57569026947021\n",
      "Accuracy of train dataset (STB baseline):  0.2364\n",
      "\n",
      "\n",
      "The time used for this iteration: 18.73482847213745, finished lines 10000/10000\n",
      "Process finished! Total time: 18.825804471969604\n",
      "Accuracy of dev dataset (STB baseline):  0.2333\n",
      "\n",
      "\n",
      "The time used for this iteration: 18.350280046463013, finished lines 10000/10000\n",
      "Process finished! Total time: 18.44671106338501\n",
      "Accuracy of test dataset (STB baseline):  0.2339\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "wc_stb_train_predict, accuracy = process_data_inChunk(train_modify,relation_list, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of train dataset (STB baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#dev dataset\n",
    "wc_stb_dev_predict, accuracy = process_data_inChunk(dev_modify,relation_list, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of dev dataset (STB baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#test dataset\n",
    "wc_stb_test_predict, accuracy = process_data_inChunk(test_modify,relation_list, model_STB, chunk_num = 10000)\n",
    "print(\"Accuracy of test dataset (STB baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRoberta Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STR = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time used for this iteration: 43.88202619552612, finished lines 10000/80000\n",
      "The time used for this iteration: 48.05389618873596, finished lines 20000/80000\n",
      "The time used for this iteration: 42.710233211517334, finished lines 30000/80000\n",
      "The time used for this iteration: 47.38363552093506, finished lines 40000/80000\n",
      "The time used for this iteration: 46.39508128166199, finished lines 50000/80000\n",
      "The time used for this iteration: 45.641247272491455, finished lines 60000/80000\n",
      "The time used for this iteration: 48.41848111152649, finished lines 70000/80000\n",
      "The time used for this iteration: 43.725785970687866, finished lines 80000/80000\n",
      "Process finished! Total time: 367.1820297241211\n",
      "Accuracy of train dataset (STR baseline):  0.170025\n",
      "\n",
      "\n",
      "The time used for this iteration: 48.297616481781006, finished lines 10000/10000\n",
      "Process finished! Total time: 48.4793484210968\n",
      "Accuracy of dev dataset (STR baseline):  0.1689\n",
      "\n",
      "\n",
      "The time used for this iteration: 44.61561417579651, finished lines 10000/10000\n",
      "Process finished! Total time: 44.798256158828735\n",
      "Accuracy of test dataset (STR baseline):  0.1648\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "wc_str_train_predict, accuracy = process_data_inChunk(train_modify,relation_list, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of train dataset (STR baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "#dev dataset\n",
    "wc_str_dev_predict, accuracy = process_data_inChunk(dev_modify,relation_list, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of dev dataset (STR baseline): \", accuracy)\n",
    "print(\"\\n\")\n",
    "\n",
    "#test dataset\n",
    "wc_str_test_predict, accuracy = process_data_inChunk(test_modify,relation_list, model_STR, chunk_num = 10000)\n",
    "print(\"Accuracy of test dataset (STR baseline): \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isi",
   "language": "python",
   "name": "isi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
