{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bert_relation_prediction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '2'\n",
    "data_path = \"./data/wc\"\n",
    "data_saved_path = \"./output_wc_result\"\n",
    "bert_model=\"bert-base-cased\"\n",
    "task_name=\"kg\"\n",
    "max_seq_length=25\n",
    "eval_batch_size=32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precessor\n",
    "processors = {\"kg\": KGProcessor,}\n",
    "processor = processors[task_name]()\n",
    "\n",
    "# obtain label\n",
    "label_list = processor.get_relations(data_path)\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# obtain entity list\n",
    "entity_list = processor.get_entities(data_path)\n",
    "\n",
    "# load model\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(data_saved_path, num_labels=num_labels)\n",
    "location_detail=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_accuracy(ranks):\n",
    "    # check the accuracy for different hits\n",
    "    max_dep = 1\n",
    "    threshold = 0\n",
    "    accuracy_dict = dict()\n",
    "    ite = 0\n",
    "    \n",
    "    while threshold <= max_dep and ite <500:\n",
    "        for rank in ranks:\n",
    "            max_dep = max(max_dep, rank)\n",
    "            if rank <= threshold:\n",
    "                accuracy_dict[threshold] = accuracy_dict.get(threshold,0)+1\n",
    "                \n",
    "        accuracy_dict[threshold] = accuracy_dict[threshold]/len(ranks)\n",
    "        threshold += 1\n",
    "        ite += 1\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2500/2500 [02:45<00:00, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0307009  -0.8751827  -1.545766   ... -3.4059386  -4.093295\n",
      "  -4.07487   ]\n",
      " [13.214246   -0.2566802  -0.93723476 ... -3.4859982  -3.6727476\n",
      "  -2.056883  ]\n",
      " [ 0.57458293 -0.88107705 -1.1419075  ... -3.3871107  -3.8235722\n",
      "  -3.6838179 ]\n",
      " ...\n",
      " [13.230572   -0.49823722 -0.56481314 ... -3.1396358  -3.620214\n",
      "  -1.97463   ]\n",
      " [13.343311   -0.0898383  -0.91775936 ... -3.6229043  -3.5652847\n",
      "  -2.1837394 ]\n",
      " [13.411948    0.04543418 -0.32135367 ... -3.4476154  -4.0423665\n",
      "  -2.4703321 ]] (80000, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "eval_examples = processor.get_train_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9946375"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:20<00:00, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44482747 -1.1770259  -0.76383555 ... -2.8228853  -3.4480188\n",
      "  -3.615299  ]\n",
      " [12.977062   -0.5456654  -1.4575281  ... -3.1515899  -3.587447\n",
      "  -2.2301178 ]\n",
      " [ 0.9858337  -0.38104838 -0.893118   ... -0.9728924  -0.69301426\n",
      "  -1.2253376 ]\n",
      " ...\n",
      " [ 0.35629842 -0.91432303 -1.0345154  ... -2.9057736  -3.9165428\n",
      "  -3.7044432 ]\n",
      " [ 1.1982713  -1.240171   -0.9476489  ... -3.6007466  -3.8590267\n",
      "  -3.6919255 ]\n",
      " [13.138125   -0.42995504 -0.79695964 ... -3.5715022  -3.910891\n",
      "  -2.2566068 ]] (10000, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dev data\n",
    "eval_examples = processor.get_dev_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9942"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:20<00:00, 15.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.168325   -0.49104282 -1.2100158  ... -3.2562113  -3.8417802\n",
      "  -1.9294806 ]\n",
      " [ 0.8387841  -1.2297618  -1.4677217  ... -2.0106244  -3.3389864\n",
      "  -2.633492  ]\n",
      " [13.231341   -0.53429186 -0.46936047 ... -3.2200503  -4.0137944\n",
      "  -1.8392601 ]\n",
      " ...\n",
      " [13.291885   -0.8537414  -0.24956194 ... -3.67075    -3.7896547\n",
      "  -2.2077584 ]\n",
      " [13.464903   -0.72202873 -0.7406412  ... -3.1743302  -3.9518793\n",
      "  -2.504445  ]\n",
      " [ 2.6166244  -1.5206773  -1.5587469  ... -4.4415092  -4.412207\n",
      "  -4.640136  ]] (10000, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "eval_examples = processor.get_test_examples(data_path)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# do predict\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Testing\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "print(preds, preds.shape)\n",
    "\n",
    "all_label_ids = all_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rank of the correct answer location in the \n",
    "ranks = []\n",
    "filter_ranks = []\n",
    "hits = []\n",
    "hits_filter = []\n",
    "for i in range(10):\n",
    "    hits.append([])\n",
    "    hits_filter.append([])\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    rel_values = torch.tensor(pred)\n",
    "    _, argsort1 = torch.sort(rel_values, descending=True)\n",
    "    argsort1 = argsort1.cpu().numpy()\n",
    "\n",
    "    rank = np.where(argsort1 == all_label_ids[i])[0][0]\n",
    "    #print(argsort1, all_label_ids[i], rank)\n",
    "    ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9949"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict = rank_accuracy(ranks)\n",
    "# show the accuracy for highest rank candit\n",
    "accuracy_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
