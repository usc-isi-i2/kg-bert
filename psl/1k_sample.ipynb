{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "julian-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, math, random, re,pickle, os\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "broad-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "cskg_embeddings_file=\"./cskg_embedding/cskg_embeddings.txt\"\n",
    "cskg_connected_file=\"../kg-bert/data/cskg/cskg_connected.tsv\"\n",
    "RICA_file=\"./RICA/RICA_material_KnowledgeTable.csv\"\n",
    "sample_1k_lines=\"./1k_lines/test_sentences.txt\"\n",
    "ground_truth_file=\"./1k_lines/test_sentences_m.txt\"\n",
    "\n",
    "# output\n",
    "temp_result = \"./cskg_embedding/result.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-grain",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "architectural-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design dependency rule\n",
    "class token_text():\n",
    "    # build a class to generate empty token\n",
    "    def __init__(self, text):\n",
    "        self.text=text\n",
    "\n",
    "def AddPeriod(sent_text):\n",
    "    # add period to sentences\n",
    "    if sent_text[-1]!=\".\":\n",
    "        sent_text+=\".\"\n",
    "    return sent_text\n",
    "\n",
    "def subitem_depCheck(subs, require={}):\n",
    "    # find the items having the required relation\n",
    "    output_token=token_text(\"\")\n",
    "    for sub in subs:\n",
    "        rel=sub.dep_\n",
    "        \n",
    "        if rel in require:\n",
    "            output_token=sub\n",
    "            break\n",
    "            \n",
    "    return output_token\n",
    "\n",
    "def walk_tree(node, depth, depths={}):\n",
    "    # walk through the sentence and find the depth\n",
    "    depths[node] = depth\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return [walk_tree(child, depth + 1,depths=depths) for child in node.children]\n",
    "\n",
    "def find_max_idx(token, res=[]):\n",
    "    # find the most right subtoken idx \n",
    "    if token.n_rights> 0:\n",
    "        for child in token.rights:\n",
    "            res.append(child.i)\n",
    "            find_max_idx(child, res=res)\n",
    "    else:\n",
    "        res.append(token.i)\n",
    "    return max(res)\n",
    "\n",
    "def find_min_idx(token, res=[]):\n",
    "    # find the most right subtoken idx \n",
    "    if token.n_lefts> 0:\n",
    "        for child in token.lefts:\n",
    "            res.append(child.i)\n",
    "            find_min_idx(child, res=res)\n",
    "    else:\n",
    "        res.append(token.i)\n",
    "    return min(res)\n",
    "    \n",
    "def find_end_tree(root,res=[], left_=False, right_=False):\n",
    "    # find the end token of dependency Tree.\n",
    "    # left_ means find left hand side children\n",
    "    # right_ means find right hand side children\n",
    "    if root.n_lefts*left_ + root.n_rights*right_ > 0:\n",
    "        if left_ and right_:\n",
    "            for child in root.children:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif left_:\n",
    "            for child in root.lefts:\n",
    "                res.append(child)\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif right_:\n",
    "            for child in root.rights:\n",
    "                res.append(child)\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "    else:\n",
    "        res.append(root)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def SentencePartDetection(sent):\n",
    "    # split sentence into different part\n",
    "    # output the root token idx of each part\n",
    "        # the span for different token\n",
    "        \n",
    "    doc=sent.doc\n",
    "    spanLength_list=[]\n",
    "    for token in sent:\n",
    "        token_index=token.i\n",
    "        for child in token.children:\n",
    "            child_index = child.i\n",
    "            \n",
    "            if child.dep_==\"punct\":\n",
    "                continue\n",
    "            spanLength_list.append((abs(token_index-child_index),(token_index, child_index)))\n",
    "            \n",
    "    spanLength_list.sort(reverse=True)\n",
    "    #print(spanLength_list)\n",
    "    part_pos=set()\n",
    "    \n",
    "    depths={}\n",
    "    #print(sent.root)\n",
    "    [walk_tree(sent.root, 0, depths=depths) for sent in doc.sents]\n",
    "    part_pos.add(sent.root.i)\n",
    "    #print(depths)\n",
    "    for span_length, tokens in spanLength_list:\n",
    "        token1_index, token2_index=tokens\n",
    "        token1=doc[token1_index]\n",
    "        token2=doc[token2_index]\n",
    "        #print(part_pos, token1,token2,depths[token2])\n",
    "        if len(list(token2.children))<1 or depths[token2]>2:\n",
    "            continue\n",
    "        \n",
    "        if token2.dep_ in {\"conj\",\"ccomp\",\"advcl\",\"advmod\"} and token2.pos_ not in {\"ADJ\", \"PUNCT\"}:\n",
    "            \n",
    "            if token1_index not in part_pos and token2_index not in part_pos and len(part_pos)>=2:\n",
    "                continue\n",
    "            \n",
    "            part_pos.add(token1_index)\n",
    "            part_pos.add(token2_index)\n",
    "        \n",
    "        if len(part_pos)>=3:\n",
    "            break\n",
    "            \n",
    "    part_pos=sorted(list(part_pos))\n",
    "    return part_pos\n",
    "\n",
    "def elements_extraction_isa(sent_text):\n",
    "    # extract elemenmt from isa format: \"X is A, Y is B, X is more/less than B\"\n",
    "    \n",
    "    sent_text=AddPeriod(sent_text)\n",
    "    \n",
    "    # obtain parsed sentences\n",
    "    doc=nlp(sent_text)\n",
    "    sent=list(doc.sents)[0]\n",
    "    \n",
    "    # find root token idx for each sentence part\n",
    "    part_pos=SentencePartDetection(sent)\n",
    "    # final compare part\n",
    "    compare_root=sent[part_pos[2]]\n",
    "    \n",
    "    # find reasoning part\n",
    "    reasoning_first_root=sent[part_pos[0]]\n",
    "    reasoning_second_root=sent[part_pos[1]]\n",
    "    \n",
    "    # generate object1 and object2 token candidates\n",
    "    leftEnd_compare_tokens=find_end_tree(compare_root,left_=True,right_=False, res=[])\n",
    "    leftEnd_first_tokens=find_end_tree(reasoning_first_root,left_=True,right_=False, res=[])\n",
    "    leftEnd_second_tokens=find_end_tree(reasoning_second_root,left_=True,right_=False, res=[])\n",
    "    \n",
    "    # find object1 and object2\n",
    "    object1, object2=token_text(\"\"),token_text(\"\")\n",
    "    \n",
    "    if len(leftEnd_first_tokens)==1:\n",
    "        object2=leftEnd_first_tokens[0]\n",
    "        \n",
    "    else:\n",
    "        for token in leftEnd_first_tokens:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                object2 = token\n",
    "                \n",
    "            if token.dep_ in {\"nsubj\",\"acomp\"}:\n",
    "                object2=token\n",
    "                \n",
    "            if list(token.ancestors)[0].text == \"is\":\n",
    "                object2 = token\n",
    "            \n",
    "    if len(leftEnd_second_tokens)==1:\n",
    "        object1=leftEnd_second_tokens[0]\n",
    "        \n",
    "    else:\n",
    "        for token in leftEnd_second_tokens:\n",
    "            if token.i < part_pos[0]:\n",
    "                continue\n",
    "                \n",
    "            if token.pos_ == \"NOUN\":\n",
    "                object1 = token\n",
    "            \n",
    "            if token.dep_ in {\"nsubj\",\"acomp\"}:\n",
    "                object1=token\n",
    "                \n",
    "            if list(token.ancestors)[0].text == \"is\":\n",
    "                object1 = token\n",
    "            \n",
    "    # use than find one object candidates\n",
    "    for token in sent:\n",
    "        if token.text == \"than\":\n",
    "            than_token=token\n",
    "            \n",
    "            than_child = list(than_token.children)\n",
    "            than_anc= list(than_token.ancestors)\n",
    "            \n",
    "            if than_child:\n",
    "                object_temp =than_child[0]\n",
    "                \n",
    "            else:\n",
    "                object_temp=than_anc[0]\n",
    "                \n",
    "    objects_text=[object1.text, object2.text]\n",
    "    \n",
    "    # replace empty object result with object candidates found by than\n",
    "    if object_temp.text in objects_text:\n",
    "        pass\n",
    "    else:\n",
    "        if objects_text[0]==\"\":\n",
    "            object1=object_temp\n",
    "        else:\n",
    "            object2=object_temp\n",
    "            \n",
    "    # find the property of each objects\n",
    "    rightEnd_first_tokens=find_end_tree(reasoning_first_root,left_=False,right_=True, res=[])\n",
    "    rightEnd_second_tokens=find_end_tree(reasoning_second_root,left_=False,right_=True, res=[])\n",
    "    \n",
    "    max_dep_obj2=0\n",
    "    \n",
    "    for token in rightEnd_first_tokens:\n",
    "        idx = token.i\n",
    "        if token.i > part_pos[1]:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            max_dep_obj2=max(max_dep_obj2, idx)\n",
    "            \n",
    "    max_dep_obj1=0\n",
    "    for token in rightEnd_second_tokens:\n",
    "        idx = token.i\n",
    "        if token.i > part_pos[2]:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            max_dep_obj1=max(max_dep_obj1, idx)\n",
    "            \n",
    "    obj2_property=sent[part_pos[0]+1:max_dep_obj2+1].text\n",
    "    obj1_property=sent[part_pos[1]+1:max_dep_obj1+1].text\n",
    "    \n",
    "    # we should knwo obj2_property is the property of obj1 or obj2\n",
    "    # already know obj2_property apears ealier than obj1_property in the sentence.\n",
    "    object1_loc=sent.text.find(object1.text)\n",
    "    object2_loc=sent.text.find(object2.text)\n",
    "    \n",
    "    if object1_loc <= object2_loc:\n",
    "        temp=obj2_property\n",
    "        obj2_property=obj1_property\n",
    "        obj1_property=temp\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # find compare aspect\n",
    "    aspect_token=subitem_depCheck(compare_root.rights, require={\"acomp\",\"attr\"})\n",
    "    \n",
    "    if aspect_token.text:\n",
    "        aspect_index=aspect_token.i\n",
    "        for left in aspect_token.lefts:\n",
    "            rel=left.dep_\n",
    "            if rel not in [\"advmod\"] and left.i < aspect_index:\n",
    "                aspect_index= left.i\n",
    "\n",
    "        aspect_span= sent[aspect_index:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "    else:\n",
    "        aspect_span = sent[part_pos[2]+1:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "        \n",
    "    # find ground truth is more or less:\n",
    "    more=0\n",
    "    less=0\n",
    "    reverse= 1\n",
    "    MoreOrLess=0\n",
    "    for token in sent[part_pos[2]:]:\n",
    "        if token.text == \"more\":\n",
    "            more +=1\n",
    "            \n",
    "        elif token.text == \"less\":\n",
    "            less += 1\n",
    "            \n",
    "        elif token.text == \"not\" or token.text == \"no\":\n",
    "            reverse=-1\n",
    "        \n",
    "    if \"er \" in sent[part_pos[2]:].text:\n",
    "        more += 1\n",
    "    \n",
    "    MoreOrLess = reverse*(more-less)>0\n",
    "    return object1,object2,obj1_property.split(\",\")[0],obj2_property.strip(\",\").split(\",\")[0],aspect_text, MoreOrLess*\"more\"+(1-MoreOrLess)*\"less\"\n",
    "\n",
    "def elements_extraction_other(sent_text):\n",
    "    # new information extraction rule for other format\n",
    "    # add punction\n",
    "    sent_text=AddPeriod(sent_text)\n",
    "    \n",
    "    doc=nlp(sent_text)\n",
    "    sent=list(doc.sents)[-1]\n",
    "    \n",
    "    # find root token idx for each sentence part\n",
    "    part_pos=SentencePartDetection(sent)\n",
    "    \n",
    "    # without than pick the last token\n",
    "    for token in list(sent)[::-1]:\n",
    "        if token.pos_!=\"PUNCT\":\n",
    "            object_2=token\n",
    "            break\n",
    "    \n",
    "    # find object2 by than\n",
    "    for token in list(sent)[::-1]:\n",
    "        #print(token)    \n",
    "        if token.text == \"than\":\n",
    "            than_token=token\n",
    "            \n",
    "            than_child = list(than_token.children)\n",
    "            than_anc= list(than_token.ancestors)\n",
    "            \n",
    "            if than_child:\n",
    "                for token in than_child:\n",
    "                    if token.pos_!=\"PUNCT\":\n",
    "                        object_2 =token\n",
    "                        break\n",
    "                break\n",
    "            else:\n",
    "                for token in than_child:\n",
    "                    if token.pos_!=\"PUNCT\":\n",
    "                        object_2 =token\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    # find compare root \n",
    "    compare_root=doc[part_pos[-1]]\n",
    "    \n",
    "    # find the left token of compare root\n",
    "    leftEnd_compare_tokens=find_end_tree(compare_root,left_=True,right_=False, res=[])\n",
    "    \n",
    "    object_1=token_text(None)\n",
    "    for token in leftEnd_compare_tokens:\n",
    "        # limitation\n",
    "        #print(token)\n",
    "        if len(part_pos)>1 and token.i < part_pos[-2]:\n",
    "            continue\n",
    "        \n",
    "        if token.pos_ == \"PUNCT\":\n",
    "            continue\n",
    "            \n",
    "        # judege whether it is object1\n",
    "        #print(token)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            object_1 = token\n",
    "            break\n",
    "            \n",
    "        if token.dep_ in {\"nsubj\",\"acomp\"}:\n",
    "            object_1 = token\n",
    "            break\n",
    "            \n",
    "    if object_1.text == None:\n",
    "        for token in leftEnd_compare_tokens:\n",
    "            # limitation\n",
    "            #print(token)\n",
    "            if len(part_pos)>1 and token.i < part_pos[-2]:\n",
    "                continue\n",
    "\n",
    "            if token.pos_ == \"PUNCT\":\n",
    "                continue\n",
    "\n",
    "            # judege whether it is object1\n",
    "\n",
    "            if list(token.ancestors)[0].text == \"is\" and token.text!=\"so\":\n",
    "                object_1 = token\n",
    "                break\n",
    "    object_1,object_2=object_1.text, object_2.text\n",
    "    \n",
    "    # find the common property of object1 and object2\n",
    "    # replace the item name to make sentence more correct\n",
    "    if object_1:\n",
    "        sent_text=sent_text.replace(object_1,\"Jack\")\n",
    "    if object_2:\n",
    "        sent_text=sent_text.replace(object_2,\"Amy\")\n",
    "        \n",
    "    doc=nlp(sent_text)\n",
    "    sent=list(doc.sents)[0]\n",
    "    \n",
    "    # find root token idx for each sentence part\n",
    "    part_pos=SentencePartDetection(sent)\n",
    "    \n",
    "    # two possible property\n",
    "    property_root=doc[part_pos[0]]\n",
    "    \n",
    "    if property_root.pos_==\"VERB\":\n",
    "        # find a cluster of token than close to root\n",
    "        root_idx=property_root.i\n",
    "        mindis_token=[token_text(\"\"),float(\"inf\")]\n",
    "        for child in property_root.rights:\n",
    "            idx=child.idx\n",
    "            dif=idx-root_idx\n",
    "            if child.text in {\"Amy\",\"Jack\"}:\n",
    "                continue\n",
    "            if dif < mindis_token[1]:\n",
    "                mindis_token=[child,dif]\n",
    "        if mindis_token[0].text==\"\":\n",
    "            most_right=root_idx\n",
    "        else:\n",
    "            most_right=find_max_idx(mindis_token[0],res=[])\n",
    "        property_tokens=doc[root_idx:most_right+1].text.replace(\" Amy \",\" \").replace(\" Jack \",\" \")\n",
    "    else:\n",
    "        root_idx=property_root.i\n",
    "        mindis_token=[token_text(\"\"),float(\"inf\")]\n",
    "        for child in property_root.rights:\n",
    "            idx=child.idx\n",
    "            dif=idx-root_idx\n",
    "            if child.text in {\"Amy\",\"Jack\"}:\n",
    "                continue\n",
    "            if dif < mindis_token[1]:\n",
    "                mindis_token=[child,dif]\n",
    "        if mindis_token[0].text==\"\":\n",
    "            most_right=root_idx\n",
    "            most_left=root_idx\n",
    "        else:\n",
    "            most_right=find_max_idx(mindis_token[0],res=[])\n",
    "            most_left=find_min_idx(mindis_token[0],res=[])\n",
    "        property_tokens=doc[most_left:most_right+1].text.replace(\" Amy \",\" \").replace(\" Jack \",\" \")\n",
    "    property_tokens=property_tokens.replace(\" Amy's \",\" \").replace(\" Jack's \",\" \")\n",
    "    property_tokens=property_tokens.replace(\" Amy\",\"\").replace(\" Jack\",\"\")\n",
    "    property_tokens=property_tokens.split(\"than\")[0]\n",
    "\n",
    "    sent=list(doc.sents)[-1]\n",
    "    # find root token idx for each sentence part\n",
    "    part_pos=SentencePartDetection(sent)\n",
    "    \n",
    "    # find compare aspect\n",
    "    compare_root=doc[part_pos[-1]]\n",
    "    aspect_token=subitem_depCheck(compare_root.rights, require={\"acomp\",\"attr\"})\n",
    "    \n",
    "    if aspect_token.text:\n",
    "        aspect_index=aspect_token.i\n",
    "        for left in aspect_token.lefts:\n",
    "            rel=left.dep_\n",
    "            if rel not in [\"advmod\"] and left.i < aspect_index:\n",
    "                aspect_index= left.i\n",
    "\n",
    "        aspect_span= sent[aspect_index:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "    else:\n",
    "        aspect_span = sent[part_pos[-1]+1:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "        \n",
    "    # find ground truth is more or less:\n",
    "    more=0\n",
    "    less=0\n",
    "    reverse= 1\n",
    "    MoreOrLess=0\n",
    "    for token in sent[part_pos[-1]:]:\n",
    "        if token.text == \"more\":\n",
    "            more +=1\n",
    "            \n",
    "        elif token.text == \"less\":\n",
    "            less += 1\n",
    "            \n",
    "        elif token.text == \"not\" or token.text == \"no\":\n",
    "            reverse=-1\n",
    "        \n",
    "    if \"er \" in sent[part_pos[-1]:].text:\n",
    "        more += 1\n",
    "    \n",
    "    MoreOrLess = reverse*(more-less)>0\n",
    "    Amy_pos=sent_text.find(\"Amy\")\n",
    "    Jack_pos=sent_text.find(\"Jack\")\n",
    "    if Amy_pos<Jack_pos:\n",
    "        hyp_sent=f\"Amy is {property_tokens}\"\n",
    "    else:\n",
    "        hyp_sent=f\"Jack is {property_tokens}\"\n",
    "    return str(object_1),str(object_2),aspect_text,property_tokens,MoreOrLess*\"more\"+(1-MoreOrLess)*\"less\",hyp_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-growth",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mounted-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "# load bert model\n",
    "model = SentenceTransformer('nli-bert-large')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fourth-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6003237/6003237 [23:40<00:00, 4225.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00286278,  0.01514893,  0.02432977, ..., -0.00663066,\n",
       "       -0.06165611, -0.05097185], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cskg_embed file\n",
    "isfile_=os.path.isfile('cskg_model_embed.pickle')\n",
    "if isfile_:\n",
    "    with open('cskg_model_embed.pickle', 'rb') as handle:\n",
    "        cskg_embed = pickle.load(handle)\n",
    "else:\n",
    "    # divide task into 1000chunks and run loop\n",
    "    chunk_num = 1000\n",
    "    chunks_size = math.ceil(len(cskg_sents)/1000)\n",
    "    cskg_embed=np.empty((0,1024), dtype=np.float32)\n",
    "    count = 0\n",
    "\n",
    "    for num in tqdm(range(1,chunk_num+1)):\n",
    "        end = num*chunks_size\n",
    "        start = chunks_size*(num-1)\n",
    "        temp_embed=model.encode(cskg_sents[start:end])\n",
    "        cskg_embed=np.append(cskg_embed, temp_embed, axis=0)\n",
    "        count += 1 \n",
    "        \n",
    "    # store file into desktop\n",
    "    with open('cskg_model_embed.pickle', 'wb') as handle:\n",
    "        pickle.dump(cskg_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "cskg_embed=np.array([S/(math.sqrt(sum(S**2))) for S in tqdm(cskg_embed)])\n",
    "cskg_embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "innovative-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_1k_lines, \"r\") as f:\n",
    "    sample_lines=[]\n",
    "    for line in f:\n",
    "        sample_lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "studied-helicopter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'after', 'before', 'better', 'easier', 'harder', 'less', 'more', 'worse'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect a list of all possible labels\n",
    "with open(ground_truth_file, \"r\") as f:\n",
    "    ground_truth=set()\n",
    "    for line in f:\n",
    "        ground_truth.add(line.strip())\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "incorrect-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_table=[[\"before\",\"after\"],[\"easier\",\"harder\"],[\"more\",\"less\"],[\"better\",\"worse\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "environmental-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out special format for information extraction\n",
    "# example\n",
    "regex_format=\".*is.*,.*is.*, so.*is.*than,*\"\n",
    "\n",
    "hyp_sents=[]\n",
    "for sent in sample_lines:\n",
    "    status=re.search(regex_format,sent)\n",
    "    \n",
    "    if not status:\n",
    "        obj1,obj2,compare_aspect, property_,truth,property_descrip=elements_extraction_other(sent)\n",
    "        # hyp sentences\n",
    "        hyp_sents.append([f\"{property_.strip()} causes {compare_aspect}\"])\n",
    "    else:\n",
    "        obj1,obj2, obj1_property, obj2_property, aspect, truth=elements_extraction_isa(sent)\n",
    "        C1= obj1_property +\" is \" + aspect\n",
    "        C2= obj2_property +\" is \" + aspect\n",
    "        hyp_sents.append([C1,C2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "infectious-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 4770.64it/s]\n"
     ]
    }
   ],
   "source": [
    "map_hyp_sents=[]\n",
    "\n",
    "for line in hyp_sents:\n",
    "    for sent in line:\n",
    "        map_hyp_sents.append(sent)\n",
    "        \n",
    "hyp_sents_embed=model.encode(map_hyp_sents)\n",
    "hyp_sents_embed=np.array([S/(math.sqrt(sum(S**2))) for S in tqdm(hyp_sents_embed)])\n",
    "\n",
    "# use faiss to find neareast\n",
    "d= cskg_embed.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(cskg_embed)\n",
    "\n",
    "min_sim=1\n",
    "max_sim=0\n",
    "# find the closest edges\n",
    "k = 1\n",
    "D, I = index.search(hyp_sents_embed, k)\n",
    "for idx,embed in zip(I,hyp_sents_embed):\n",
    "    idx=idx[0]\n",
    "    embed1=cskg_embed[idx]\n",
    "    embed2=embed\n",
    "    similar=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "    min_sim=min(min_sim,similar)\n",
    "    max_sim=max(max_sim,similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "suspected-intermediate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8234521150588989"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold=(min_sim+max_sim)/2\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "verbal-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ajol', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('smaller', 'JJR')]\n",
      "[('than', 'IN')]\n",
      "[('wzuexmkld', 'NN')]\n",
      "[(',', ',')]\n",
      "[('so', 'RB')]\n",
      "[('wzuexmkld', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('harder', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('put', 'NN')]\n",
      "[('into', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('box', 'NN')]\n",
      "[('than', 'IN')]\n",
      "[('ajol', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('ajol is smaller than wzuexmkld, so wzuexmkld is harder to put into a box than ajol')\n",
    "for token in doc:\n",
    "    print(pos_tag([token.text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "unknown-lover",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/hanzhizh/miniconda3/envs/isi/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n"
     ]
    }
   ],
   "source": [
    "prediction_truth=[]\n",
    "sent_idx=0\n",
    "\n",
    "for i in range(len(sample_lines)):\n",
    "    sent=sample_lines[i]\n",
    "    status=re.search(regex_format,sent)\n",
    "\n",
    "    if not status:\n",
    "        obj1,obj2,compare_aspect, property_,truth,property_descrip=elements_extraction_other(sent)\n",
    "        idx=I[sent_idx][0]\n",
    "        embed1=cskg_embed[idx]\n",
    "        embed2=hyp_sents_embed[sent_idx]\n",
    "        similar=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "        property_item=property_descrip.split(\" \")[0]\n",
    "\n",
    "        if property_item== \"Jack\":\n",
    "            for more,less in ground_truth_table:\n",
    "                if more in sent or less in sent:\n",
    "                    candiates=[less,more]\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for more,less in ground_truth_table:\n",
    "                if more in sent or less in sent:\n",
    "                    candiates=[more,less]\n",
    "                    break\n",
    "\n",
    "        predict_result=candiates[similar>threshold]\n",
    "        \n",
    "        prediction_truth.append(predict_result)\n",
    "        sent_idx+=1\n",
    "    else:\n",
    "        obj1,obj2, obj1_property, obj2_property, aspect, truth=elements_extraction_isa(sent)\n",
    "        idx1=I[sent_idx][0]\n",
    "        idx2=I[sent_idx+1][0]\n",
    "        edge1_embed=cskg_embed[idx1]\n",
    "        edge2_embed=cskg_embed[idx2]\n",
    "        sent_embed1=hyp_sents_embed[sent_idx]\n",
    "        sent_embed2=hyp_sents_embed[sent_idx+1]\n",
    "\n",
    "        # claculate similarity\n",
    "        similar1=dot(edge1_embed, sent_embed1)/(norm(edge1_embed)*norm(sent_embed1))\n",
    "        similar2=dot(edge2_embed, sent_embed2)/(norm(edge2_embed)*norm(sent_embed2))\n",
    "        \n",
    "        for more,less in ground_truth_table:\n",
    "            if more in sent or less in sent:\n",
    "                candiates=[less,more]\n",
    "                break\n",
    "        if similar1 > similar2:\n",
    "            predict_result=more\n",
    "        else:\n",
    "            predict_result=less\n",
    "\n",
    "        prediction_truth.append(predict_result)\n",
    "        sent_idx+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "based-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(temp_result, \"w\") as f:\n",
    "    for predict in prediction_truth:\n",
    "        f.write(predict+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "located-scratch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49625"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find accuracy\n",
    "with open(ground_truth_file, \"r\") as f:\n",
    "    truth=[]\n",
    "    for line in f:\n",
    "        truth.append(line.strip())\n",
    "\n",
    "accuracy=0\n",
    "for ground, predict in zip(truth,prediction_truth):\n",
    "    if ground ==predict:\n",
    "        accuracy+=1\n",
    "    \n",
    "accuracy=accuracy/len(truth)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "athletic-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"92d67b41446e475da19376c5b67c8a80-0\" class=\"displacy\" width=\"3025\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">hcwoctv</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">glass</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">mhzg</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">stone,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">so</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">hcwoctv</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">not</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">better</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">blocking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">light</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">mhzg</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M555.0,441.5 L563.0,429.5 547.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-3\" stroke-width=\"2px\" d=\"M420,439.5 C420,264.5 735.0,264.5 735.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,441.5 L743.0,429.5 727.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-4\" stroke-width=\"2px\" d=\"M245,439.5 C245,177.0 915.0,177.0 915.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,441.5 L923.0,429.5 907.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1080.0,441.5 L1088.0,429.5 1072.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-6\" stroke-width=\"2px\" d=\"M245,439.5 C245,89.5 1270.0,89.5 1270.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,441.5 L1278.0,429.5 1262.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-8\" stroke-width=\"2px\" d=\"M245,439.5 C245,2.0 1625.0,2.0 1625.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,441.5 L1633.0,429.5 1617.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-10\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,264.5 1960.0,264.5 1960.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1960.0,441.5 L1968.0,429.5 1952.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-11\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2130.0,441.5 L2138.0,429.5 2122.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-12\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2305.0,441.5 L2313.0,429.5 2297.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-13\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,352.0 2480.0,352.0 2480.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2480.0,441.5 L2488.0,429.5 2472.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-14\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,352.0 2655.0,352.0 2655.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2655.0,441.5 L2663.0,429.5 2647.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-92d67b41446e475da19376c5b67c8a80-0-15\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,352.0 2830.0,352.0 2830.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-92d67b41446e475da19376c5b67c8a80-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2830.0,441.5 L2838.0,429.5 2822.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc=nlp(\"hcwoctv is glass and mhzg is stone, so hcwoctv is not better at blocking light than mhzg\")\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-document",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
