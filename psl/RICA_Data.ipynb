{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre for RICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,os, faiss, spacy, math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:\n",
    "cskg_connected_file=\"../kg-bert/data/cskg/cskg_connected.tsv\"\n",
    "RICA_file=\"./RICA/RICA_material_KnowledgeTable.csv\"\n",
    "cskg_embed_file=\"cskg_model_embed.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design dependency rule\n",
    "class token_text():\n",
    "    def __init__(self, text):\n",
    "        self.text=text\n",
    "\n",
    "def subitem_depCheck(subs, require={}):\n",
    "    output_token=token_text(\"\")\n",
    "    for sub in subs:\n",
    "        rel=sub.dep_\n",
    "        \n",
    "        if rel in require:\n",
    "            output_token=sub\n",
    "            break\n",
    "            \n",
    "    return output_token\n",
    "\n",
    "def walk_tree(node, depth, depths={}):\n",
    "    depths[node] = depth\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return [walk_tree(child, depth + 1,depths=depths) for child in node.children]\n",
    "\n",
    "def find_end_tree(root,res=[], left_=False, right_=False):\n",
    "    # find the end token of dependency True.\n",
    "    # left_ means find left hand side children\n",
    "    # right_ means find right hand side children\n",
    "    if root.n_lefts*left_ + root.n_rights*right_ > 0:\n",
    "        if left_ and right_:\n",
    "            for child in root.children:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif left_:\n",
    "            for child in root.lefts:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif right_:\n",
    "            for child in root.rights:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "    else:\n",
    "        res.append(root)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def elements_extraction(sent):\n",
    "    # new information extraction rule\n",
    "    if sent[-1]!=\".\":\n",
    "        sent+=\".\"\n",
    "    \n",
    "    doc=nlp(sent)\n",
    "    sent=list(doc.sents)[0]\n",
    "    part_roots=set()\n",
    "    \n",
    "    spanLength_list=[]\n",
    "    for token in sent:\n",
    "        token_index=token.i\n",
    "        for child in token.children:\n",
    "            child_index = child.i\n",
    "            \n",
    "            if child.dep_==\"punct\":\n",
    "                continue\n",
    "            spanLength_list.append((abs(token_index-child_index),(token_index, child_index)))\n",
    "            \n",
    "    spanLength_list.sort(reverse=True)\n",
    "    part_pos=set()\n",
    "    \n",
    "    depths={}\n",
    "    [walk_tree(sent.root, 0, depths=depths) for sent in doc.sents]\n",
    "    #print(depths)\n",
    "    for span_length, tokens in spanLength_list:\n",
    "        token1_index, token2_index=tokens\n",
    "        token1=sent[token1_index]\n",
    "        token2=sent[token2_index]\n",
    "        #print(token1, token2, token2.dep_, part_pos)\n",
    "        if len(list(token2.children))<1 or depths[token2]>2:\n",
    "            continue\n",
    "        \n",
    "        if token2.dep_ in {\"conj\",\"ccomp\",\"advcl\",\"dep\"}:\n",
    "            part_pos.add(token1_index)\n",
    "            part_pos.add(token2_index)\n",
    "        \n",
    "        if len(part_pos)>=3:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    part_pos=sorted(list(part_pos))\n",
    "    \n",
    "    # final compaare part\n",
    "    # print(part_pos)\n",
    "    compare_root=sent[part_pos[2]]\n",
    "    \n",
    "    # find reasoning part\n",
    "    reasoning_first_root=sent[part_pos[0]]\n",
    "    reasoning_second_root=sent[part_pos[1]]\n",
    "    \n",
    "    # find object1 and object2\n",
    "    leftEnd_compare_tokens=find_end_tree(compare_root,left_=True,right_=False, res=[])\n",
    "    leftEnd_first_tokens=find_end_tree(reasoning_first_root,left_=True,right_=False, res=[])\n",
    "    leftEnd_second_tokens=find_end_tree(reasoning_second_root,left_=True,right_=False, res=[])\n",
    "    \n",
    "    leftEnd_first_tokens_text=[_.text for _ in leftEnd_first_tokens]\n",
    "    leftEnd_second_tokens_text=[_.text for _ in leftEnd_second_tokens]\n",
    "    \n",
    "    object1, object2=token_text(\"\"),token_text(\"\")\n",
    "    \n",
    "    if len(leftEnd_first_tokens)==1:\n",
    "        object2=leftEnd_first_tokens[0]\n",
    "        \n",
    "    else:\n",
    "        for token in leftEnd_first_tokens:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                object2 = token\n",
    "                \n",
    "            if token.dep_ in {\"nsubj\",\"acomp\"}:\n",
    "                object2=token\n",
    "                \n",
    "            if list(token.ancestors)[0].text == \"is\":\n",
    "                object2 = token\n",
    "            \n",
    "    if len(leftEnd_second_tokens)==1:\n",
    "        object1=leftEnd_second_tokens[0]\n",
    "        \n",
    "    else:\n",
    "        for token in leftEnd_second_tokens:\n",
    "            if token.i < part_pos[0]:\n",
    "                continue\n",
    "                \n",
    "            if token.pos_ == \"NOUN\":\n",
    "                object1 = token\n",
    "            \n",
    "            if token.dep_ in {\"nsubj\",\"acomp\"}:\n",
    "                object1=token\n",
    "                \n",
    "            if list(token.ancestors)[0].text == \"is\":\n",
    "                object1 = token\n",
    "            \n",
    "    # use than find one object candidates\n",
    "    for token in sent:\n",
    "        if token.text == \"than\":\n",
    "            than_token=token\n",
    "            \n",
    "            than_child = list(than_token.children)\n",
    "            than_anc= list(than_token.ancestors)\n",
    "            \n",
    "            if than_child:\n",
    "                object_temp =than_child[0]\n",
    "                \n",
    "            else:\n",
    "                object_temp=than_anc[0]\n",
    "                \n",
    "    objects_text=[object1.text, object2.text]\n",
    "    \n",
    "    # replace empty object result with object candidates found by than\n",
    "    if object_temp.text in objects_text:\n",
    "        pass\n",
    "    else:\n",
    "        if objects_text[0]==\"\":\n",
    "            object1=object_temp\n",
    "        else:\n",
    "            object2=object_temp\n",
    "            \n",
    "    # find the property of each objects\n",
    "    rightEnd_first_tokens=find_end_tree(reasoning_first_root,left_=False,right_=True, res=[])\n",
    "    rightEnd_second_tokens=find_end_tree(reasoning_second_root,left_=False,right_=True, res=[])\n",
    "    \n",
    "    max_dep_obj2=0\n",
    "    \n",
    "    for token in rightEnd_first_tokens:\n",
    "        idx = token.i\n",
    "        if token.i > part_pos[1]:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            max_dep_obj2=max(max_dep_obj2, idx)\n",
    "            \n",
    "    max_dep_obj1=0\n",
    "    for token in rightEnd_second_tokens:\n",
    "        idx = token.i\n",
    "        if token.i > part_pos[2]:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            max_dep_obj1=max(max_dep_obj1, idx)\n",
    "            \n",
    "    obj2_property=sent[part_pos[0]+1:max_dep_obj2+1].text\n",
    "    obj1_property=sent[part_pos[1]+1:max_dep_obj1+1].text\n",
    "    \n",
    "    # find compafre aspect\n",
    "    aspect_token=subitem_depCheck(compare_root.rights, require={\"acomp\",\"attr\"})\n",
    "    \n",
    "    if aspect_token.text:\n",
    "        aspect_index=aspect_token.i\n",
    "        for left in aspect_token.lefts:\n",
    "            rel=left.dep_\n",
    "            if rel not in [\"advmod\",\"amod\"] and left.i < aspect_index:\n",
    "                aspect_index= left.i\n",
    "\n",
    "        aspect_span= sent[aspect_index:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "    else:\n",
    "        aspect_span = sent[part_pos[2]+1:]\n",
    "        aspect_text=aspect_span.text.split(\" than \")[0]\n",
    "        \n",
    "    # find ground truth is more or less:\n",
    "    more=0\n",
    "    less=0\n",
    "    reverse= 1\n",
    "    MoreOrLess=0\n",
    "    for token in sent[part_pos[2]:]:\n",
    "        if token.text == \"more\":\n",
    "            more +=1\n",
    "            \n",
    "        elif token.text == \"less\":\n",
    "            less += 1\n",
    "            \n",
    "        elif token.text == \"not\" or token.text == \"no\":\n",
    "            reverse=-1\n",
    "        \n",
    "    if \"er \" in sent[part_pos[2]:].text:\n",
    "        more += 1\n",
    "    \n",
    "    MoreOrLess = reverse*(more-less)>0\n",
    "    return object1,object2,obj1_property.split(\",\")[0],obj2_property.strip(\",\").split(\",\")[0],aspect_text, MoreOrLess\n",
    "\n",
    "def embed_normalize(model_embeddings):\n",
    "    # normalize embedding\n",
    "    model_embeddings=np.array([S/(math.sqrt(sum(S**2))) for S in tqdm(model_embeddings)])\n",
    "    return model_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('nli-bert-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cskg lines\n",
    "cskg_lines=[]\n",
    "with open(cskg_connected_file,\"r\") as f:\n",
    "    head = f.readline().strip().split(\"\\t\")\n",
    "    \n",
    "    for item in f:\n",
    "        line = item.strip().split(\"\\t\")\n",
    "        cskg_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation distinct label set\n",
    "rels=set()\n",
    "for line in cskg_lines:\n",
    "    rel_id=line[2].split(\"/\")[-1].split(\":\")[-1]\n",
    "    rels.add(rel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cskg embedding\n",
    "# cskg embedding is generated by sentence transformer bert model on hypo sentence \"subject label relation lebale object label\"\n",
    "isfile_=os.path.isfile(cskg_embed_file)\n",
    "if isfile_:\n",
    "    with open(cskg_embed_file, 'rb') as handle:\n",
    "        cskg_embed = pickle.load(handle)\n",
    "    \n",
    "    cskg_lines=[]\n",
    "    \n",
    "    with open(cskg_connected_file,\"r\") as f:\n",
    "        head = f.readline().strip().split(\"\\t\")\n",
    "        \n",
    "        for item in f:\n",
    "            line = item.strip().split(\"\\t\")\n",
    "            relation_id=line[2]\n",
    "\n",
    "            cskg_lines.append(line)\n",
    "else:\n",
    "    model = SentenceTransformer('nli-bert-large')\n",
    "    cskg_lines=[]\n",
    "    \n",
    "    with open(cskg_connected_file,\"r\") as f:\n",
    "        head = f.readline().strip().split(\"\\t\")\n",
    "        \n",
    "        for item in f:\n",
    "            line = item.strip().split(\"\\t\")\n",
    "            relation_id=line[2]\n",
    "\n",
    "            if relation_id == \"/r/HasProperty\":\n",
    "                cskg_lines.append(line)\n",
    "            \n",
    "    # build sentence\n",
    "    sents=[]\n",
    "    for line in lines_HasProperty:\n",
    "        sent=f\"{line[4]} {line[6]} {line[5]}\"\n",
    "        sents.append(sent)\n",
    "        \n",
    "    cskg_embed=model.encode(sents)\n",
    "    \n",
    "    # store file into desk\n",
    "    with open(cskg_embed_file, 'wb') as handle:\n",
    "        pickle.dump(cskg_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6003237/6003237 [21:49<00:00, 4584.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalize cskg embedding\n",
    "cskg_embed=embed_normalize(cskg_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rica data\n",
    "with open(RICA_file,\"r\") as f:\n",
    "    head = f.readline().strip().split(\",\")\n",
    "    RICA_lines=[]\n",
    "    \n",
    "    for item in f:\n",
    "        line = item.strip().split(\",\")\n",
    "        RICA_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n",
      "gas\n",
      "liquid\n",
      "Item_gas is gas, Item_glass is glass, so Item_gas is more contain liquid than Item_glass\n",
      "\n",
      "glass\n",
      "gold\n",
      "reflective\n",
      "Item_gold is gold, Item_glass is glass, so Item_gold is more very reflective than Item_glass\n",
      "\n",
      "glass\n",
      "plastic\n",
      "reflective\n",
      "Item_plastic is plastic, Item_glass is glass, so Item_plastic is more very reflective than Item_glass\n",
      "\n",
      "glass\n",
      "sand\n",
      "reflective\n",
      "Item_sand is sand, Item_glass is glass, so Item_sand is more very reflective than Item_glass\n",
      "\n",
      "glass\n",
      "metal\n",
      "reflective\n",
      "Item_metal is metal, Item_glass is glass, so Item_metal is more very reflective than Item_glass\n",
      "\n",
      "glass\n",
      "silver\n",
      "reflective\n",
      "Item_silver is silver, Item_glass is glass, so Item_silver is more very reflective than Item_glass\n",
      "\n",
      "paper\n",
      "blood\n",
      "lightweight\n",
      "Item_cow_blood is cow blood, Item_paper is paper, so Item_cow_blood is more lightweight than Item_paper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check accuracy\n",
    "count =0\n",
    "C1s=[]\n",
    "C2s=[]\n",
    "extracted_info=[]\n",
    "for line in RICA_lines:\n",
    "    temp1=line[1].replace(\" \",\"_\")\n",
    "    temp2=line[0].replace(\" \",\"_\")\n",
    "    item1=f\"Item_{temp1}\"\n",
    "    item2=f\"Item_{temp2}\"\n",
    "    sent= f\"{item1} is {line[1]}, {item2} is {line[0]}, so {item1} is {line[2].lower()} {line[3]} than {item2}\"\n",
    "    obj1,obj2, obj1_property, obj2_property, aspect, truth=elements_extraction(sent)\n",
    "    \n",
    "    extracted_info.append([obj1.text,obj2.text,obj1_property,obj2_property,aspect])\n",
    "    if obj1_property== line[0] and obj2_property== line[1] and aspect==line[3]:\n",
    "        count +=1\n",
    "    else:\n",
    "        print(obj1_property)\n",
    "        print(obj2_property)\n",
    "        print(aspect)\n",
    "        print(sent)\n",
    "        print()\n",
    "    # build two sentence:\n",
    "    C1_sentence=f\"{obj1_property} is {aspect}\"\n",
    "    C2_sentence=f\"{obj2_property} is {aspect}\"\n",
    "    \n",
    "    C1s.append(C1_sentence)\n",
    "    C2s.append(C2_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 5071.08it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 5060.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# transfer sents to embedding\n",
    "\n",
    "C1_sents_embedding=model.encode(C1s,device=3)\n",
    "C2_sents_embedding=model.encode(C2s,device=3)\n",
    "\n",
    "C1_sents_embedding=embed_normalize(C1_sents_embedding)\n",
    "C2_sents_embedding=embed_normalize(C2_sents_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use faiss to find neareast\n",
    "d= cskg_embed.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(cskg_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the closest edges\n",
    "\n",
    "k = 1\n",
    "D_C1, I_C1 = index.search(C1_sents_embedding, k)\n",
    "\n",
    "k = 1\n",
    "D_C2, I_C2 = index.search(C2_sents_embedding, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx1_, idx2_,sent_embed1, sent_embed2,rica_line in zip(I_C1,I_C2,C1_sents_embedding,C2_sents_embedding,extracted_info):\n",
    "    idx1=idx1_[0]\n",
    "    idx2=idx2_[0]\n",
    "    line1=cskg_lines[idx1]\n",
    "    line2=cskg_lines[idx2]\n",
    "    edge1_embed=cskg_embed[idx1]\n",
    "    edge2_embed=cskg_embed[idx2]\n",
    "    \n",
    "    rel1=line1[2]\n",
    "    rel2=line2[2]\n",
    "    \n",
    "    similar1=dot(edge1_embed, sent_embed1)/(norm(edge1_embed)*norm(sent_embed1))\n",
    "    similar2=dot(edge2_embed, sent_embed2)/(norm(edge2_embed)*norm(sent_embed2))\n",
    "    # item1 line\n",
    "    rica_line.append([rel1,similar1])\n",
    "    # item2 line\n",
    "    rica_line.append([rel2,similar2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sim=0\n",
    "min_sim=1\n",
    "for line in extracted_info:\n",
    "    _, sim1=line[-2]\n",
    "    _, sim2=line[-1]\n",
    "    \n",
    "    max_sim=max(max_sim,sim1)\n",
    "    max_sim=max(max_sim,sim2)\n",
    "    \n",
    "    min_sim=min(min_sim,sim1)\n",
    "    min_sim=min(min_sim,sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7802252, 0.98956645)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sim, max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_info.pickle', 'wb') as handle:\n",
    "    pickle.dump(extracted_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescall sim\n",
    "\n",
    "scale=1/(max_sim-min_sim)\n",
    "for i in range(len(extracted_info)):\n",
    "    line=extracted_info[i]\n",
    "    \n",
    "    extracted_info[i][-1][-1]=(line[-1][-1]-min_sim)*scale\n",
    "    extracted_info[i][-2][-1]=(line[-2][-1]-min_sim)*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data for psl:\n",
    "psl_data=dict()\n",
    "material_property=dict()\n",
    "for rel in rels:\n",
    "    psl_data[rel]=dict()\n",
    "    material_property[rel]=defaultdict(dict)\n",
    "\n",
    "compare_aspects=set()\n",
    "items_=set()\n",
    "for i in range(len(extracted_info)):\n",
    "    line=extracted_info[i]\n",
    "    item1_name,item2_name,item1_property,item2_property,compare_aspect,item1_info,item2_info=line\n",
    "    \n",
    "    item1_rel=item1_info[0].split(\"/\")[-1].split(\":\")[-1]\n",
    "    item2_rel=item2_info[0].split(\"/\")[-1].split(\":\")[-1]\n",
    "    sim1=item1_info[-1]\n",
    "    sim2=item2_info[-1]\n",
    "    compare_aspects.add(compare_aspect)\n",
    "    items_.add(item1_name)\n",
    "    items_.add(item2_name)\n",
    "    \n",
    "    psl_data[item1_rel][item1_name]=item1_property\n",
    "    psl_data[item2_rel][item2_name]=item2_property\n",
    "    material_property[item1_rel][item1_property][compare_aspect]=sim1\n",
    "    material_property[item2_rel][item2_property][compare_aspect]=sim2\n",
    "    \n",
    "# write item and material relation obs file\n",
    "with open(f\"./RICA/material_knowledge/ItemMaterial_obs.txt\",\"w\") as f:\n",
    "    item_material=set()\n",
    "    for rel in psl_data:\n",
    "        psl_rel_data=psl_data[rel]\n",
    "        \n",
    "        for item, material in psl_rel_data.items():\n",
    "            item_material.add((item,material))\n",
    "            \n",
    "    f.write(\"\\n\".join([\"\\t\".join(_) for _ in item_material]))\n",
    "            \n",
    "# write material and compare_aspect 1 or 0:\n",
    "for rel in material_property:\n",
    "    if not material_property[rel]:\n",
    "        continue\n",
    "    with open(f\"./RICA/material_knowledge/{rel}_obs.txt\",\"w\") as f:\n",
    "        material_data=material_property[rel]\n",
    "        for material in material_data:\n",
    "            property_dict=material_data[material]\n",
    "            for aspect, sim in property_dict.items():\n",
    "                f.write(f\"{material}\\t{aspect}\\t{sim}\\n\")\n",
    "                \n",
    "\n",
    "# write predict result:\n",
    "with open(f\"./RICA/material_knowledge/more_targets.txt\",\"w\") as f1, open(f\"./RICA/material_knowledge/more_truth.txt\",\"w\") as f2:\n",
    "    for item1 in items_:\n",
    "        for item2 in items_:\n",
    "            if item1==item2:\n",
    "                continue\n",
    "            for aspect in compare_aspects:\n",
    "                f1.write(f\"{item1}\\t{item2}\\t{aspect}\\t0\\n\")\n",
    "                f2.write(f\"{item1}\\t{item2}\\t{aspect}\\t1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write PSL rules\n",
    "# \"10: Material(I1,M1) & HasProperty(M1, T) & Material(I2,M2) & !HasProperty(M2, T) & I1 != I2 -> More(I1, I2, T) ^2\"\n",
    "# \"10: More(I1, I2, T) & I1 != I2 -> !More(I2, I1, T) ^2\"\n",
    "\n",
    "Rules=[]\n",
    "\n",
    "for rel in rels:\n",
    "    if not material_property[rel]:\n",
    "        continue\n",
    "    rule1=f\"10: Material(I1,M1) & {rel}(M1, T) & Material(I2,M2) & !{rel}(M2, T) & I1 != I2 -> More(I1, I2, T) ^2\"\n",
    "    Rules.append(rule1)\n",
    "Rules.append(\"1000: More(I1, I2, T) & I1 != I2 -> !More(I2, I1, T) ^2\")\n",
    "Rules.append(\"1000: More(I1, I2, T) + More(I2, I1, T) = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write psl rules into desk\n",
    "with open(\"rules.txt\", \"w\") as f:\n",
    "    for rule in Rules:\n",
    "        f.write(rule)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data into desk\n",
    "with open('rel.pickle', 'wb') as handle:\n",
    "    pickle.dump([rel for rel in rels if material_property[rel]], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('rules.pickle', 'wb') as handle:\n",
    "    pickle.dump(Rules, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prepareation steps are finished  \n",
    "Run MaterialKnowledge_1k.py to infere prediction  \n",
    "bash: python MaterialKnowledge_1k.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data preparation is finished\n",
    "# the file PSL python file should be runed\n",
    "# After running psl check the psl inference result \n",
    "\n",
    "with open(\"inferred-predicates/MORE.txt\", \"r\") as f:\n",
    "    psl_result=[]\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        line[-1]=round(eval(line[-1]))\n",
    "        psl_result.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buid dict of infere result\n",
    "df=pd.DataFrame(psl_result, columns=['item1',\"item2\",\"aspect\",\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item1</th>\n",
       "      <th>item2</th>\n",
       "      <th>aspect</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item_silver</td>\n",
       "      <td>Item_plastic_but_plastic_with_rubber_band</td>\n",
       "      <td>crumple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item_steel</td>\n",
       "      <td>Item_milk</td>\n",
       "      <td>liquid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item_ferment_milk</td>\n",
       "      <td>Item_metal_and_not_plastic</td>\n",
       "      <td>durable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item_silicon</td>\n",
       "      <td>Item_alcohol</td>\n",
       "      <td>opaque</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item_carbon</td>\n",
       "      <td>Item_rum</td>\n",
       "      <td>contain liquid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26671</th>\n",
       "      <td>Item_wood</td>\n",
       "      <td>Item_water</td>\n",
       "      <td>crumple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26672</th>\n",
       "      <td>Item_gas</td>\n",
       "      <td>Item_ceramic</td>\n",
       "      <td>fragile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26673</th>\n",
       "      <td>Item_crystal</td>\n",
       "      <td>Item_ferment_milk</td>\n",
       "      <td>reflective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26674</th>\n",
       "      <td>Item_sugar</td>\n",
       "      <td>Item_cow_blood</td>\n",
       "      <td>flat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26675</th>\n",
       "      <td>Item_liquor</td>\n",
       "      <td>Item_carbon</td>\n",
       "      <td>liquid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26676 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   item1                                      item2  \\\n",
       "0            Item_silver  Item_plastic_but_plastic_with_rubber_band   \n",
       "1             Item_steel                                  Item_milk   \n",
       "2      Item_ferment_milk                 Item_metal_and_not_plastic   \n",
       "3           Item_silicon                               Item_alcohol   \n",
       "4            Item_carbon                                   Item_rum   \n",
       "...                  ...                                        ...   \n",
       "26671          Item_wood                                 Item_water   \n",
       "26672           Item_gas                               Item_ceramic   \n",
       "26673       Item_crystal                          Item_ferment_milk   \n",
       "26674         Item_sugar                             Item_cow_blood   \n",
       "26675        Item_liquor                                Item_carbon   \n",
       "\n",
       "               aspect  result  \n",
       "0             crumple       1  \n",
       "1              liquid       1  \n",
       "2             durable       1  \n",
       "3              opaque       1  \n",
       "4      contain liquid       1  \n",
       "...               ...     ...  \n",
       "26671         crumple       1  \n",
       "26672         fragile       0  \n",
       "26673      reflective       0  \n",
       "26674            flat       0  \n",
       "26675          liquid       0  \n",
       "\n",
       "[26676 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "fp = 0\n",
    "for line1, line2 in zip(extracted_info, RICA_lines):\n",
    "    item1, item2, property1, property2, aspect, _, _ = line1\n",
    "    ground = line2[2]\n",
    "    \n",
    "    predict=df.loc[(df['item1'] == item1) & (df['item2'] == item2)& (df['aspect'] == aspect)][\"result\"].values[0]\n",
    "    \n",
    "    if ground==\"More\":\n",
    "        ground=1\n",
    "    else:\n",
    "        ground=0\n",
    "        \n",
    "    if predict==ground:\n",
    "        if ground==1:\n",
    "            tp +=1\n",
    "            \n",
    "        elif ground==0:\n",
    "            tn +=1\n",
    "            \n",
    "    else:\n",
    "        if ground==1:\n",
    "            fp +=1\n",
    "        elif ground ==0:\n",
    "            fn+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive: 87\n",
      "true negative: 6\n",
      "false positive: 26\n",
      "false negative: 7\n"
     ]
    }
   ],
   "source": [
    "confusion matix\n",
    "print(\"true positive:\",tp)\n",
    "print(\"true negative:\",tn)\n",
    "print(\"false positive:\",fp)\n",
    "print(\"false negative:\",fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isi",
   "language": "python",
   "name": "isi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
