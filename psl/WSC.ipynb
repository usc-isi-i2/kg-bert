{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,bs4,lxml,spacy,os,pickle,math,faiss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DOWNLOAD_URL = \"https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml\"\n",
    "cskg_embeddings_file=\"./cskg_embedding/cskg_embeddings.txt\"\n",
    "cskg_connected_file=\"../kg-bert/data/cskg/cskg_connected.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model = SentenceTransformer('nli-bert-large')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download WSC Dataset\n",
    "\n",
    "The code is based on the tensorflow. \n",
    "Link: https://www.tensorflow.org/datasets/catalog/wsc273  \n",
    "Source code: https://github.com/tensorflow/datasets/blob/45ba108ef87162b17335245fc97cabc75efcfa54/tensorflow_datasets/text/wsc273/wsc273.py#L115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.strip()\n",
    "    # Correct a misspell.\n",
    "    text = text.replace(\"recieved\", \"received\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "def normalize_cap(option, pron):\n",
    "    \"\"\"Normalize the capitalization of the option according to the pronoun.\"\"\"\n",
    "    cap_tuples = [\n",
    "      (\"The\", \"the\"), (\"His\", \"his\"), (\"My\", \"my\"),\n",
    "      (\"Her\", \"her\"), (\"Their\", \"their\"), (\"An\", \"an\"), (\"A\", \"a\")]\n",
    "    uncap_dict = dict(cap_tuples)\n",
    "    cap_dict = dict([(t[1], t[0]) for t in cap_tuples])\n",
    "    words = option.split(\" \")\n",
    "    first_word = words[0]\n",
    "    if pron[0].islower():\n",
    "        first_word = uncap_dict.get(first_word, first_word)\n",
    "    else:\n",
    "        first_word = cap_dict.get(first_word, first_word)\n",
    "    words[0] = first_word\n",
    "    option = \" \".join(words)\n",
    "    return option\n",
    "\n",
    "def parse_wsc273_xml(xml_data):\n",
    "    \"\"\"Parse the XML file containing WSC273 examples.\"\"\"\n",
    "    soup = bs4.BeautifulSoup(xml_data, \"lxml\")\n",
    "    schemas = soup.find_all(\"schema\")\n",
    "    # Only the first 273 examples are included in WSC273.\n",
    "    for i, schema in enumerate(schemas[:273]):\n",
    "        txt1 = schema.find_all(\"txt1\")[0].get_text()\n",
    "        txt1 = normalize_text(txt1)\n",
    "        txt2 = schema.find_all(\"txt2\")[0].get_text()\n",
    "        txt2 = normalize_text(txt2)\n",
    "        pron = schema.find_all(\"pron\")[0].get_text()\n",
    "        pron = normalize_text(pron)\n",
    "        answers = [ans.get_text().strip() for ans in schema.find_all(\"answer\")]\n",
    "        normalized_answers = [normalize_cap(ans, pron) for ans in answers]\n",
    "        assert len(answers) == 2\n",
    "        choice = schema.find_all(\"correctanswer\")[0].get_text().strip()\n",
    "        label = {\"A\": 0, \"B\": 1}[choice[0]]\n",
    "        if len(txt2) == 1:\n",
    "            # If there is only one punctuation left after the pronoun,\n",
    "            # then no space should be inserted.\n",
    "            text = f\"{txt1} {pron}{txt2}\"\n",
    "        else:\n",
    "            text = f\"{txt1} {pron} {txt2}\"\n",
    "        pronoun_text = pron\n",
    "        pronoun_start = len(txt1) + 1\n",
    "        pronoun_end = len(txt1) + len(pron) + 1\n",
    "        example = dict(\n",
    "            text=text,\n",
    "            pronoun_text=pronoun_text,\n",
    "            pronoun_start=pronoun_start,\n",
    "            pronoun_end=pronoun_end,\n",
    "            option1=answers[0],\n",
    "            option2=answers[1],\n",
    "            option1_normalized=normalized_answers[0],\n",
    "            option2_normalized=normalized_answers[1],\n",
    "            label=label,\n",
    "            idx=i)\n",
    "        assert text[pronoun_start:pronoun_end] == pronoun_text\n",
    "        yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(_DOWNLOAD_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=parse_wsc273_xml(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=list(parse_wsc273_xml(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis WSC\n",
    "\n",
    "Extract property  \n",
    "Example:  \n",
    "'The city councilmen refused the demonstrators a permit because they feared violence.'  \n",
    "Extracted property: refused a permit, feared violence  \n",
    "Hyp sentence: feared violence casuses refused a permit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class token_text():\n",
    "    # build a class to generate empty token\n",
    "    def __init__(self, text):\n",
    "        self.text=text\n",
    "\n",
    "def find_end_tree(root,res=[], left_=False, right_=False):\n",
    "    # find the end token of dependency Tree.\n",
    "    # left_ means find left hand side children\n",
    "    # right_ means find right hand side children\n",
    "    if root.n_lefts*left_ + root.n_rights*right_ > 0:\n",
    "        if left_ and right_:\n",
    "            res.append(root)\n",
    "            for child in root.children:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif left_:\n",
    "            res.append(root)\n",
    "            for child in root.lefts:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "        elif right_:\n",
    "            res.append(root)\n",
    "            for child in root.rights:\n",
    "                find_end_tree(child, res=res, left_=left_,right_=right_)\n",
    "    else:\n",
    "        res.append(root)\n",
    "    \n",
    "    return res        \n",
    "        \n",
    "def AddPeriod(sent_text):\n",
    "    # add period to sentences\n",
    "    if sent_text[-1]!=\".\":\n",
    "        sent_text+=\".\"\n",
    "    return sent_text\n",
    "def TokensByText(sent, label,doc):\n",
    "    # find tokens by label name\n",
    "    blacklist = {\n",
    "      \"the\",\"his\",\"my\",\"her\", \"their\", \"an\", \"a\"}\n",
    "    \n",
    "    stack=[]\n",
    "    res_stacks=[]\n",
    "    for token in sent:\n",
    "        token_label=token.text\n",
    "        token_lemma=token.lemma_\n",
    "        if token_label in blacklist or token_lemma in blacklist:\n",
    "            continue\n",
    "            \n",
    "        if token_label in label or token_lemma in label:\n",
    "            if stack:\n",
    "                # check whether idx is continuous\n",
    "                last_token_idx=stack[-1].i\n",
    "                if token.i-last_token_idx==1:\n",
    "                    # continous\n",
    "                    stack.append(token)\n",
    "                else:\n",
    "                    stack=[]\n",
    "                    stack.append(token)\n",
    "            else:\n",
    "                stack.append(token)\n",
    "            \n",
    "            # check whether tokens is found.\n",
    "            tokens_label=doc[stack[0].i:stack[-1].i+1]\n",
    "            if tokens_label.text==label:\n",
    "                res_stacks.append(stack)\n",
    "    if not res_stacks:    \n",
    "        return stack\n",
    "    else:\n",
    "        for stack_ in res_stacks:\n",
    "            if SubjectIdentifier(stack_,sent):\n",
    "                return stack_\n",
    "            \n",
    "        return res_stacks[-1]\n",
    "\n",
    "def ObjModify(obj):\n",
    "    blacklist = {\n",
    "      \"the\",\"his\",\"my\",\"her\", \"their\", \"an\", \"a\"}\n",
    "    obj=obj.lower()\n",
    "    obj_list=obj.split(\" \")\n",
    "    tmp=[]\n",
    "    for token in obj_list:\n",
    "        if token in blacklist:\n",
    "            continue\n",
    "        else:\n",
    "            tmp.append(token)\n",
    "            \n",
    "    return \" \".join(tmp)\n",
    "\n",
    "def SubjectIdentifier(tokens,sent):\n",
    "    # identify tokens are subject or object\n",
    "    # use the last token of tokens\n",
    "    \n",
    "    SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "    token = tokens[-1]\n",
    "    \n",
    "    # token is root:\n",
    "    if token==sent.root:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    if token.dep_ in SUBJECTS:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def SubjectFindProp(obj_tokens,other_tokens,doc):\n",
    "    # find property of subject\n",
    "    obj_last_token=obj_tokens[-1]\n",
    "    obj_text=\" \".join([_.text for _ in obj_tokens])\n",
    "    other_text=\" \".join([_.text for _ in other_tokens])\n",
    "    anc=list(obj_last_token.ancestors)\n",
    "    if len(anc)>0:\n",
    "        root_token=list(obj_last_token.ancestors)[0]\n",
    "    else:\n",
    "        root_token=obj_last_token\n",
    "    root_idx=root_token.i\n",
    "    mindis_token=[token_text(\"\"),float(\"inf\")]\n",
    "    for child in root_token.rights:\n",
    "        idx=child.idx\n",
    "        dif=idx-root_idx\n",
    "        if child.text in other_text:\n",
    "            continue\n",
    "        if dif < mindis_token[1]:\n",
    "            mindis_token=[child,dif]\n",
    "    if mindis_token[0].text==\"\":\n",
    "        most_right=root_idx\n",
    "    else:\n",
    "        most_right=find_max_idx(mindis_token[0],res=[])\n",
    "    property_tokens=doc[root_idx:most_right+1]\n",
    "    skip_blacklist = {\n",
    "      \"the\",\"his\",\"my\",\"her\", \"their\", \"an\", \"a\"}\n",
    "    tmp=[]\n",
    "    for token_ in property_tokens:\n",
    "        if token_.text in skip_blacklist:\n",
    "            continue\n",
    "        else:\n",
    "            tmp.append(token_)\n",
    "    property_tokens=tmp\n",
    "    break_blacklist = {\n",
    "      \"that\",\"for\",\"so\",\"because\", \"and\", \"until\",\"but\",\"when\"}\n",
    "    \n",
    "    tmp=[]\n",
    "    for token_ in property_tokens:\n",
    "        if token_.text in break_blacklist:\n",
    "            break\n",
    "        else:\n",
    "            tmp.append(token_)\n",
    "    property_tokens=tmp\n",
    "    property_text=\" \".join([_.text for _ in property_tokens]).replace(obj_text,\"\").replace(other_text,\"\").strip()\n",
    "    property_text=property_text.replace(\"  \",\" \")\n",
    "    return property_text\n",
    "\n",
    "def find_max_idx(token, res=[]):\n",
    "    # find the most right subtoken idx \n",
    "    if token.n_rights> 0:\n",
    "        for child in token.rights:\n",
    "            res.append(child.i)\n",
    "            find_max_idx(child, res=res)\n",
    "    else:\n",
    "        res.append(token.i)\n",
    "    return max(res)\n",
    "\n",
    "def find_min_idx(token, res=[]):\n",
    "    # find the most right subtoken idx \n",
    "    if token.n_lefts> 0:\n",
    "        for child in token.lefts:\n",
    "            res.append(child.i)\n",
    "            find_min_idx(child, res=res)\n",
    "    else:\n",
    "        res.append(token.i)\n",
    "    return min(res)\n",
    "\n",
    "def LastRootDetection(sent):\n",
    "    root_token=sent[-2]\n",
    "    for token in sent:\n",
    "        anc=list(token.ancestors)\n",
    "        token_idx=token.i\n",
    "        if token.pos_==\"PUNCT\" or token.pos_==\"ADV\":\n",
    "            continue\n",
    "            \n",
    "        if anc:\n",
    "            anc_i=anc[0].i\n",
    "            diff=token_idx-anc_i\n",
    "            \n",
    "            if diff >= 0.3*len(sent):\n",
    "                root_token=token\n",
    "        else:\n",
    "            root_token=token\n",
    "                \n",
    "    return root_token\n",
    "\n",
    "def elements_extraction_other(example):\n",
    "    # new information extraction rule for other format\n",
    "    # add punction\n",
    "    \n",
    "    sent_text=example[\"text\"]\n",
    "    # get the text of two object\n",
    "    obj1=example[\"option1\"]\n",
    "    obj2=example[\"option2\"]\n",
    "    \n",
    "    sent_text=AddPeriod(sent_text)\n",
    "    \n",
    "    doc=nlp(sent_text)\n",
    "    # find entity label.\n",
    "    ents_=list(doc.ents)\n",
    "    \n",
    "    person_num=0\n",
    "        \n",
    "    for ent_ in ents_:\n",
    "        if ent_.text in {obj1,obj2} or obj1 in ent_.text or obj2 in ent_.text:\n",
    "            if ent_.label_==\"PERSON\":\n",
    "                person_num+=1\n",
    "    \n",
    "    # find property 2\n",
    "    last_sent=list(doc.sents)[-1]\n",
    "    last_root=LastRootDetection(last_sent)\n",
    "    start_=last_root.i\n",
    "    for token_ in doc[last_root.i:]:\n",
    "        if token_.pos_==\"AUX\" or token_.lemma_ in {\"is\",\"are\",\"be\"}:\n",
    "            start_=token_.i\n",
    "            continue\n",
    "        \n",
    "        if token_.text in {\"he\",\"she\",\"they\"} and SubjectIdentifier([token_],last_sent):\n",
    "            start_=token_.i+1\n",
    "            continue\n",
    "    prop_last=(\" \").join([_.text for _ in doc[start_:] if _.pos_ not in {\"PUNCT\"} and _.text not in {\"so\",\"too\",\"very\"}])\n",
    "\n",
    "    if person_num>0:\n",
    "        reverse={\"that\",\"because\",\"for\"}\n",
    "        # example: Joan made sure to thank Susan for all the help she had recieved.\n",
    "        # recieved causes thank all the help\n",
    "        doc=nlp(sent_text)\n",
    "        obj1_tokens=[]\n",
    "        obj2_tokens=[]\n",
    "        for _sent in doc.sents:\n",
    "            if not obj1_tokens:\n",
    "                obj1_tokens=TokensByText(_sent,obj1,doc)\n",
    "                if obj1_tokens:\n",
    "                    sent1=_sent\n",
    "            if not obj2_tokens:\n",
    "                obj2_tokens=TokensByText(_sent,obj2,doc)\n",
    "                if obj2_tokens:\n",
    "                    sent2=_sent\n",
    "            if not obj1_tokens or not obj2_tokens:\n",
    "                return None\n",
    "            \n",
    "        # use obj tokens to decide it is subject or object\n",
    "        obj1_pos=SubjectIdentifier(obj1_tokens,sent1)\n",
    "        obj2_pos=SubjectIdentifier(obj2_tokens,sent2)\n",
    "        if obj1_pos and obj2_pos:\n",
    "            # both are subject\n",
    "            # A is property, B is property\n",
    "            obj1_prop=SubjectFindProp(obj1_tokens,obj2_tokens,doc)\n",
    "            obj2_prop=SubjectFindProp(obj2_tokens,obj1_tokens,doc)\n",
    "            #sent1= obj1_prop+\" \"+prop_last\n",
    "            #sent2= obj2_prop+\" \"+prop_last\n",
    "            for token in doc:\n",
    "                for item in reverse:\n",
    "                    if item in token.text:\n",
    "                        sent1= prop_last+\" causes \"+obj1_prop\n",
    "                        sent2= prop_last+\" causes \"+obj2_prop\n",
    "                        return 0, sent1, sent2\n",
    "                    else:\n",
    "                        sent1= obj1_prop+\" causes \"+prop_last\n",
    "                        sent2= obj2_prop+\" causes \"+prop_last\n",
    "            return 0, sent1, sent2\n",
    "        elif obj1_pos or obj2_pos:\n",
    "            # A property B\n",
    "            if obj1_pos:\n",
    "                obj_prop=SubjectFindProp(obj1_tokens,obj2_tokens,doc)\n",
    "                status=[\"obj1\",\"obj2\"]\n",
    "            else:\n",
    "                obj_prop=SubjectFindProp(obj2_tokens,obj1_tokens,doc)\n",
    "                status=[\"obj2\",\"obj1\"]\n",
    "            \n",
    "            sent=\"\"\n",
    "            for token in doc:\n",
    "                for item in reverse:\n",
    "                    if item in token.text:\n",
    "                        sent= prop_last+\" causes \"+obj_prop\n",
    "                        return 1, sent\n",
    "                    else:\n",
    "                        sent= obj_prop+\" causes \"+prop_last\n",
    "            return 1, sent\n",
    "        else:\n",
    "            obj1_prop=\" \".join([_.text for _ in obj1_tokens])\n",
    "            obj2_prop=\" \".join([_.text for _ in obj2_tokens])\n",
    "            return\n",
    "    else:\n",
    "        # example: The city councilmen refused the demonstrators a permit because they feared violence.\n",
    "        # sentences: The city councilmen feared violence; the demonstrators feared violence\n",
    "        sent1= obj1+\" \"+prop_last\n",
    "        sent2= obj2+\" \"+prop_last\n",
    "        \n",
    "        return 2, sent1, sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122619"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed1=model.encode(\"was upset causes comforted\")\n",
    "embed2=model.encode(\"relieve antonym upset\")\n",
    "sim=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_information =[]\n",
    "with open(\"./cskg_embedding/wsc.txt\",\"w\") as f:\n",
    "    for example in examples:\n",
    "        sent=example[\"text\"]\n",
    "        tmp=elements_extraction_other(example)\n",
    "        if tmp:\n",
    "            if tmp[0]==0:\n",
    "                hypo_sent=[tmp[1],tmp[2]]\n",
    "            elif tmp[0]==1:\n",
    "                hypo_sent=[tmp[1]]\n",
    "            elif tmp[0]==2:\n",
    "                hypo_sent=[tmp[1],tmp[2]]\n",
    "        else:\n",
    "            hypo_sent=[\"\"]\n",
    "        \n",
    "        f.write(\"Sentence: \")\n",
    "        f.write(sent+\"\\n\")\n",
    "        f.write(\"Hypo Sentence: \")\n",
    "        f.write(\"\\t\".join(hypo_sent)+\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        example_information.append({\"Sentence\":sent,\n",
    "                                \"hypo sentences\":hypo_sent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cskg embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cskg file\n",
    "\n",
    "with open(cskg_connected_file, \"r\") as f:\n",
    "    head = f.readline().strip().split(\"\\t\")\n",
    "    \n",
    "    # load lines only contain relation==HasProperty\n",
    "    cskg_lines=[]\n",
    "    \n",
    "    for item in f:\n",
    "        line = item.strip().split(\"\\t\")\n",
    "        relation_id=line[2]\n",
    "        cskg_lines.append(line)\n",
    "        \n",
    "cskg_sents=[]\n",
    "for line in cskg_lines:\n",
    "    cskg_sents.append(f\"{line[4]} {line[6]} {line[5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isfile_=os.path.isfile('cskg_model_embed.pickle')\n",
    "if isfile_:\n",
    "    with open('cskg_model_embed.pickle', 'rb') as handle:\n",
    "        cskg_embed = pickle.load(handle)\n",
    "else:\n",
    "    # divide task into 1000chunks and run loop\n",
    "    chunk_num = 1000\n",
    "    chunks_size = math.ceil(len(cskg_sents)/1000)\n",
    "    cskg_embed=np.empty((0,1024), dtype=np.float32)\n",
    "    count = 0\n",
    "\n",
    "    for num in tqdm(range(1,chunk_num+1)):\n",
    "        end = num*chunks_size\n",
    "        start = chunks_size*(num-1)\n",
    "        temp_embed=model.encode(cskg_sents[start:end])\n",
    "        cskg_embed=np.append(cskg_embed, temp_embed, axis=0)\n",
    "        count += 1 \n",
    "        \n",
    "    # store file into desktop\n",
    "    with open('cskg_model_embed.pickle', 'wb') as handle:\n",
    "        pickle.dump(cskg_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6003237/6003237 [19:42<00:00, 5078.00it/s]\n"
     ]
    }
   ],
   "source": [
    "cskg_embed=np.array([S/(math.sqrt(sum(S**2))) for S in tqdm(cskg_embed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:00<00:00, 3418.65it/s]\n"
     ]
    }
   ],
   "source": [
    "hyp_sents=[]\n",
    "for line in example_information:\n",
    "    sents=line[\"hypo sentences\"]\n",
    "    for sent in sents:\n",
    "        hyp_sents.append(sent)\n",
    "    \n",
    "hyp_sents_embed=model.encode(hyp_sents)\n",
    "hyp_sents_embed=np.array([S/(math.sqrt(sum(S**2))) for S in tqdm(hyp_sents_embed)])\n",
    "\n",
    "# use faiss to find neareast\n",
    "d= cskg_embed.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(cskg_embed)\n",
    "\n",
    "min_sim=1\n",
    "max_sim=0\n",
    "# find the closest edges\n",
    "k = 1\n",
    "D, I = index.search(hyp_sents_embed, k)\n",
    "for idx,embed in zip(I,hyp_sents_embed):\n",
    "    idx=idx[0]\n",
    "    embed1=cskg_embed[idx]\n",
    "    embed2=embed\n",
    "    similar=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "    min_sim=min(min_sim,similar)\n",
    "    max_sim=max(max_sim,similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7112264, 1.0)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sim,max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=(min_sim+max_sim)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "with open(\"./cskg_embedding/wsc.txt\", \"w\") as f:\n",
    "    f.write(f\"The threshold of cosine similar is: {threshold}\\n\")\n",
    "    f.write(\"######################\\n\")\n",
    "    count=0\n",
    "    QA_num=0\n",
    "    for idx in range(len(examples)):\n",
    "        example=examples[idx]\n",
    "        line=example_information[idx]\n",
    "        sent=example[\"text\"]\n",
    "        hypo_sents=line[\"hypo sentences\"]\n",
    "        Most_Similar_CSKG=[]\n",
    "        sims=[]\n",
    "        for hypo_sent in hypo_sents:\n",
    "            idx=I[count][0]\n",
    "            Most_Similar_CSKG.append(cskg_sents[idx])\n",
    "            embed1=cskg_embed[idx]\n",
    "            embed2=hyp_sents_embed[count]\n",
    "            similar=dot(embed1, embed2)/(norm(embed1)*norm(embed2))\n",
    "            sims.append(str(round(similar,2)))\n",
    "            count+=1\n",
    "        QA_num+=1\n",
    "        if len(sims)==1:\n",
    "            ground=example[\"label\"]\n",
    "            if float(sims[0]) >= threshold:\n",
    "                prediction=0\n",
    "                if ground == 0:\n",
    "                    correct+=1\n",
    "            else:\n",
    "                prediction=1\n",
    "                if ground==1:\n",
    "                    correct+=1\n",
    "        elif len(sims)==2:\n",
    "            ground=example[\"label\"]\n",
    "            if float(sims[0]) >= float(sims[1]):\n",
    "                prediction=0\n",
    "                if ground ==0:\n",
    "                    correct+=1\n",
    "            else:\n",
    "                prediction=1\n",
    "                if ground == 1:\n",
    "                    correct+=1\n",
    "                    \n",
    "        f.write(str(QA_num)+\").\"+sent+\"\\n\")\n",
    "        f.write(f\"Hypo Sentences:{'; '.join(hypo_sents)}\\n\")\n",
    "        f.write(f\"Cosine Similarity:{'; '.join(sims)}\\n\")\n",
    "        f.write(f\"Most Similar CSKG Line:{'; '.join(Most_Similar_CSKG)}\\n\")\n",
    "        f.write(f\"Ground:{ground}\\n\")\n",
    "        f.write(f\"Prediction:{prediction}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4945054945054945"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['counterfeit watch subclass of counterfeit consumer good',\n",
       " \"loses wallet|personx loses persony's wallet effect on person x will be blamed\"]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Most_Similar_CSKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=model.encode(\"abdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypo_sents_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"39629d4f0dd94192a6e2aff0e2b553e6-0\" class=\"displacy\" width=\"1975\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">bob</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">paid</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">charlie</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">college</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">education.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">very</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">generous.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210.0,354.0 L218.0,342.0 202.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1095.0,89.5 1095.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-5\" stroke-width=\"2px\" d=\"M420,352.0 C420,2.0 1100.0,2.0 1100.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,354.0 L1108.0,342.0 1092.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-7\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39629d4f0dd94192a6e2aff0e2b553e6-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc=nlp(\"bob paid for charlie's college education. He is very generous.\")\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isi",
   "language": "python",
   "name": "isi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
